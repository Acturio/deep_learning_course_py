<!-- ::: watermark -->
<!-- <img src="img/aif-logo.png" width="400"/> -->
<!-- ::: -->


# Perceptrón Multicapa

## Perceptrones multicapa

En la Sección anterior, se presentó la regresión softmax, implementando el algoritmo desde cero. Esto permitió entrenar clasificadores capaces de reconocer 10 categorías de prendas de vestir a partir de imágenes de baja resolución. Durante el proceso, aprendimos a manipular los datos, a convertir las salidas en una distribución de probabilidad válida, a aplicar una función de pérdida adecuada y a minimizarla con respecto a los parámetros del modelo. Ahora que se conocen estos aspectos en el contexto de modelos lineales simples, podemos comenzar la exploración de las redes neuronales profundas, la clase de modelos relativamente rica que constituye el tema principal de este curso.

### Capas ocultas

En la sección 3.1., se describen las transformaciones afines como transformaciones lineales con un sesgo añadido. Para empezar, recordemos la arquitectura del modelo correspondiente a nuestro ejemplo de regresión softmax, ilustrado en la figura 4.1.1. Este modelo asigna directamente las entradas a las salidas mediante una única transformación afín, seguida de una operación softmax. Si nuestras etiquetas estuvieran realmente relacionadas con los datos de entrada mediante una simple transformación afín, este enfoque sería suficiente. Sin embargo, la linealidad (en las transformaciones afines) es una suposición muy restrictiva.

```{r echo=FALSE, fig.align='center', out.height='300pt', out.width='650pt'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig15.png")
```

#### Limitaciones de los modelos lineales

La linealidad supone una relación monotónica: si una variable aumenta, la salida del modelo siempre sube o baja según el signo del peso. Esto a veces es razonable, como al predecir el pago de un préstamo según ingresos, aunque la relación no sea estrictamente lineal. En estos casos, funciones como la logística pueden hacer más plausible la linealidad.

Sin embargo, muchas relaciones reales no son monotónicas. Por ejemplo, la temperatura corporal: valores por encima o por debajo de 37 °C aumentan el riesgo, por lo que conviene transformar la variable (p. ej., usar la distancia a 37 °C).

```{r echo=FALSE, fig.align='center', out.height='300pt', out.width='650pt'}
knitr::include_graphics("img/05_Multilayer_Perceptron/lineal_vs_no_lineal.png")
```

En problemas como la clasificación de imágenes, la linealidad es claramente insuficiente: cambiar el brillo de un solo píxel no determina si hay un gato o un perro. Aquí el significado de cada píxel depende de su contexto, y no existe un preprocesamiento simple que lo capture. Las redes neuronales profundas resuelven esto aprendiendo simultáneamente una representación adecuada y un predictor lineal sobre esa representación.

La necesidad de modelar no linealidades se conoce desde hace un siglo y ha dado lugar a enfoques como árboles de decisión, métodos kernel, splines y, más recientemente, redes neuronales, inspiradas en las conexiones jerárquicas entre neuronas del cerebro.

#### Incorporación de Capas Ocultas

Para superar las limitaciones de los modelos lineales, podemos agregar capas ocultas. La idea más básica es apilar varias capas totalmente conectadas: cada capa envía su salida a la siguiente. Las primeras capas aprenden una representación de los datos y la última capa actúa como un modelo lineal sobre esa representación.

A esta arquitectura se le llama perceptrón multicapa o MLP.

Observa la siguiente imagen que contempla un MLP con 4 entradas, 1 capa oculta con 5 neuronas y 3 salidas. Como la capa de entrada no calcula nada, realmente el modelo tiene 2 capas de cálculo: la oculta y la de salida.

```{r echo=FALSE, fig.align='center', out.height='300pt', out.width='650pt'}
knitr::include_graphics("img/05_Multilayer_Perceptron/MLP01_example.png")
```

En un MLP, cada neurona de una capa está conectada con todas las neuronas de la siguiente. Es decir, cada entrada afecta a todas las neuronas ocultas, y cada una de estas afecta a todas las neuronas de salida.

#### De lo Lineal a lo No Lineal

Como antes, denotamos con la matriz $X \in \mathbb{R}^{n \times d}$ un minibatch de $n$ ejemplos, donde cada ejemplo tiene $d$ entradas (características). Para un MLP de una capa oculta cuya capa oculta tiene $h$ unidades ocultas, denotamos por $H \in \mathbb{R}^{n \times h}$ las salidas de la capa oculta, que son representaciones ocultas. Dado que las capas oculta y de salida están completamente conectadas, tenemos pesos de capas ocultas $W^{(1)} \in \mathbb{R}^{d \times h}$ y sesgos $b^{(1)} \in \mathbb{R}^{1 \times h}$ y pesos $W^{(2)} \in \mathbb{R}^{h \times q}$ y sesgos $b^{(2)} \in \mathbb{R}^{1 \times q}$ de la capa de salida. Esto nos permite calcular las salidas $O \in \mathbb{R}^{n \times q}$ del MLP de una capa oculta de la siguiente manera:

$$
H=XW^{(1)}+b^{(1)},
$$
$$
O=HW^{(2)}+b^{(2)}.
$$

::: {.infobox .important data-latex="{important}"}
**¡¡ RECORDATORIO !!**

Una función "Afín" está dada por: $f(x)=Ax+b$,

Mientras que una función lineal está dado por: $f(x)=Ax$
:::

Hay que tener en cuenta que, tras añadir la capa oculta, el modelo requiere que se rastree y actualicen conjuntos adicionales de parámetros. ¿Qué se ha ganado a cambio? Sorprendentemente, el modelo definido anteriormente, ¡NO GANA NADA! y la razón es sencilla. Las unidades ocultas anteriores se dan mediante una función afín de las entradas, y las salidas (pre-softmax) son simplemente una función afín de las unidades ocultas. La composición de funciones afines es todavía una función afín. Además, el modelo lineal ya era capaz de representar cualquier función afín.

Para verlo formalmente, se puede simplemente colapsar la capa oculta en la definición anterior, lo que genera un modelo equivalente de una sola capa con parámetros.

$$
W=W^{(1)}W^{(2)} \text{ and } b=b^{1}W^{(2)}+b^{(2)}:
$$
$$
O=(XW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}=XW^{(1)}W^{(2)}+b^{(1)}W^{(2)}+b^{(2)}=XW+b
$$

Para aprovechar el potencial de las arquitecturas multicapa, se necesita un ingrediente clave adicional: **una función de activación no lineal** que se aplique a cada unidad oculta tras la transformación afín. na opción popular es la función de activación ReLU (unidad lineal rectificada) (Nair y Hinton, 2010) $\sigma(x)=max(0, x)$, que opera sobre sus argumentos elemento por elemento. Las salidas de las funciones de activación $\sigma(\cdot)$ se denominan **activaciones**. En general, con las funciones de activación establecidas, ya no es posible convertir el MLP en un modelo lineal:

$$
H=\sigma(XW^{(1)}+b^{(1)}),
$$
$$
O=HW^{(2)}+b^{(2)}.
$$

Dado que cada fila en $X$ corresponde a un ejemplo en el minibatch, con cierto abuso de notación, definimos la no linealidad $\sigma$ para aplicarla a sus entradas por filas, es decir, un ejemplo a la vez. Con frecuencia, las funciones de activación se aplican no solo por filas, sino también por elementos. Esto significa que, tras calcular la parte lineal de la capa, se calcula cada activación sin tener en cuenta los valores tomados por las demás unidades ocultas.

#### Aproximadores Universales

::: {.infobox .quicktip data-latex="{quicktip}"}
**¿Qué es un aproximador universal?**

Es un resultado teórico que dice que **una red neuronal con una sola capa oculta puede representar prácticamente cualquier función**, si tiene suficientes neuronas y parámetros.
:::

**Pero ojo:**

Que pueda representar cualquier función no significa que sea fácil aprenderla. Tener una sola capa oculta puede requerir muchísimas neuronas (algo poco práctico). A veces es mejor usar: métodos de kernels, cuando aplican, porque resuelven el problema de forma exacta; redes más profundas, que permiten representar las mismas funciones de manera más compacta y eficiente.

**Idea didáctica clave**

La profundidad importa. Con suficiente profundidad, una red puede aprender funciones complejas de forma más eficiente que una red muy ancha con una sola capa.

### Funciones de activación

Las funciones de activación determinan si una neurona debe activarse o no calculando la suma ponderada y agregándole un sesgo. Son operadores diferenciables que transforman señales de entrada en salidas, y la mayoría añade no linealidad. Dado que las funciones de activación son fundamentales para el aprendizaje profundo, se revisarán algunas de las más comunes.

#### Función ReLU

La opción más popular, debido tanto a su simplicidad de implementación como a su buen rendimiento en diversas tareas predictivas, es la unidad lineal rectificada (ReLU) (Nair y Hinton, 2010). ReLU proporciona una transformación no lineal muy simple. Dado un elemento , la función se define como el máximo de ese elemento y 0:

$$
ReLU(x)=max(x, 0).
$$

De manera informal, **la función ReLU conserva solo los elementos positivos y descarta todos los negativos** estableciendo las activaciones correspondientes a 0. Para una mayor comprensión, podemos representar gráficamente la función. Como se puede observar, la función de activación es lineal por partes.

```{python, fig.align='center'}
import torch
import matplotlib.pyplot as plt

# Datos
x = torch.arange(-8.0, 8.0, 0.1)
y = torch.relu(x)

# Gráfica
plt.figure(figsize=(5, 2.5))
plt.plot(x, y)
plt.xlabel("x")
plt.ylabel("ReLU(x)")
plt.title("Función de activación ReLU")
plt.grid(True)
plt.show()
```

Cuando la entrada es negativa, la derivada de la función ReLU es 0, y cuando la entrada es positiva, la derivada de la función ReLU es 1. Nótese que la función ReLU no es diferenciable cuando la entrada toma valor exactamente igual a 0. En estos casos, se usa por defecto la derivada del lado izquierdo y decimos que la derivada es 0 cuando la entrada es 0. Podemos evitar esto porque la entrada puede que nunca sea realmente cero (los matemáticos dirían que no es diferenciable en un conjunto de medida cero). Hay un viejo adagio que dice que si las condiciones de contorno sutiles importan, probablemente estemos haciendo matemáticas (reales), no ingeniería. Esa sabiduría convencional puede aplicarse aquí, o al menos, el hecho de que no estamos realizando optimización restringida (Mangasarian, 1965, Rockafellar, 1970). se traza la derivada de la función ReLU a continuación.

```{python, fig.align='center'}
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)

y.backward(torch.ones_like(x))

plt.figure(figsize=(5, 2.5))
plt.plot(x.detach(), x.grad)
plt.xlabel('x')
plt.ylabel('grad ReLU(x)')
plt.title('Derivada de ReLU')
plt.grid(True)
plt.show()
```

La razón para usar ReLU es que sus derivadas se comportan particularmente bien: o se anulan o simplemente dejan pasar el argumento. Esto mejora el comportamiento de la optimización y mitiga el problema bien documentado de los gradientes evanescentes que afectaba a versiones anteriores de redes neuronales (más sobre esto más adelante).

Cabe destacar que existen muchas variantes de la función ReLU, incluyendo la función ReLU parametrizada (pReLU) (He et al., 2015). Esta variación añade un término lineal a ReLU, por lo que parte de la información se transmite, incluso cuando el argumento es negativo:

$$
pReLU(x)=max(0,x)+\alpha \cdot min(0,x).
$$

#### Función Sigmoide

La función sigmoidea transforma las entradas cuyos valores se encuentran en el dominio $\mathbb{R}$, en salidas que se encuentran en el intervalo (0, 1). Por esta razón, la función sigmoidea suele denominarse función de aplastamiento (*squashing function*): aplasta cualquier entrada en el rango (-inf, inf) a un valor en el rango (0, 1):

$$
\text{sigmoid}(x)=\frac{1}{1+\text{exp}(-x)}
$$

A continuación, se grafica la función sigmoidea. Nótese que cuando la entrada se acerca a 0, la función sigmoidea se aproxima a una transformación lineal.

```{python, fig.align='center'}
x = torch.arange(-8.0, 8.0, 0.1)
y = torch.sigmoid(x)

plt.figure(figsize=(5, 2.5))
plt.plot(x.detach(), y.detach())
plt.xlabel("x")
plt.ylabel("sigmoid(x)")
plt.title("Función Sigmoid")
plt.grid(True)
plt.show()
```

La derivada de la función sigmoide está dada por la siguiente ecuación:

$$
\frac{d}{dx}\text{sigmoid}(x)=\frac{\text{exp(-x)}}{(1+\text{exp}(-x))^2}=\text{sigmoid}(x)(1-\text{sigmoid}(x))
$$

La derivada de la función sigmoidea se grafica a continuación. Nótese que cuando la entrada es 0, la derivada de la función sigmoidea alcanza un máximo de 0,25. A medida que la entrada diverge de 0 en cualquier dirección, la derivada tiende a 0.

```{python, fig.align='center'}
# Definir x con gradientes
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.sigmoid(x)

# Borrar gradientes previos (si los hubiera)
if x.grad is not None:
    x.grad.zero_()

# Backprop para obtener la derivada
y.backward(torch.ones_like(x), retain_graph=True)

# Graficar
plt.figure(figsize=(5, 2.5))
plt.plot(x.detach(), x.grad.detach())
plt.xlabel("x")
plt.ylabel("grad sigmoid(x)")
plt.title("Derivada de la función Sigmoid")
plt.grid(True)
plt.show()
```

#### Función Tanh

Al igual que la función sigmoidea, la función tangente hiperbólica también reduce sus valores de entrada, transformándolos en elementos del intervalo entre -1 y 1.

$$
\text{tanh}(x)=\frac{1-\text{exp}(-2x)}{1+\text{exp}(-2x)}
$$

Nótese que, a medida que la entrada se acerca a 0, la función tanh se aproxima a una transformación lineal. Aunque la forma de la función es similar a la de la función sigmoidea, la función tanh presenta **simetría puntual respecto al origen** del sistema de coordenadas (Kalman y Kwasny, 1992).

```{python, fig.align='center'}
# Datos
x = torch.arange(-8.0, 8.0, 0.1)
y = torch.tanh(x)

# Gráfica
plt.figure(figsize=(5, 2.5))
plt.plot(x.detach(), y.detach())
plt.xlabel("x")
plt.ylabel("tanh(x)")
plt.title("Función tanh")
plt.grid(True)
plt.show()
```

La derivada de la función *tanh* es:

$$
\frac{d}{dx}\text{tanh}(x)=1-\text{tanh}^{2}(x)
$$

A medida que la entrada se acerca a 0, la derivada de la función *tanh* se acerca a un máximo de 1. Y, como se vio con la función sigmoidea, a medida que la entrada se aleja de 0 en cualquier dirección, la derivada de la función *tanh* se acerca a 0.

```{python, fig.align='center'}
# Definir x con gradientes activados
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.tanh(x)

# Borrar gradientes anteriores
if x.grad is not None:
    x.grad.zero_()

# Backprop para obtener la derivada
y.backward(torch.ones_like(x), retain_graph=True)

# Graficar la derivada
plt.figure(figsize=(5, 2.5))
plt.plot(x.detach(), x.grad.detach())
plt.xlabel("x")
plt.ylabel("grad tanh(x)")
plt.title("Derivada de la función tanh")
plt.grid(True)
plt.show()
```

### Discusión

Ahora sabemos cómo incorporar **no linealidades** para construir arquitecturas expresivas de redes neuronales multicapa. Una ventaja de la función ReLU es que es significativamente más fácil de optimizar que la función sigmoidea o la función tanh. Se podría argumentar que esta fue una de las innovaciones clave que impulsaron el resurgimiento del aprendizaje profundo en la última década. 

Cabe destacar, sin embargo, que la investigación en funciones de activación no se ha detenido. Por ejemplo, la función de activación GELU (unidad lineal de error gaussiano) $x\phi(x)$ de Hendrycks y Gimpel (2016) $\phi(x)$ (es la función de distribución acumulativa gaussiana estándar) y la función de activación *Swish* $\sigma(x)=x\cdot\text{sigmoid}(\beta x)$, propuesta por Ramachandran et al. (2017), pueden ofrecer una mayor precisión en muchos casos.

### Ejercicios

1. Demuestre que añadir capas a una red lineal profunda, es decir, una red sin no linealidad $\sigma$, nunca puede aumentar su potencia expresiva. Dé un ejemplo donde la reduzca activamente.

2. Calcule la derivada de la función de activación pReLU.

3. Calcule la derivada de la función de activación Swish $x\cdot \text{sigmoid}(\beta x)$.

4. Demuestre que una MLP que utiliza solo ReLU (o pReLU) construye una función lineal continua por partes.

5. Sigmoid y tanh son muy similares.

* Demuestre que $\text{tanh}(x)+1 = 2\text{sigmoid}(2x)$.

* Demuestre que las clases de función parametrizadas por ambas no linealidades son idénticas. Pista: las capas afines también tienen términos de sesgo.

6. Suponga que tenemos una no linealidad que se aplica a un minibatch a la vez, como la normalización por lotes (Ioffe y Szegedy, 2015). ¿Qué tipo de problemas espera que esto cause?

7. Dé un ejemplo donde los gradientes se anulen para la función de activación sigmoide.



## Implementación de Perceptrones Multicapa

Los perceptrones multicapa (MLP) no son mucho más complejos de implementar que los modelos lineales simples. La diferencia conceptual clave radica en que ahora se concatenan múltiples capas.

### Implementación desde Cero

Comencemos de nuevo implementando dicha red desde cero.

#### Inicialización de parámetros del modelo

El conjunto de datos **Fashion-MNIST** contiene 10 clases de imágenes y cada imagen consiste en una cuadrícula de valores de píxeles en escala de grises. Como antes, por ahora ignoraremos la estructura espacial entre los píxeles, por lo que podemos considerarlo como un conjunto de datos de clasificación con 784 características de entrada y 10 clases. Para comenzar, implementaremos un MLP con una capa oculta y 256 unidades ocultas. Tanto el número de capas como su ancho son ajustables (se consideran hiperparámetros). 

::: {.infobox .quicktip data-latex="{quicktip}"}
**Tip:**

Normalmente, se busca que **el ancho de las capas (número de neuronas) sea divisible por potencias mayores de 2**, es decir: 4, 8, 16, 32, 64, 128, 256, 512, etc. Esto es computacionalmente eficiente debido a que las GPUs trabajan internamente con bloques de tamaño 32 y potencias de 2, haciendo que esos anchos sean:

* Más rápidos
* Más fáciles de paralelizar
* Más eficientes en memoria
:::

De nuevo, representaremos nuestros parámetros con varios tensores. Tenga en cuenta que, para cada capa, debemos registrar una matriz de ponderación y un vector de sesgo. Es importante tomar en cuenta la asignación de memoria para los gradientes de pérdida con respecto a estos parámetros.

```{python}
import torch
from torch import nn
import matplotlib.pyplot as plt
import inspect

class Module(nn.Module):
    def __init__(self):
        super().__init__()
        self.metrics = {}

    def save_hyperparameters(self, ignore=[]):
        frame = inspect.currentframe().f_back
        _, _, _, local_vars = inspect.getargvalues(frame)
        for k, v in local_vars.items():
            if k not in ignore and k != "self":
                setattr(self, k, v)

    def plot(self, name, value, train=True):
        """Guarda valores para graficarlos al final."""
        key = f"{name}_{'train' if train else 'val'}"
        if key not in self.metrics:
            self.metrics[key] = []
        self.metrics[key].append(value)


    def forward(self, X):
        raise NotImplementedError("Define forward() in subclass")


class Classifier(Module):
    def loss(self, y_hat, y):
        return nn.CrossEntropyLoss()(y_hat, y)

    def accuracy(self, y_hat, y):
        preds = y_hat.argmax(dim=1)
        return (preds == y).float().mean()

    # def train_step(self, batch):
    #     X, y = batch
    #     y_hat = self.forward(X)
    #     loss = self.loss(y_hat, y)
    # 
    #     # backprop
    #     self.optimizer.zero_grad()
    #     loss.backward()
    #     self.optimizer.step()
    # 
    #     return loss.item(), self.accuracy(y_hat, y).item()
       
    def evaluation_step(self, X, y):
        with torch.no_grad():
            y_hat = self(X)
            return (
                self.loss(y_hat, y).item(),
                self.accuracy(y_hat, y).item(),
            )



# ------ Decorador para agregar métodos ------
def add_to_class(cls):
    def decorator(fn):
        setattr(cls, fn.__name__, fn)
        return fn
    return decorator

```

En el código siguiente usamos nn.Parameter para registrar automáticamente un atributo de clase como un parámetro que será rastreado por autograd.

```{python}
class MLPScratch(Classifier):
    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.lr = lr

        self.W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)
        self.b1 = nn.Parameter(torch.zeros(num_hiddens))
        
        self.W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)
        self.b2 = nn.Parameter(torch.zeros(num_outputs))

```

#### Modelo

Para asegurarnos de que sabemos cómo funciona todo, implementaremos la activación de ReLU nosotros mismos en lugar de invocar directamente la función relu incorporada.

```{python}
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)

```

Como ignoramos la estructura espacial, transformamos cada imagen bidimensional en un vector plano de longitud num_inputs. Finalmente, implementamos nuestro modelo con solo unas pocas líneas de código. Como usamos el framework autograd integrado, esto es todo lo que necesitamos.

```{python}
# Añadir método usando el decorador
@add_to_class(MLPScratch)
def forward(self, X):
    X = X.reshape((-1, self.num_inputs))
    H = relu(torch.matmul(X, self.W1) + self.b1)
    O = torch.matmul(H, self.W2) + self.b2
    return O

```

#### Entrenamiento

Afortunadamente, el ciclo de entrenamiento para las MLP es exactamente el mismo que para la regresión softmax. Definimos el modelo, los datos y el entrenador, y finalmente invocamos el método de ajuste en el modelo y los datos.

Primero, unas clases auxiliares:

##### DataLoader: {-}

```{python}
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

class FashionMNISTData:
    def __init__(self, batch_size=256):
        self.batch_size = batch_size
        self.transform = transforms.ToTensor()

        self.train = datasets.FashionMNIST(
            root="./data", train=True, download=True, transform=self.transform
        )
        self.test = datasets.FashionMNIST(
            root="./data", train=False, download=True, transform=self.transform
        )

        self.train_loader = DataLoader(self.train, batch_size=batch_size, shuffle=True)
        self.test_loader = DataLoader(self.test, batch_size=batch_size)

    def __iter__(self):
        return iter(self.train_loader)

    def __len__(self):
        return len(self.train_loader)

```

##### Trainer minimalista {-}

```{python}
class Trainer:
    def __init__(self, max_epochs=10):
        self.max_epochs = max_epochs

    def fit(self, model, data):
        model.metrics = {}
        optimizer = torch.optim.SGD(model.parameters(), lr=model.lr)

        # Asegura que exista test_loader
        if not hasattr(data, "test_loader"):
            raise ValueError("El dataset debe tener data.test_loader para validación")
        
        for epoch in range(self.max_epochs):
            total_loss, total_acc, count = 0, 0, 0
            
            for X, y in data:
                optimizer.zero_grad()
                y_hat = model(X)
                loss = model.loss(y_hat, y)
                loss.backward()
                optimizer.step()

                total_loss += loss.item() * X.size(0)
                total_acc += model.accuracy(y_hat, y).item() * X.size(0)
                count += X.size(0)
                
            # Al final de cada epoch: guardar promedios de entrenamiento
            model.plot("loss", total_loss / count, train=True)
            model.plot("acc", total_acc / count, train=True)

            # ---------- VALIDACIÓN ----------
            val_loss, val_acc, val_count = 0, 0, 0
            for X, y in data.test_loader:
                l, a = model.evaluation_step(X, y)
                val_loss += l * X.size(0)
                val_acc += a * X.size(0)
                val_count += X.size(0)
            
            # guardar métricas de validación por epoch
            model.plot("loss", val_loss / val_count, train=False)
            model.plot("acc", val_acc / val_count, train=False)
            
            # -------- PRINT FULL METRICS --------
            print(
                f"Epoch {epoch+1}: "
                f"train_loss={total_loss/count:.3f}, "
                f"val_loss={val_loss/val_count:.3f}, "
                f"train_acc={total_acc/count:.3f}, "
                f"val_acc={val_acc/val_count:.3f}"
            )
            
        # Al final → graficar exactamente como el libro
        self._plot_metrics(model)

    def _plot_metrics(self, model):
        metrics = model.metrics
        
        n = len(metrics["loss_train"])
        epochs = range(n)
    
        plt.plot(epochs, metrics["loss_train"], label="train_loss")
        #plt.plot(epochs, metrics["loss_val"], label="val_loss")
        plt.plot(epochs, metrics["acc_train"], label="train_acc")
        plt.plot(epochs, metrics["acc_val"], label="val_acc")
    
        plt.xlabel("epoch")
        plt.legend()
        plt.title("Training and Validation Metrics")
        plt.show()
        
        plt.clf()       # Limpia la figura actual
        plt.close()

```


```{python}
# Crear modelo
model = MLPScratch(
    num_inputs=784,
    num_outputs=10,
    num_hiddens=256,
    lr=0.1
)

# Cargar datos (reemplaza d2l.FashionMNIST)
data = FashionMNISTData(batch_size=256)
```


```{python, eval=T}
trainer = Trainer(max_epochs=10)
#trainer.fit(model, data)
```

### Implementación

La implementación pasada es bastante útil para entender cómo funciona el modelo, no obstante, al implementar las API de alto nivel, podemos crear MLP de forma aún más concisa.

#### Modelo

En comparación con la implementación concisa de la regresión softmax, la única diferencia radica en que se añaden dos capas completamente conectadas donde antes solo se añadía una. La primera es la capa oculta y la segunda, la capa de salida.

```{python}
class MLP(Classifier):
    def __init__(self, num_outputs, num_hiddens, lr):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
            nn.Flatten(), 
            nn.LazyLinear(num_hiddens),
            nn.ReLU(), 
            nn.LazyLinear(num_outputs)
        )

    def forward(self, X):
        return self.net(X)

```

Anteriormente, definimos métodos **forward** para que los modelos transformen la entrada utilizando los parámetros del modelo. Estas operaciones son esencialmente una secuencia: se toma una entrada y se aplica una transformación (por ejemplo, multiplicación de matrices con pesos seguida de la adición de sesgos), y luego se utiliza repetidamente la salida de la transformación actual como entrada para la siguiente transformación. Sin embargo, es posible notar que aquí no se define ningún método **forward**. 

De hecho, MLP hereda el método de *forward* de la clase Module para simplemente invocar self.net(X) (X es la entrada), que ahora se define como una secuencia de transformaciones mediante la clase *Sequential*. La clase *Sequential* abstrae el proceso de avance (forward), lo que nos permite centrarnos en las transformaciones. Analizaremos con más detalle el funcionamiento de la clase *Sequential* en secciones posteriores.

#### Entrenamiento

El ciclo de entrenamiento es exactamente el mismo que cuando implementamos la regresión softmax. Esta modularidad permite separar los aspectos relacionados con la arquitectura del modelo de las consideraciones ortogonales.

```{python}
model = MLP(num_outputs=10, num_hiddens=256, lr=0.1)
trainer.fit(model, data)

```

### Ejercicios

1. Cambie el número de unidades ocultas num_hiddens y grafique cómo su número afecta la precisión del modelo. ¿Cuál es el mejor valor para este hiperparámetro?

2. Intente añadir una capa oculta para ver cómo afecta a los resultados.

3. ¿Por qué es una mala idea insertar una capa oculta con una sola neurona? ¿Qué podría salir mal?

4. ¿Cómo altera los resultados el cambio de la tasa de aprendizaje? Con todos los demás parámetros fijos, ¿qué tasa de aprendizaje ofrece los mejores resultados? ¿Cómo se relaciona esto con el número de épocas?

5. Optimicemos todos los hiperparámetros conjuntamente: tasa de aprendizaje, número de épocas, número de capas ocultas y número de unidades ocultas por capa.

 * ¿Cuál es el mejor resultado que se puede obtener optimizando todos ellos?
 * ¿Por qué es mucho más difícil gestionar varios hiperparámetros?
 * Describa una estrategia eficiente para optimizar varios parámetros conjuntamente.

6. Compare la velocidad del framework y la implementación desde cero para un problema complejo. ¿Cómo cambia con la complejidad de la red?

7. Mide la velocidad de las multiplicaciones tensor-matriz para matrices bien alineadas y desalineadas. Por ejemplo, prueba matrices con dimensiones 1024, 1025, 1026, 1028 y 1032.

 * ¿Cómo cambia esto entre GPU y CPU?
 * Determina el ancho del bus de memoria de tu CPU y GPU.

8. Prueba diferentes funciones de activación. ¿Cuál funciona mejor?

9. ¿Existe alguna diferencia entre las inicializaciones de peso de la red? ¿Tiene importancia?


## Forward Propagation, Backward Propagation

Hasta ahora hemos entrenado redes con descenso de gradiente usando minibatches, pero siempre confiando en que el framework calcule los gradientes por nosotros. Gracias a la diferenciación automática, no necesitamos derivar a mano expresiones complicadas, como se hacía antes en los artículos académicos.

Pero para entender realmente cómo aprenden las redes, es importante saber qué ocurre detrás de escena. Por eso, en esta sección vamos a estudiar backpropagation, el mecanismo que calcula cómo cambia cada parámetro según el error cometido. Usaremos matemáticas básicas y grafos computacionales, trabajando con un ejemplo sencillo: un MLP de una capa oculta con regularización (weight decay $l_2$). Esto nos permitirá entender no sólo el qué, sino el cómo del aprendizaje profundo.


### Forward Propagation

La propagación hacia adelante (forward propagation o pase hacia adelante) se refiere al cálculo y almacenamiento de variables intermedias (incluidas las salidas) para una red neuronal, desde la capa de entrada hasta la capa de salida. A continuación, se explica paso a paso la mecánica de una red neuronal con una capa oculta. 

Para simplificar, supongamos que el ejemplo de entrada es $x\in \mathbb{R}^{d}$ y que nuestra capa oculta no incluye un término de sesgo. En este caso, la variable intermedia es:

$$\mathbb{z}=W^{(1)}x,$$

donde $W^{(1)} \in \mathbb{R}^{h\times d}$ es el peso del parámetro de las capas ocultas. Luego de ejecutar la variable intermedia $z\in \mathbb{R}^{h}$ a través de la función de activación $\phi$, se obtiene el vector de activación oculto de longitud $h$:

$$h=\phi(z).$$

La salida de la capa oculta $h$ también es una variable intermedia. Suponiendo que los parámetros de la capa de salida solo tienen un peso de $W^{(2)}\in \mathbb{R}^{q\times h}$, se puede obtener una variable de capa de salida con un vector de longitud $q$:

$$\mathbb{o}=W^{(2)}h$$

Asumiendo que la función de pérdida es $l$ y la etiqueta de ejemplo es $y$, se puede calcular el término de pérdida para un solo ejemplo de datos,

$$L=l(o,y)$$

Como veremos en la definición de regularización ($l_2$) que se presentará más adelante, dado el hiperparámetro $\lambda$, el término de regularización es:

$$s=\frac{\lambda}{2} \left(||W^{(1)}||^{2}_{F}+||W^{(2)}||^{2}_{F} \right),$$

Donde la *norma de Frobenius* de la matriz es simplemente la norma $l_2$ aplicada tras aplanar la matriz a un vector. Finalmente, la pérdida regularizada del modelo en un ejemplo de datos dado es:

$$J=L+s.$$

Refiriéndose a $J$ como la *función objetivo* en la siguiente discusión. A continuación se muestra el flujo computacional del proceso **Forward**.

```{r echo=FALSE, fig.align='center', out.height='300pt', out.width='650pt', fig.cap = "Computational graph of forward propagation of Dive into Deep Learning Book"}
knitr::include_graphics("img/05_Multilayer_Perceptron/computational_grapg_forward_propagation.png")
```



### Backpropagation

La retropropagación se refiere al método para calcular el gradiente de los parámetros de una red neuronal. En resumen, el método recorre **la red en orden inverso**, desde la capa de salida hasta la de entrada, según la **regla de la cadena** del cálculo. El algoritmo almacena las variables intermedias (derivadas parciales) necesarias para calcular el gradiente con respecto a algunos parámetros. Supongamos que tenemos funciones $Y=f(X)$ y $Z=g(Y)$, donde la entrada y la salida $X, Y, Z$ son tensores de formas arbitrarias. Utilizando la regla de la cadena, podemos calcular la derivada de $Z$ con respecto a $X$ mediante:

$$\frac{\partial Z}{\partial X} = \text{prod} \left( \frac{\partial Z}{\partial Y}, \frac{\partial Y}{\partial X} \right)$$

Aquí usamos el operador $prod$ para multiplicar sus argumentos después de realizar las operaciones necesarias, como la transposición y el intercambio de posiciones de entrada. Para vectores, esto es sencillo: se trata simplemente de una multiplicación matriz-matriz. Para tensores de mayor dimensión, usamos el operador correspondiente. El operador $prod$ elimina toda la sobrecarga de notación.

```{r echo=FALSE, fig.align='center', out.height='300pt', out.width='650pt'}
knitr::include_graphics("img/05_Multilayer_Perceptron/Gradient_descent.gif")
```

El objetivo de la retropropagación es calcular los gradientes $\frac{\partial J}{\partial W^{(1)}}$ y $\frac{\partial J}{\partial W^{(2)}}$. Para ello, aplicamos la regla de la cadena y calculamos, a su vez, el gradiente de cada variable y parámetro intermedio. El orden de los cálculos se invierte con respecto a los realizados en la propagación hacia adelante, ya que debemos comenzar con el resultado del grafo computacional y avanzar hacia los parámetros. El primer paso es calcular los gradientes de la función objetivo $J=L+s$ con respecto al término de pérdida $L$ y al término de regularización $s$:

$$
\frac{\partial J}{\partial L} = 1 \quad \text{and} \quad\frac{\partial J}{\partial s}=1
$$
A continuación, calculamos el gradiente de la función objetivo con respecto a la variable de la capa de salida $o$ según la regla de la cadena:

$$
\frac{\partial J}{\partial o}=\text{prod}\left( \frac{\partial J}{\partial L}, \frac{\partial L}{\partial o} \right) = \frac{\partial J}{\partial o}\in \mathbb{R}^{q}.
$$
A continuación, calculamos los gradientes del término de regularización con respecto a ambos parámetros:

$$
\frac{\partial s}{\partial W^{(1)}}=\lambda W^{(1)} \quad and \quad \frac{\partial s}{\partial W^{(2)}}=\lambda W^{(2)}
$$
Ahora podemos calcular el gradiente $\frac{\partial J}{\partial W^{(2)}}$ de los parámetros del modelo más cercanos a la capa de salida. Usando la regla de la cadena obtenemos:

$$
\frac{\partial J}{\partial W^{(2)}}=prod\left(\frac{\partial J}{\partial o}, \frac{\partial o}{\partial W^{(2)}}\right)+prod\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial W^{(2)}}\right)=\frac{\partial J}{\partial o}h^{T}+\lambda W^{(2)}
$$

Para obtener el gradiente con respecto a $W^{(1)}$, necesitamos continuar la retropropagación a lo largo de la capa de salida hasta la capa oculta. El gradiente con respecto a la salida de la capa oculta $\frac{\partial J}{\partial h}\in \mathbb{R}^{h}$ está dado por:

$$
\frac{\partial J}{\partial h}=prod\left(\frac{\partial J}{\partial o}, \frac{\partial o}{\partial h}\right)=W^{(2)^{T}}\frac{\partial J}{\partial o}
$$
Dado que la función de activación $\phi$ se aplica elemento por elemento, para calcular el gradiente $\frac{\partial J}{\partial z} \in \mathbb{R}^{h}$ de la variable intermedia $z$ es necesario utilizar el operador de multiplicación elemento por elemento, que denotamos como $\odot$:

$$
\frac{\partial J}{\partial z}=prod\left(\frac{\partial J}{\partial h}, \frac{\partial h}{\partial z}\right)=\frac{\partial J}{\partial h}\odot\phi^{\prime}(z).
$$
Finalmente, podemos obtener el gradiente $\frac{\partial J}{\partial W^{(1)}}\in \mathbb{R}^{h\times d}$ de los parámetros del modelo más cercanos a la capa de entrada. Según la regla de la cadena, obtenemos:

$$
\frac{\partial J}{\partial W^{(1)}}=prod\left(\frac{\partial J}{\partial z}, \frac{\partial z}{\partial W^{(1)}}\right)+prod\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial W^{(1)}}\right)=\frac{\partial J}{\partial z}x^{T}+\lambda W^{(1)}
$$

Al entrenar redes neuronales, la propagación hacia adelante y la retropropagación se necesitan mutuamente. En la fase hacia adelante se recorre el grafo computacional según sus dependencias y se calculan variables intermedias que luego serán reutilizadas en la retropropagación.

En una red simple, por ejemplo, el término de regularización depende de los parámetros actuales $W^{(1)}$ y $W^{(2)}$, actualizados previamente mediante retropropagación. A su vez, el cálculo de gradientes en la retropropagación depende de valores generados hacia adelante, como la salida oculta $h&.

En resumen, el entrenamiento alterna continuamente entre ambas fases: la propagación hacia adelante produce valores intermedios, la retropropagación usa esos valores para obtener gradientes y actualizar parámetros, y esos parámetros actualizados se usan en la siguiente pasada hacia adelante. Esto requiere almacenar los valores intermedios hasta que termine la retropropagación.


## Estabilidad Numérica e Inicialización

## Generalización en Deep Learning

## Dropout

## Ejemplo















