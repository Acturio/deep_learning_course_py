---
output:
  html_document: default
  pdf_document: default
---
<!-- ::: watermark -->
<!-- <img src="img/aif-logo.png" width="400"/> -->
<!-- ::: -->

# Embeddings y Graph Neural Network
## ¿Qué es un embedding?

En el aprendizaje automático tradicional, las categorías (como nombres de
ciudades, tipos de átomos o IDs de usuarios) se representaban mediante representaciones
numéricas como One-Hot Encoding.

Aunque de utilidad, en la práctica dicha soluciones padecen de algunos problemas:
i) **incapacidad semántica**: en el caso palabras, las representaciones de pareja
que tiene un significado cercano, sus encondings se pueden encontrar muy alejados
(e.g. El vector de "perro" [1,0,0] y el de "cachorro" [0,1,0] son ortogonales;
no tienen ninguna relación); ii) **explosión de memoria**:al tener muchos elementos,
cada vector puede tener un número grande dimensiones, pero que la mayoría sean de ceros (sparse).

Para dar solucion a este tema, se emplean los embeddings, los cuale son
representaciones densas de datos discretos. Es decir, para representar los puntos
consolidamos una transformacion de los datos hacia un espacio, que en 
lugar de crear un vector gigante de ceros y unos, representa a cada elemento
como un vector de números reales de tamaño fijo (por ejemplo, 128 o 300 dimensiones).

La idea esencial es proyectar los puntos en un espacio continuo de baja dimensión donde la
la similitud de los puntos (e.g semantica, geométrica, estructural) se conserve
geometricamente. Es decir puntos que son parecidos en el espacio original,
tengan una representacion cercana bajo dicha función.

En Deep Learning, los embeddding ajustando los pesos pesos de capaz de unaa red neuronal.
La red comienza con vectores aleatorios y a medida que la red intenta resolver 
una tarea (e.g. predecir la siguiente palabra, image o clasificar un nodo), se
ajustan la representación vectorial.

Al final, elementos que aparecen en contextos similares terminan "empujados"
hacia la misma zona en el espacio vectorial.

Para ejemplificarlo podemos construir embeddings para los elementos
del Dataset Iris.

```{python}
import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA

# Datos
iris = load_iris()
X = torch.tensor(iris.data, dtype=torch.float32)
y = torch.tensor(iris.target, dtype=torch.long)

dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

# Modelo
class Classifier(nn.Module):
    def __init__(self, input_dim=4, emb_dim=8, num_classes=3):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, emb_dim),
            nn.ReLU()
        )
        self.classifier = nn.Linear(emb_dim, num_classes)

    def forward(self, x):
        z = self.encoder(x)
        return self.classifier(z), z

model = Classifier()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()

# Entrenamiento breve
for _ in range(100):
    for xb, yb in loader:
        optimizer.zero_grad()
        logits, _ = model(xb)
        loss = criterion(logits, yb)
        loss.backward()
        optimizer.step()
    if epoch % 10 == 0:
      print(f"Epoch {epoch:>3} | Loss: {loss:.5f}")

# Obtener embeddings
with torch.no_grad():
    embeddings = model.encoder(X).numpy()

print("Embeddings: ")
embeddings[:2]
```

Ahora usaremos PCA para obtener la representación de dichos puntos:

```{python}
# PCA a 2 dimensiones
pca = PCA(n_components=2)
emb_2d = pca.fit_transform(embeddings)

# Plot (una sola figura)
plt.figure()
scatter = plt.scatter(
    emb_2d[:, 0],
    emb_2d[:, 1],
    c=y
)
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.title("Embedding del dataset Iris (PyTorch + PCA)")

# Leyenda correcta
handles, _ = scatter.legend_elements()
plt.legend(handles, iris.target_names, title="Clase")

plt.show()

```
Matematicamente, si tenemos V elementos y queremos vectores de tamaño d,
un embedding es una matriz $E$ que transforma los datos de entrada de la siguiente
forma:

\begin{equation}
E \in \mathbb{R}^{V \times d}
\end{equation}

En donde
* $E$: Matriz de parámetros aprendibles (Embedding Layer).
* $V$: Tamaño del vocabulario o número total de entidades.
* $d$: Dimensionalidad del espacio latente (hiperparámetro).
* $e_1 \cdot e_2$: Producto punto, que mide la dirección común de los vectores.
* $\|e\|$: Norma euclidiana que normaliza el vector para ignorar su magnitud y enfocarse en la orientación.

Para obtener el vector $e_i$ de un elemento con índice $i$, multiplicamos un vector $\textit{one-hot} $x_i$
por la matriz:

\begin{equation}
e_i = x_i^\top E
\end{equation}

Un forma común de saber qué tan parecidos son dos embeddings es mediante el coseno del ángulo entre ellos:

\begin{equation}
\text{sim}(e_1, e_2) = \frac{e_1 \cdot e_2}{\|e_1\| \|e_2\|}
\end{equation}

Los embedding han tenido mucha adopcion en diversas aplicaciones de Deep Learning:

* NLP: Word2Vec, GloVe y los embeddings de Transformers (BERT/GPT) para entender el lenguaje.
* Sistemas de Recomendación: Representar usuarios y productos en el mismo espacio para medir su afinidad.
* Visión por Computador: Face Embeddings para reconocimiento facial (si dos fotos generan vectores cercanos, son la misma persona).
* GNNs: Todos los modelos previos (GCN, GAT, SAGE) generan, en última instancia, Node Embeddings.

## Word2Vec

WIP


## Node2Vec

Antes de que las GNN fueran el estándar, el reto era: ¿Cómo convertimos un nodo
de un grafo en un vector de números (embedding) que una red neuronal tradicional
pueda entender?

Como hemos mencionado previamente, en el área de procesamiento de lenguaje natural
se propuso al modelo *word2vec* en donde se construye un embedding de palabras
usando la idea que las palabras que aparecen en contextos similares deben
induceir vectores similares. Node2vec se construye con ideas similares: para
obtener nodos en contexto similares se ejecutan caminatas aleatorias sobre
la gráfica  (random walks), de forma que podemos tratar un grafo como un lenguaje.

Node2vec permite dicho el algoritmo explore el grafo de forma más flexible,
capturando tanto comunidades locales como roles estructurales. Dicha exploración
se basa en generar "paseos aleatorios" con cierto nivel de sesgo (biased random
walk) desde cada nodo y luego alimentar esos paseos a un modelo Skip-gram (el 
mismo de word2vec).

Las caminatas aleatorias en este enfoque tiene dos hiper-parámetros para explorar
la gráfica mediante caminan que obtienen nodos usando trayectorias que usan
información de los nodos visitados:

* **Parámetro de Retorno (p):** Controla la probabilidad de regresar inmediatamente al
nodo anterior. Un p bajo favorece la exploración local (BFS - Breadth-First Search),
capturando la similitud estructural (nodos que actúan como "hubs").

* **Parámetro de "In-out" (q):** Controla la probabilidad de alejarse hacia nodos no visitados.
Un q bajo favorece la exploración profunda (DFS - Depth-First Search), capturando comunidades o macro-estructuras.

El proceso de optimización

El objetivo es maximizar la probabilidad de co-ocurrencia de nodos en una vecindad:

Donde f(u) es la función de mapeo (el embedding) que queremos aprender para el nodo u.


\begin{equation}
\mathcal{L} = \sum_{w \in \mathcal{W}} - \log \left(\sigma(\mathbf{z}_v^{\top} \mathbf{z}_w) \right) + \sum_{w \sim \mathcal{V} \setminus \mathcal{W}} - \log \left( 1 - \sigma(\mathbf{z}_v^{\top} \mathbf{z}_w) \right),
\end{equation}



```{python}
import random
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
from sklearn.decomposition import PCA
from gensim.models import KeyedVectors, Word2Vec


class Graph():
    def __init__(self, nx_G, is_directed, p, q):
        self.G = nx_G
        self.is_directed = is_directed
        self.p = p
        self.q = q

    def node2vec_walk(self, walk_length, start_node):
        """
        Simulate a random walk starting from start node.
        """
        G = self.G
        alias_nodes = self.alias_nodes
        alias_edges = self.alias_edges

        walk = [start_node]

        while len(walk) < walk_length:
            cur = walk[-1]
            cur_nbrs = sorted(G.neighbors(cur))
            if len(cur_nbrs) > 0:
                if len(walk) == 1:
                    walk.append(
                        cur_nbrs[
                            alias_draw(
                                alias_nodes[cur][0],
                                alias_nodes[cur][1]
                            )
                        ]
                    )
                else:
                    prev = walk[-2]
                    next_node = cur_nbrs[
                        alias_draw(
                            alias_edges[(prev, cur)][0],
                            alias_edges[(prev, cur)][1]
                        )
                    ]
                    walk.append(next_node)
            else:
                break

        return walk

    def simulate_walks(self, num_walks, walk_length):
        """
        Repeatedly simulate random walks from each node.
        """
        G = self.G
        walks = []
        nodes = list(G.nodes())
        print('Walk iteration:')
        for walk_iter in range(num_walks):
            print(f'{walk_iter + 1} / {num_walks}')
            random.shuffle(nodes)
            for node in nodes:
                walks.append(
                    self.node2vec_walk(
                        walk_length=walk_length,
                        start_node=node
                    )
                )
        return walks

    def get_alias_edge(self, src, dst):
        """
        Get the alias edge setup lists for a given edge.
        """
        G = self.G
        p = self.p
        q = self.q

        unnormalized_probs = []
        for dst_nbr in sorted(G.neighbors(dst)):
            weight = G[dst][dst_nbr].get('weight', 1.0)

            if dst_nbr == src:
                unnormalized_probs.append(weight / p)
            elif G.has_edge(dst_nbr, src):
                unnormalized_probs.append(weight)
            else:
                unnormalized_probs.append(weight / q)

        norm_const = sum(unnormalized_probs)
        normalized_probs = [
            float(u_prob) / norm_const for u_prob in unnormalized_probs
        ]

        return alias_setup(normalized_probs)

    def preprocess_transition_probs(self):
        """
        Preprocessing of transition probabilities for guiding the random walks.
        """
        G = self.G
        is_directed = self.is_directed

        alias_nodes = {}
        for node in G.nodes():
            unnormalized_probs = [
                G[node][nbr].get('weight', 1.0)
                for nbr in sorted(G.neighbors(node))
            ]
            norm_const = sum(unnormalized_probs)
            normalized_probs = [
                float(u_prob) / norm_const for u_prob in unnormalized_probs
            ]
            alias_nodes[node] = alias_setup(normalized_probs)

        alias_edges = {}

        if is_directed:
            for edge in G.edges():
                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])
        else:
            for edge in G.edges():
                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])
                alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])

        self.alias_nodes = alias_nodes
        self.alias_edges = alias_edges


def alias_setup(probs):
    """
    Compute utility lists for non-uniform sampling from discrete distributions.
    """
    K = len(probs)
    q = np.zeros(K)
    J = np.zeros(K, dtype=int)

    smaller = []
    larger = []

    for kk, prob in enumerate(probs):
        q[kk] = K * prob
        if q[kk] < 1.0:
            smaller.append(kk)
        else:
            larger.append(kk)

    while smaller and larger:
        small = smaller.pop()
        large = larger.pop()

        J[small] = large
        q[large] = q[large] + q[small] - 1.0

        if q[large] < 1.0:
            smaller.append(large)
        else:
            larger.append(large)

    return J, q


def alias_draw(J, q):
    """
    Draw sample from a non-uniform discrete distribution using alias sampling.
    """
    K = len(J)
    kk = int(np.floor(np.random.rand() * K))

    if np.random.rand() < q[kk]:
        return kk
    else:
        return J[kk]
```

```{python}
def learn_embeddings(
    walks,
    vector_size: int = 128,
    window: int = 10,
    workers: int = 8,
    epochs: int = 1
):
    """
    Learn embeddings by optimizing the Skip-gram objective using SGD.
    """

    # gensim requires lists, not iterators
    walks = [list(map(str, walk)) for walk in walks]

    model = Word2Vec(
        sentences=walks,
        vector_size=vector_size,
        window=window,
        min_count=0,
        sg=1,              # skip-gram
        workers=workers,
        epochs=epochs
    )

    # save in word2vec format (node2vec-compatible)
    model.wv.save_word2vec_format("embeddings.emb")

    return model

```

```{python}
import networkx as nx

nx_G = nx.karate_club_graph()
G = Graph(nx_G, is_directed=False, p=0.8, q=0.1)

G.preprocess_transition_probs()
walks = G.simulate_walks(num_walks=20, walk_length=10)

```

```{python}
walks[1]
```

```{python}
model = learn_embeddings(walks,epochs=100)
```

```{python}
def load_embeddings(path="embeddings.emb"):
    kv = KeyedVectors.load_word2vec_format(path)
    nodes = kv.index_to_key
    X = np.array([kv[node] for node in nodes])
    return nodes, X


def plot_embeddings_pca(nodes, X, graph):
    pca = PCA(n_components=2, random_state=42)
    X_2d = pca.fit_transform(X)

    club_to_color = {
        "Mr. Hi": "tab:blue",
        "Officer": "tab:orange"
    }

    colors = [
        club_to_color[graph.nodes[int(node)]["club"]]
        for node in nodes
    ]

    plt.figure(figsize=(12, 10))
    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=colors, s=80, alpha=0.85)

    for i, node in enumerate(nodes):
        plt.text(
            X_2d[i, 0] + 0.01,
            X_2d[i, 1] + 0.01,
            node,
            fontsize=9
        )

    for club, color in club_to_color.items():
        plt.scatter([], [], c=color, label=club)

    plt.legend(title="Karate Club Group")
    plt.title("Node2Vec Embeddings (PCA)")
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()


# usage
G = nx.karate_club_graph()
nodes, X = load_embeddings("embeddings.emb")
plot_embeddings_pca(nodes, X, G)
```

```{python}
import networkx as nx
import matplotlib.pyplot as plt


# graph
G = nx.karate_club_graph()

# color by club
club_to_color = {
    "Mr. Hi": "tab:blue",
    "Officer": "tab:orange"
}

node_colors = [
    club_to_color[G.nodes[n]["club"]]
    for n in G.nodes()
]

# layout
pos = nx.spring_layout(G, seed=42)

# plot
plt.figure(figsize=(12, 10))
nx.draw_networkx_nodes(
    G,
    pos,
    node_color=node_colors,
    node_size=600,
    alpha=0.9
)
nx.draw_networkx_edges(G, pos, alpha=0.4)
nx.draw_networkx_labels(G, pos, font_size=9)
```


# Shallow Embeddinf

```{r echo=FALSE, fig.align='center', out.width="120%"}
knitr::include_graphics("img/21-gnn/shallow_node_embeddings.png")
```

La diferencia clave entre las incrustaciones de nodos superficiales
(p. ej., Node2Vec) y las incrustaciones de nodos profundos (p. ej., GNN) reside
en la elección del codificador. Específicamente, las técnicas de incrustación de
nodos superficiales se basan en la incrustación de nodos en representaciones 
vectoriales de baja dimensión mediante una tabla de búsqueda de incrustaciones
superficiales, de modo que se maximiza la probabilidad de preservar las
vecindades; es decir, los nodos cercanos deberían recibir incrustaciones similares,
mientras que los nodos distantes deberían recibir incrustaciones distintas. Estas
técnicas generalizan el famoso modelo SkipGram para obtener incrustaciones de
palabras de baja dimensión, en el que las secuencias de palabras se interpretan
ahora como secuencias de nodos, p. ej., dadas mediante recorridos generados
aleatoriamente:

Específicamente, dado un recorrido aleatorio de longitud que comienza en el nodo,
el objetivo es maximizar la probabilidad de observar el nodo dado el nodo. 

Este objetivo se puede entrenar eficientemente mediante el descenso de gradiente
estocástico en un escenario de aprendizaje contrastivo, en el que los recorridos
inexistentes (denominados ejemplos negativos) se muestrean y entrenan conjuntamente,
lo que denota la función. Cabe destacar que el producto escalar entre las 
incrustaciones se utiliza habitualmente para medir la similitud, pero también son
aplicables otras medidas de similitud.

Es importante destacar que las incrustaciones de nodos superficiales se entrenan
de forma no supervisada y pueden utilizarse como entrada para una tarea posterior
determinada; por ejemplo, en tareas a nivel de nodo, pueden utilizarse directamente
como entrada para un clasificador final. Para tareas a nivel de borde, las
representaciones a nivel de borde se pueden obtener mediante el promedio o el
producto de Hadamard.

A pesar de la simplicidad de las técnicas de incrustación de nodos, también presentan
ciertas deficiencias. En particular, no incorporan información rica de características
asociada a nodos y aristas, y no pueden aplicarse fácilmente a grafos no visibles,
ya que los parámetros aprendibles están fijados a los nodos de un grafo en particular
(lo que hace que este enfoque sea transductivo por naturaleza y difícil de escalar
debido a la complejidad de los parámetros). Sin embargo, sigue siendo una técnica
común para preservar la información estructural del grafo en vectores de tamaño fijo,
y a menudo también se utiliza para generar entradas a las GNN para su posterior
procesamiento en caso de que el conjunto inicial de características de los nodos
no sea rico.





# SageConv

La mayoría de los modelos anteriores (como GCN o node2vec) asumen que el grafo
es estático y que todos los nodos están presentes durante el entrenamiento. Esto
se conoce como aprendizaje transductivo.

El problema: En el mundo real (como en la red de Amazon o Twitter), el grafo 
cambia cada segundo.

Si llega un usuario nuevo, los modelos transductivos no saben qué hacer; tendrías
que reentrenar todo el modelo desde cero para generar un embedding para ese nuevo nodo.

GraphSAGE (SAGE viene de SAmple and aggreGatE) fue diseñado para ser inductivo:
aprende una función de agregación, no un embedding fijo. Esto le permite generar
representaciones para nodos que nunca vio durante el entrenamiento.

2. Ideas Generales: ¿Cómo funciona SAGEConv?
A diferencia de las GCN que utilizan la matriz de adyacencia completa, SAGEConv
opera mediante un proceso de muestreo de vecindad.

Los dos pilares de GraphSAGE:

Muestreo (Sampling): En lugar de usar todos los vecinos (que podrían ser miles),
SAGEConv selecciona un subconjunto aleatorio de tamaño fijo. Esto hace que el
tiempo de cómputo sea predecible y constante.

Agregación (Aggregation): SAGEConv no se limita a un promedio. Permite usar
diferentes funciones para "resumir" la información de los vecinos:

Mean aggregator: Promedio de los vectores.

LSTM aggregator: Usa una red LSTM (aplicada a una permutación aleatoria de los vecinos).

Pooling aggregator: Aplica una red densa seguida de un operador de máximo (MAX).



Paso 1: Agregación de la vecindad

Primero, se recolecta la información de los vecinos muestreados:

Fragmento de código
\begin{equation}
h_{\mathcal{N}(i)}^{(k)} = \text{aggregate}_k \left( \{ h_j^{(k-1)}, \forall j \in \mathcal{N}(i) \} \right)
\end{equation}
Paso 2: Combinación y actualización

Luego, se concatena la información agregada con la representación actual del nodo y se proyecta:

Fragmento de código
\begin{equation}
h_i^{(k)} = \sigma \left( \mathbf{W}^{(k)} \cdot \text{concat}(h_i^{(k-1)}, h_{\mathcal{N}(i)}^{(k)}) \right)
\end{equation}
Desglose de los componentes:

Fragmento de código
* $h_{\mathcal{N}(i)}^{(k)}$: Representación agregada de los vecinos del nodo $i$ en la capa $k$.
*$\text{aggregate}_k$: Función de agregación (puede ser \textit{Mean, Pooling} o \textit{LSTM}).
* $\text{concat}(\cdot, \cdot)$: Operación de concatenación que preserva la identidad del nodo central diferenciándola de su contexto.
* $\mathbf{W}^{(k)}$: Matriz de pesos aprendible de la capa $k$.
* $\sigma$: Función de activación no lineal.

## Implementación en PyTorch Geometric

### Ejemplo: Clasificación de artículos de investigación por categoría (Parte II)

La arquitecura del ejemplo es la siguiente:

```{r echo=FALSE, fig.align='center', out.width="120%"}
knitr::include_graphics("img/21-gnn/link_prediction_example.png")
```


```{python}
import torch
from torch import nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv
from torch_geometric.utils import negative_sampling
from torch_geometric.loader import DataLoader
from tqdm.notebook import tqdm

```


```{python}

```


```{python}

```


```{python}

```


```{python}

```






