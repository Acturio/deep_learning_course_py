<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>CapÃ­tulo 4 Redes neuronales Lineales para ClasificaciÃ³n | Deep Learning</title>
  <meta name="description" content="CapÃ­tulo 4 Redes neuronales Lineales para ClasificaciÃ³n | Deep Learning" />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="CapÃ­tulo 4 Redes neuronales Lineales para ClasificaciÃ³n | Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="CapÃ­tulo 4 Redes neuronales Lineales para ClasificaciÃ³n | Deep Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="CapÃ­tulo 4 Redes neuronales Lineales para ClasificaciÃ³n | Deep Learning" />
  
  <meta name="twitter:description" content="CapÃ­tulo 4 Redes neuronales Lineales para ClasificaciÃ³n | Deep Learning" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="redes-neuronales-lineales-para-regresiÃ³n.html"/>
<link rel="next" href="perceptrÃ³n-multicapa.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="img/aif-logo.png" width="280"></a></li|
|:-:|  
<center>Curso de Redes Neuronales Artificiales</center>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>BIENVENIDA</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#objetivo"><i class="fa fa-check"></i>Objetivo</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#alcances-del-programa"><i class="fa fa-check"></i>Alcances del Programa</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#temario"><i class="fa fa-check"></i>Temario:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#c%C3%B3digo"><i class="fa fa-check"></i>CÃ³digo</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#duraci%C3%B3n-y-evaluaci%C3%B3n-del-programa"><i class="fa fa-check"></i>DuraciÃ³n y evaluaciÃ³n del programa</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recursos-y-din%C3%A1mica"><i class="fa fa-check"></i>Recursos y dinÃ¡mica</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agenda"><i class="fa fa-check"></i>Agenda</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bibliograf%C3%ADa"><i class="fa fa-check"></i>BibliografÃ­a</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Parte 1: Bases y Preeliminares</b></span></li>
<li class="chapter" data-level="1" data-path="introducciÃ³n-a-deep-learning.html"><a href="introducciÃ³n-a-deep-learning.html"><i class="fa fa-check"></i><b>1</b> IntroducciÃ³n a Deep Learning</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introducciÃ³n-a-deep-learning.html"><a href="introducciÃ³n-a-deep-learning.html#introducci%C3%B3n-al-aprendizaje-autom%C3%A1tico"><i class="fa fa-check"></i><b>1.1</b> IntroducciÃ³n al aprendizaje automÃ¡tico</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introducciÃ³n-a-deep-learning.html"><a href="introducciÃ³n-a-deep-learning.html#motivaci%C3%B3n-los-paradigmas-del-conocimiento"><i class="fa fa-check"></i><b>1.1.1</b> MotivaciÃ³n: Los paradigmas del conocimiento</a></li>
<li class="chapter" data-level="1.1.2" data-path="introducciÃ³n-a-deep-learning.html"><a href="introducciÃ³n-a-deep-learning.html#machine-learning-supervisado"><i class="fa fa-check"></i><b>1.1.2</b> Machine learning supervisado</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introducciÃ³n-a-deep-learning.html"><a href="introducciÃ³n-a-deep-learning.html#por-qu%C3%A9-deep-learning"><i class="fa fa-check"></i><b>1.2</b> Â¿Por quÃ© Deep Learning?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introducciÃ³n-a-deep-learning.html"><a href="introducciÃ³n-a-deep-learning.html#dimensi%C3%B3n-vc"><i class="fa fa-check"></i><b>1.2.1</b> DimensiÃ³n VC</a></li>
<li class="chapter" data-level="1.2.2" data-path="introducciÃ³n-a-deep-learning.html"><a href="introducciÃ³n-a-deep-learning.html#el-truco-del-kernel-svm"><i class="fa fa-check"></i><b>1.2.2</b> El truco del Kernel (SVM)</a></li>
<li class="chapter" data-level="1.2.3" data-path="introducciÃ³n-a-deep-learning.html"><a href="introducciÃ³n-a-deep-learning.html#ingenier%C3%ADa-de-caracteristicas"><i class="fa fa-check"></i><b>1.2.3</b> IngenierÃ­a de caracteristicas</a></li>
<li class="chapter" data-level="1.2.4" data-path="introducciÃ³n-a-deep-learning.html"><a href="introducciÃ³n-a-deep-learning.html#beneficios-del-deep-learning"><i class="fa fa-check"></i><b>1.2.4</b> Beneficios del deep learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="preliminares.html"><a href="preliminares.html"><i class="fa fa-check"></i><b>2</b> Preliminares</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminares.html"><a href="preliminares.html#algebra-lineal"><i class="fa fa-check"></i><b>2.1</b> Algebra Lineal</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="preliminares.html"><a href="preliminares.html#introducci%C3%B3n"><i class="fa fa-check"></i><b>2.1.1</b> IntroducciÃ³n</a></li>
<li class="chapter" data-level="2.1.2" data-path="preliminares.html"><a href="preliminares.html#motivaci%C3%B3n"><i class="fa fa-check"></i><b>2.1.2</b> MotivaciÃ³n</a></li>
<li class="chapter" data-level="2.1.3" data-path="preliminares.html"><a href="preliminares.html#escalares"><i class="fa fa-check"></i><b>2.1.3</b> Escalares</a></li>
<li class="chapter" data-level="2.1.4" data-path="preliminares.html"><a href="preliminares.html#vectores-1"><i class="fa fa-check"></i><b>2.1.4</b> Vectores</a></li>
<li class="chapter" data-level="2.1.5" data-path="preliminares.html"><a href="preliminares.html#matrices-1"><i class="fa fa-check"></i><b>2.1.5</b> Matrices</a></li>
<li class="chapter" data-level="2.1.6" data-path="preliminares.html"><a href="preliminares.html#tensores"><i class="fa fa-check"></i><b>2.1.6</b> Tensores</a></li>
<li class="chapter" data-level="2.1.7" data-path="preliminares.html"><a href="preliminares.html#reducciones-sobre-tensores"><i class="fa fa-check"></i><b>2.1.7</b> Reducciones sobre tensores</a></li>
<li class="chapter" data-level="2.1.8" data-path="preliminares.html"><a href="preliminares.html#producto-punto"><i class="fa fa-check"></i><b>2.1.8</b> Producto punto</a></li>
<li class="chapter" data-level="2.1.9" data-path="preliminares.html"><a href="preliminares.html#producto-entre-matrices-y-vectores"><i class="fa fa-check"></i><b>2.1.9</b> Producto entre matrices y vectores</a></li>
<li class="chapter" data-level="2.1.10" data-path="preliminares.html"><a href="preliminares.html#multiplicaci%C3%B3n-matricial"><i class="fa fa-check"></i><b>2.1.10</b> MultiplicaciÃ³n matricial</a></li>
<li class="chapter" data-level="2.1.11" data-path="preliminares.html"><a href="preliminares.html#normas"><i class="fa fa-check"></i><b>2.1.11</b> Normas</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="preliminares.html"><a href="preliminares.html#c%C3%A1lculo-diferencial"><i class="fa fa-check"></i><b>2.2</b> CÃ¡lculo Diferencial</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="preliminares.html"><a href="preliminares.html#introducci%C3%B3n-1"><i class="fa fa-check"></i><b>2.2.1</b> IntroducciÃ³n</a></li>
<li class="chapter" data-level="2.2.2" data-path="preliminares.html"><a href="preliminares.html#motivaci%C3%B3n-1"><i class="fa fa-check"></i><b>2.2.2</b> MotivaciÃ³n</a></li>
<li class="chapter" data-level="2.2.3" data-path="preliminares.html"><a href="preliminares.html#derivadas-y-diferenciaci%C3%B3n"><i class="fa fa-check"></i><b>2.2.3</b> Derivadas y diferenciaciÃ³n</a></li>
<li class="chapter" data-level="2.2.4" data-path="preliminares.html"><a href="preliminares.html#regla-de-la-cadena"><i class="fa fa-check"></i><b>2.2.4</b> Regla de la cadena</a></li>
<li class="chapter" data-level="2.2.5" data-path="preliminares.html"><a href="preliminares.html#interpretaci%C3%B3n-geom%C3%A9trica-de-la-derivada"><i class="fa fa-check"></i><b>2.2.5</b> InterpretaciÃ³n geomÃ©trica de la derivada</a></li>
<li class="chapter" data-level="2.2.6" data-path="preliminares.html"><a href="preliminares.html#teorema-fundamental-del-c%C3%A1lculo"><i class="fa fa-check"></i><b>2.2.6</b> Teorema fundamental del cÃ¡lculo</a></li>
<li class="chapter" data-level="2.2.7" data-path="preliminares.html"><a href="preliminares.html#autodiferenciaci%C3%B3n"><i class="fa fa-check"></i><b>2.2.7</b> AutodiferenciaciÃ³n</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html"><i class="fa fa-check"></i><b>3</b> Redes neuronales Lineales para RegresiÃ³n</a>
<ul>
<li class="chapter" data-level="3.1" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#introducci%C3%B3n-2"><i class="fa fa-check"></i><b>3.1</b> IntroducciÃ³n</a></li>
<li class="chapter" data-level="3.2" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#regresiones-lineales-simples"><i class="fa fa-check"></i><b>3.2</b> Regresiones lineales simples</a></li>
<li class="chapter" data-level="3.3" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#funci%C3%B3n-de-p%C3%A9rdida-y-funci%C3%B3n-de-costo"><i class="fa fa-check"></i><b>3.3</b> FunciÃ³n de pÃ©rdida y funciÃ³n de costo</a></li>
<li class="chapter" data-level="3.4" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#supuestos-en-la-regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>3.4</b> Supuestos en la regresiÃ³n lineal</a></li>
<li class="chapter" data-level="3.5" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#regresi%C3%B3n-lineal-m%C3%BAltiple"><i class="fa fa-check"></i><b>3.5</b> RegresiÃ³n Lineal MÃºltiple</a></li>
<li class="chapter" data-level="3.6" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#estimaci%C3%B3n-de-los-par%C3%A1metros"><i class="fa fa-check"></i><b>3.6</b> EstimaciÃ³n de los parÃ¡metros</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#derivaci%C3%B3n-paso-a-paso"><i class="fa fa-check"></i><b>3.6.1</b> DerivaciÃ³n paso a paso</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#bondad-de-ajuste"><i class="fa fa-check"></i><b>3.7</b> Bondad de ajuste</a></li>
<li class="chapter" data-level="3.8" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#supuestos-del-modelo-lineal-m%C3%BAltiple"><i class="fa fa-check"></i><b>3.8</b> Supuestos del modelo lineal mÃºltiple</a></li>
<li class="chapter" data-level="3.9" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#regularizaci%C3%B3n-en-la-regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>3.9</b> RegularizaciÃ³n en la RegresiÃ³n Lineal</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#ridge-regression"><i class="fa fa-check"></i><b>3.9.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="3.9.2" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#lasso-regression"><i class="fa fa-check"></i><b>3.9.2</b> Lasso Regression</a></li>
<li class="chapter" data-level="3.9.3" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#elastic-net-regression"><i class="fa fa-check"></i><b>3.9.3</b> Elastic Net Regression</a></li>
<li class="chapter" data-level="3.9.4" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#comparaci%C3%B3n-de-m%C3%A9todos"><i class="fa fa-check"></i><b>3.9.4</b> ComparaciÃ³n de mÃ©todos</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#consideraciones"><i class="fa fa-check"></i><b>3.10</b> Consideraciones</a></li>
<li class="chapter" data-level="3.11" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#sesgo-y-varianza"><i class="fa fa-check"></i><b>3.11</b> Sesgo y Varianza</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#sesgo"><i class="fa fa-check"></i><b>3.11.1</b> Sesgo</a></li>
<li class="chapter" data-level="3.11.2" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#varianza"><i class="fa fa-check"></i><b>3.11.2</b> Varianza</a></li>
<li class="chapter" data-level="3.11.3" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#compromiso-sesgovarianza"><i class="fa fa-check"></i><b>3.11.3</b> Compromiso sesgoâ€“varianza</a></li>
<li class="chapter" data-level="3.11.4" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#en-regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>3.11.4</b> En regresiÃ³n lineal</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="redes-neuronales-lineales-para-regresiÃ³n.html"><a href="redes-neuronales-lineales-para-regresiÃ³n.html#conclusi%C3%B3n-de-la-secci%C3%B3n"><i class="fa fa-check"></i><b>3.12</b> ConclusiÃ³n de la secciÃ³n</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="redes-neuronales-lineales-para-clasificaciÃ³n.html"><a href="redes-neuronales-lineales-para-clasificaciÃ³n.html"><i class="fa fa-check"></i><b>4</b> Redes neuronales Lineales para ClasificaciÃ³n</a>
<ul>
<li class="chapter" data-level="4.1" data-path="redes-neuronales-lineales-para-clasificaciÃ³n.html"><a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#modelos-lineales-generalizados"><i class="fa fa-check"></i><b>4.1</b> Modelos Lineales Generalizados</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="redes-neuronales-lineales-para-clasificaciÃ³n.html"><a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#historia-1"><i class="fa fa-check"></i><b>4.1.1</b> Historia</a></li>
<li class="chapter" data-level="4.1.2" data-path="redes-neuronales-lineales-para-clasificaciÃ³n.html"><a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#definici%C3%B3n"><i class="fa fa-check"></i><b>4.1.2</b> DefiniciÃ³n</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="redes-neuronales-lineales-para-clasificaciÃ³n.html"><a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#regresi%C3%B3n-log%C3%ADstica-para-clasificaci%C3%B3n"><i class="fa fa-check"></i><b>4.2</b> RegresiÃ³n LogÃ­stica para ClasificaciÃ³n</a></li>
<li class="chapter" data-level="4.3" data-path="redes-neuronales-lineales-para-clasificaciÃ³n.html"><a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#redes-neuronales"><i class="fa fa-check"></i><b>4.3</b> Redes neuronales</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="redes-neuronales-lineales-para-clasificaciÃ³n.html"><a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#historia-2"><i class="fa fa-check"></i><b>4.3.1</b> Historia</a></li>
<li class="chapter" data-level="4.3.2" data-path="redes-neuronales-lineales-para-clasificaciÃ³n.html"><a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#definici%C3%B3n-1"><i class="fa fa-check"></i><b>4.3.2</b> DefiniciÃ³n</a></li>
<li class="chapter" data-level="4.3.3" data-path="redes-neuronales-lineales-para-clasificaciÃ³n.html"><a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#neurona"><i class="fa fa-check"></i><b>4.3.3</b> Neurona</a></li>
<li class="chapter" data-level="4.3.4" data-path="redes-neuronales-lineales-para-clasificaciÃ³n.html"><a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#redes-neuronales-1"><i class="fa fa-check"></i><b>4.3.4</b> Redes Neuronales</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="redes-neuronales-lineales-para-clasificaciÃ³n.html"><a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#notas-adicionales"><i class="fa fa-check"></i>Notas adicionales</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html"><i class="fa fa-check"></i><b>5</b> PerceptrÃ³n Multicapa</a>
<ul>
<li class="chapter" data-level="5.1" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#perceptrones-multicapa"><i class="fa fa-check"></i><b>5.1</b> Perceptrones multicapa</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#capas-ocultas"><i class="fa fa-check"></i><b>5.1.1</b> Capas ocultas</a></li>
<li class="chapter" data-level="5.1.2" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#funciones-de-activaci%C3%B3n"><i class="fa fa-check"></i><b>5.1.2</b> Funciones de activaciÃ³n</a></li>
<li class="chapter" data-level="5.1.3" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#discusi%C3%B3n"><i class="fa fa-check"></i><b>5.1.3</b> DiscusiÃ³n</a></li>
<li class="chapter" data-level="5.1.4" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#ejercicios"><i class="fa fa-check"></i><b>5.1.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#implementaci%C3%B3n-de-perceptrones-multicapa"><i class="fa fa-check"></i><b>5.2</b> ImplementaciÃ³n de Perceptrones Multicapa</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#implementaci%C3%B3n-desde-cero"><i class="fa fa-check"></i><b>5.2.1</b> ImplementaciÃ³n desde Cero</a></li>
<li class="chapter" data-level="5.2.2" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#implementaci%C3%B3n"><i class="fa fa-check"></i><b>5.2.2</b> ImplementaciÃ³n</a></li>
<li class="chapter" data-level="5.2.3" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#ejercicios-1"><i class="fa fa-check"></i><b>5.2.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#forward-propagation-backward-propagation"><i class="fa fa-check"></i><b>5.3</b> Forward Propagation, Backward Propagation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#forward-propagation"><i class="fa fa-check"></i><b>5.3.1</b> Forward Propagation</a></li>
<li class="chapter" data-level="5.3.2" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#backpropagation"><i class="fa fa-check"></i><b>5.3.2</b> Backpropagation</a></li>
<li class="chapter" data-level="5.3.3" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#resumen-6"><i class="fa fa-check"></i><b>5.3.3</b> Resumen</a></li>
<li class="chapter" data-level="5.3.4" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#ejercicios-2"><i class="fa fa-check"></i><b>5.3.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#estabilidad-num%C3%A9rica-e-inicializaci%C3%B3n"><i class="fa fa-check"></i><b>5.4</b> Estabilidad NumÃ©rica e InicializaciÃ³n</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#explotaci%C3%B3n-y-desvanecimiento-de-gradientes"><i class="fa fa-check"></i><b>5.4.1</b> ExplotaciÃ³n y Desvanecimiento de Gradientes</a></li>
<li class="chapter" data-level="5.4.2" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#inicializaci%C3%B3n-param%C3%A9trica"><i class="fa fa-check"></i><b>5.4.2</b> InicializaciÃ³n paramÃ©trica</a></li>
<li class="chapter" data-level="5.4.3" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#ejercicios-3"><i class="fa fa-check"></i><b>5.4.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#generalizaci%C3%B3n-en-deep-learning"><i class="fa fa-check"></i><b>5.5</b> GeneralizaciÃ³n en Deep Learning</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#sobreajuste-y-regularizaci%C3%B3n"><i class="fa fa-check"></i><b>5.5.1</b> Sobreajuste y RegularizaciÃ³n</a></li>
<li class="chapter" data-level="5.5.2" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#inspiraci%C3%B3n-de-los-no-param%C3%A9tricos"><i class="fa fa-check"></i><b>5.5.2</b> InspiraciÃ³n de los no paramÃ©tricos</a></li>
<li class="chapter" data-level="5.5.3" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#early-stopping"><i class="fa fa-check"></i><b>5.5.3</b> Early Stopping</a></li>
<li class="chapter" data-level="5.5.4" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#m%C3%A9todos-cl%C3%A1sicos-de-regularizaci%C3%B3n-para-redes-profundas"><i class="fa fa-check"></i><b>5.5.4</b> MÃ©todos clÃ¡sicos de regularizaciÃ³n para redes profundas</a></li>
<li class="chapter" data-level="5.5.5" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#ejercicios-4"><i class="fa fa-check"></i><b>5.5.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#dropout"><i class="fa fa-check"></i><b>5.6</b> Dropout</a></li>
<li class="chapter" data-level="5.7" data-path="perceptrÃ³n-multicapa.html"><a href="perceptrÃ³n-multicapa.html#ejemplo"><i class="fa fa-check"></i><b>5.7</b> Ejemplo</a></li>
</ul></li>
<li class="part"><span><b>Parte 2: TÃ©cnicas Modernas de Deep Learning</b></span></li>
<li class="chapter" data-level="6" data-path="guÃ­a-del-constructor.html"><a href="guÃ­a-del-constructor.html"><i class="fa fa-check"></i><b>6</b> GuÃ­a del Constructor</a></li>
<li class="chapter" data-level="7" data-path="redes-neuronales-convolucionales.html"><a href="redes-neuronales-convolucionales.html"><i class="fa fa-check"></i><b>7</b> Redes Neuronales Convolucionales</a></li>
<li class="chapter" data-level="8" data-path="redes-neuronales-convolucionales-modernas.html"><a href="redes-neuronales-convolucionales-modernas.html"><i class="fa fa-check"></i><b>8</b> Redes Neuronales Convolucionales Modernas</a></li>
<li class="chapter" data-level="9" data-path="redes-neuronales-recurrentes.html"><a href="redes-neuronales-recurrentes.html"><i class="fa fa-check"></i><b>9</b> Redes Neuronales Recurrentes</a></li>
<li class="chapter" data-level="10" data-path="redes-neuronales-recurrentes-modernas.html"><a href="redes-neuronales-recurrentes-modernas.html"><i class="fa fa-check"></i><b>10</b> Redes Neuronales Recurrentes Modernas</a></li>
<li class="chapter" data-level="11" data-path="mecanismos-de-atenciÃ³n-y-transformers.html"><a href="mecanismos-de-atenciÃ³n-y-transformers.html"><i class="fa fa-check"></i><b>11</b> Mecanismos de AtenciÃ³n y Transformers</a></li>
<li class="part"><span><b>Parte 3: Escalabilidad, Eficiencia y Aplicaciones</b></span></li>
<li class="chapter" data-level="12" data-path="algoritmos-de-optimizaciÃ³n.html"><a href="algoritmos-de-optimizaciÃ³n.html"><i class="fa fa-check"></i><b>12</b> Algoritmos de OptimizaciÃ³n</a></li>
<li class="divider"></li>
<li><a href="./"><img src="img/aif-logo.png" width="280"></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="redes-neuronales-lineales-para-clasificaciÃ³n" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">CapÃ­tulo 4</span> Redes neuronales Lineales para ClasificaciÃ³n<a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#redes-neuronales-lineales-para-clasificaci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="modelos-lineales-generalizados" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Modelos Lineales Generalizados<a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#modelos-lineales-generalizados" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="historia-1" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Historia<a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#historia-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>RegresiÃ³n lineal</strong></p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig1.png" width="50%" style="display: block; margin: auto;" />
El primer mÃ©todo de regresiÃ³n lineal documentado es el mÃ©todo de los mÃ­nimos cuadrados, publicado por Legendre en 1805.
Posteriormente, Gauss publicÃ³ un trabajo donde se desarrolla con mayor detalle el tema.</p>
<p><strong>Modelo Lineal Generalizado (GLM)</strong></p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig2.png" width="60%" style="display: block; margin: auto;" />
Los GLM fueron formulados por John Nelder y Robert Wedderburn en 1972 como una manera de unificar modelos estadÃ­sticos.</p>
<p>Un modelo lineal generalizado es una generalizaciÃ³n flexible de la regresiÃ³n lineal ordinaria.</p>
</div>
<div id="definiciÃ³n" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> DefiniciÃ³n<a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#definici%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Los Modelos Lineales Generalizados (GLM) son una extensiÃ³n de los modelos lineales tradicionales que permiten modelar variables respuesta que no necesariamente siguen una distribuciÃ³n normal (por ejemplo, binarias, de conteo o proporciones). Relacionan la distribuciÃ³n aleatoria de la variable dependiente (variable objetivo) en el experimento, con la parte sistemÃ¡tica a travÃ©s de una funciÃ³n llamada funciÃ³n de enlace.</p>
<p>Por lo tanto, un modelo lineal generalizado, tiene tres componentes bÃ¡sicos:</p>
<ul>
<li><strong>Componente aleatorio</strong> <em>(Y ~ DistribuciÃ³n de probabilidad)</em>: Identifica la variable de objetivo y su distribuciÃ³n de probabilidad.</li>
<li><strong>Componente sistemÃ¡tica</strong> <em>(ğœ‚ = xW)</em>: Especifica las variables explicativas de la funciÃ³n predictora lineal.</li>
<li><strong>FunciÃ³n de enlace</strong> <em>(ğ‘”(ğ) = ğœ‚)</em>: Es una funciÃ³n del valor esperado de Y como una combinaciÃ³n lineal de las variables explicativas</li>
</ul>
<p>Algebraicamente:
<span class="math display">\[
    ğ”¼(ğ’€)=ğ=ğ‘”^{âˆ’1}(WX)
\]</span></p>
<p>Los estudios de Wedderburn se limitaron los componentes aleatorios a la familia exponencial, cuya aplicaciÃ³n se puede dar de la siguiente manera:</p>
<ul>
<li><strong>Normal</strong>: regresiÃ³n lineal</li>
<li><strong>Binomial</strong>: regresiÃ³n logÃ­stica</li>
<li><strong>Poisson</strong>: regresiÃ³n de conteo</li>
<li><strong>Gamma</strong>: regresiÃ³n para tiempos o tasas positivas</li>
<li>Inversa Gaussiana, etc.</li>
</ul>
<table>
<thead>
<tr class="header">
<th>DistribuciÃ³n</th>
<th>FunciÃ³n de Enlace</th>
<th>Nombre de modelo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Normal</td>
<td>g(Î¼) = Î¼</td>
<td>RegresiÃ³n lineal</td>
</tr>
<tr class="even">
<td>Binomial</td>
<td>g(Î¼) = log(Î¼ /(1âˆ’Î¼))</td>
<td>RegresiÃ³n logÃ­stica</td>
</tr>
<tr class="odd">
<td>Poisson</td>
<td>g(Î¼) = log(Î¼)</td>
<td>RegresiÃ³n de Poisson</td>
</tr>
</tbody>
</table>
<p><strong>RepresentaciÃ³n grÃ¡fica</strong></p>
<p>Podemos apreciar una regresiÃ³n lineal como una neurona:
<img src="img/04_Linear_Neural_Networks_for_Classification/fig18.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Haciendo uso de una regresiÃ³n logÃ­stica, podemos pasar de <em>how much?</em> a <em>wich class?</em>, el diagrama serÃ­a el siguiente:
<img src="img/04_Linear_Neural_Networks_for_Classification/fig19.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="regresiÃ³n-logÃ­stica-para-clasificaciÃ³n" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> RegresiÃ³n LogÃ­stica para ClasificaciÃ³n<a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#regresi%C3%B3n-log%C3%ADstica-para-clasificaci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Iniciemos recordando que los algoritmos de Machine Learning pueden ser clasificados como:
<img src="img/04_Linear_Neural_Networks_for_Classification/fig5.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Los <strong><em>mÃ©todos de clasificaciÃ³n, mÃ©todos predictivos, reconocimiento de patrones, aprendizaje supervisado</em></strong> son aquellos algoritmos en los cuales vamos a clasificar a los objetos en un conjunto de categorÃ­as previamente definidas.</p>
<p>En estos mÃ©todos existe una variable, conocida como variable objetivo con la cual se buscarÃ¡n dos resultados:</p>
<ul>
<li><strong>Evidenciar</strong> la razÃ³n por la cual existen las clases en cuestiÃ³n, es decir, encontrar factores que puedan agrupar a los individuos en sus respectivos grupos.</li>
<li>Proyectar el ingreso de <strong>nuevos individuos</strong> en alguna de las caracterÃ­sticas anteriores.</li>
</ul>
<p><strong>DefiniciÃ³n</strong></p>
<p>La clasificaciÃ³n es la tarea de <strong>aprender una funciÃ³n, f,</strong> que pueda mapear cada conjunto de atributos a una categorÃ­a predefinida.
Busca predecir la clase a la cual pertenece un registro.
La diferencia principal contra el anÃ¡lisis de conglomerados es el deseo de predecir a nuevos individuos.</p>
<p>Buscamos una funciÃ³n que vaya de D a C
<span class="math display">\[
ğ‘“:ğ·â†’ Y
\]</span>
Donde:</p>
<ul>
<li><span class="math inline">\(ğ·={(ğ‘¥_ğ‘–,ğ‘¦_ğ‘–) | ğ‘–=1,2,â€¦,ğ‘}\)</span> es el conjunto de individuos, incluyendo <span class="math inline">\(ğ‘¥_ğ‘–\)</span> corresponde a las caracterÃ­sticas explicativas de un individuo y <span class="math inline">\(ğ‘¦_ğ‘–\)</span> corresponde al valor de la clase</li>
<li><span class="math inline">\(Y={y _1,  y_2,â€¦, y_ğ‘š}\)</span> es el conjunto de clases.</li>
</ul>
<p>La funciÃ³n resultante puede ser un Ã¡rbol de decisiÃ³n, SVM, etc.</p>
<p>La funciÃ³n que clasificarÃ¡ el conjunto X a la clase Y, comÃºnmente es llamada modelo de clasificaciÃ³n.
<img src="img/04_Linear_Neural_Networks_for_Classification/fig6.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Una vez recordado lo anterior, podemos definir a una regresiÃ³n logÃ­stica como un modelo en el cual se recibe una tabla relacional con <em>d</em> caracterÃ­sticas descriptivas y una clase a la cual pertenece cada individuo de la tabla.</p>
<p>Como resultado, el modelo nos proporciona la <em>probabilidad de pertenecer</em> a alguna clase.</p>
<p><strong>Planteamiento del problema</strong></p>
<p>El primer paso, consistirÃ­a en construir el modelo de regresiÃ³n, de tal forma que el componente aleatorio (<span class="math inline">\(y_i\)</span>) se conecte con la componente sistÃ©mica (regresiÃ³n lineal):
<span class="math display">\[
ğ‘¦_ğ‘–â†”ğ›½_0+ğ›½_1 ğ‘¥_ğ‘–+ğœ–_ğ‘–
\]</span></p>
<hr />
<p>Vamos a suponer un modelo de <strong>regresiÃ³n lineal clÃ¡sica</strong> de tal forma que:
<span class="math display">\[
ğ”¼(ğ‘¦_ğ‘– | ğ‘¥_ğ‘– )=ğ›½_0+ğ›½_1 ğ‘¥_ğ‘–
\]</span>
Al intentar buscar probabilidades a travÃ©s de una regresiÃ³n lineal, obtendrÃ­amos lo siguiente:</p>
<p>Sea <span class="math inline">\(ğ‘_ğ‘–\)</span> la probabilidad de pertenecer a la clase 1 dadas las caracterÃ­sticas del individuo:
<span class="math display">\[
ğ‘_ğ‘–=â„™(ğ‘¦=1|ğ‘¥_ğ‘– )
\]</span></p>
<p>Podemos asumir que <span class="math inline">\(ğ‘_ğ‘–\)</span> sigue una distribuciÃ³n Bernoulli, de tal forma que su esperanza sea:
<span class="math display">\[
ğ”¼(ğ‘¦|ğ‘¥_ğ‘– )=ğ‘_ğ‘–Ã—1+(1âˆ’ğ‘_ğ‘– )Ã—0=ğ‘_ğ‘–
\]</span>
De tal forma que:
<span class="math display">\[
ğ‘_ğ‘–= ğ›½_0+ğ›½_1 ğ‘¥_ğ‘–
\]</span>
No obstante, los resultados pueden resultar absurdos debido a que nuestra fÃ³rmula nos puede proporcionar <strong>probabilidades negativas</strong> o <strong>mayores a cero</strong> <span class="math inline">\(!\)</span></p>
<hr />
<p>Las probabilidades no tienen valores superiores a 1 ni negativas, pero el concepto de momios <span class="math inline">\(p / (1-p)\)</span> tiene como imagen de cero a infinito. La funciÃ³n exponencial tiene la misma imagen, entonces, la regresiÃ³n logÃ­stica relaciona ambos concpetos a travÃ©s de la siguiente igualdad
<span class="math display">\[
â„™(ğ‘¦=1|ğ‘¥_ğ‘– )/(1âˆ’â„™(ğ‘¦=1|ğ‘¥_ğ‘– ) )=ğ‘’^{(ğ›½_0+ğ›½_1 ğ‘¥_ğ‘– )}
\]</span>
Al aplicar el logaritmo natual, podemos apreciar la definiciÃ³n de la funciÃ³n logit:
<span class="math display">\[
logit[ğ‘_ğ‘– ]=ln[ğ‘_ğ‘–/(1âˆ’ğ‘_ğ‘– )]=ğ›½_0+ğ›½_1 ğ‘¥_ğ‘–
\]</span>
Recordando que la funciÃ³n logit es la inversa de la funciÃ³n logistica (o sigmoide)</p>
<p><img src="deep_learning_course_python_files/figure-html/unnamed-chunk-31-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>La funciÃ³n logit es apropiada para los problemas de clasificaciÃ³n con dos clases. No obstante, la generalizaciÃ³n para mÃºltiples clases se hace con la funciÃ³n Softmax()
<span class="math display">\[
\sigma_{softmax} = \frac{e^{z_m}}{\sum_{m=1}^M e^{z_m}}
\]</span></p>
<p>La estructura aplicada en Redes Neuronales es la siguiente:
<img src="img/04_Linear_Neural_Networks_for_Classification/fig20.png" width="70%" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="redes-neuronales" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Redes neuronales<a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#redes-neuronales" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="historia-2" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Historia<a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#historia-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Redes Neuronales</strong></p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig3.png" width="50%" style="display: block; margin: auto;" />
En 1943 Warren McCulloch (neurocientÃ­fico, mÃ©dico, neurÃ³logo y fisiÃ³logo) y Walter Pitts (matemÃ¡tico, psicÃ³logo, filÃ³sofo y neurocientÃ­fico) crearon un modelo para redes neuronales basados en la lÃ³gica de umbral. Este modelo seÃ±alÃ³ el camino para que la investigaciÃ³n de redes neuronales se divida en dos enfoques distintos:</p>
<ol style="list-style-type: decimal">
<li>Un enfoque centrado en los procesos biolÃ³gicos en el cerebro.</li>
<li>Otro en la aplicaciÃ³n de redes neuronales para la inteligencia artificial.</li>
</ol>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig4.png" width="40%" style="display: block; margin: auto;" />
Para la aplicaciÃ³n de la inteligencia artificial, fue Marvin Minsky quien dio una gran aportaciÃ³n con el libro <strong>â€œPerceptronesâ€</strong> en 1969.
Minsky es considerado como uno de los padres de las ciencias de la computaciÃ³n y fue cofundador del laboratorio de inteligencia artificial del MIT</p>
</div>
<div id="definiciÃ³n-1" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> DefiniciÃ³n<a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#definici%C3%B3n-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Las redes neuronales son algoritmos que tienen como objetivo el simular en una computadora la forma en la cual funcionan las redes neuronales en los humanos.
Puede recibir variables cuantitativas y cualitativas. Es un algoritmo sensible a cambios en los datos y parÃ¡metros</p>
</div>
<div id="neurona" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Neurona<a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#neurona" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>CÃ©lula del sistema nerviosos formada por un nÃºcleo y una serie de prolongaciones, una de las cuales es mÃ¡s larga que las demÃ¡s.
EstÃ¡n especializadas en la recepciÃ³n de estÃ­mulos y conducciÃ³n del impulso nerviosos entre ellas o con otros tipos celulas</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig7.png" width="60%" style="display: block; margin: auto;" /></p>
<p><br></p>
<p><strong>Dentrita:</strong> Es la fuente de un impulso nerviosos. De hecho el impulso nervioso es unidireccional, es decir, solamente se transmite desde las dendritas hacia el axÃ³n (canal de entrada)</p>
<p><strong>AxÃ³n:</strong> ProlongaciÃ³n que arranca del cuerpo de la neurona y termina en una ramificaciÃ³n que estÃ¡ en contacto con otras cÃ©lulas. Es la vÃ­a por la cual circulan los impulsos elÃ©ctricos (canal de salida)</p>
<p><strong>Sinapsis:</strong> RegiÃ³n de comunicaciÃ³n entre el axÃ³n de una neurona y las dendritas o el cuerpo de otra</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig8.png" width="60%" style="display: block; margin: auto;" /></p>
<p><strong>EmulaciÃ³n:</strong></p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig9.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Las redes neuronales, tratan de modelar una red neuronal donde cada nodo es una neurona, y los arcos representan la sinapsis entre las neuronas.
Como algoritmo, las redes neuronales reciben n entradas y pueden otorgar n salidas.</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig10.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Un perceptrÃ³n puede entenderse como la unidad bÃ¡sica de inferencia en forma de discriminador lineal. Un perceptrÃ³n va a separar a los puntos a travÃ©s de un hiperplano
Es el modelo biolÃ³gico mÃ¡s sencillo hace referencia a una sola neurona. El perceptrÃ³n consiste de dos tipos de nodos:</p>
<ul>
<li>Nodos de entrada: ingreso de los atributos</li>
<li>Nodos de salida: representa a los resultados del modelo</li>
</ul>
<p>Un perceptrÃ³n genera sus resultados realizando una suma ponderada de sus atributos de entrada, restando un valor t y examinando el signo del resultado.
Supongamos la siguiente tabla:</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig11.png" width="30%" style="display: block; margin: auto;" /></p>
<p>El perceptrÃ³n serÃ­a aquella red neuronal que recibiera de entrada cada uno de los valores x y diera como resultado y</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig12.png" width="60%" style="display: block; margin: auto;" /></p>
<p><span class="math display">\[
y =
\begin{cases}
    \text{1, si:  } 0.3ğ‘¥_1+0.3ğ‘¥_1+0.3ğ‘¥_1âˆ’0.4 â‰¥ 0 \\
    \text{-1, si:  } 0.3ğ‘¥_1+0.3ğ‘¥_1+0.3ğ‘¥_1âˆ’0.4 &lt; 0 \\
\end{cases}
\]</span>
Como el resultado depende del signo, la funciÃ³n se representa como:
<span class="math display">\[
ğ‘¦=ğ‘ ğ‘–ğ‘”ğ‘›(ğ‘¤,ğ‘¥)
\]</span></p>
<p>Cuando intentamos hacer la separaciÃ³n de un XOr, podemos notar que necesitamos dos discriminadores lineales:</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig13.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="redes-neuronales-1" class="section level3 hasAnchor" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Redes Neuronales<a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#redes-neuronales-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Las redes neuronales, son una estructura mÃ¡s compleja que un perceptrÃ³n. Algunas de las complejidades son:</p>
<ul>
<li>Las redes neuronales tienen <strong>varios nodos intermedios</strong> entre los nodos de entrada y los nodos de salida.</li>
<li>Las capas ocultas pueden utilizar <strong>funciones de activaciÃ³n diferentes</strong></li>
</ul>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig14.png" width="60%" style="display: block; margin: auto;" /></p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig15.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Con una capa oculta, es posible generar una funciÃ³n de discriminaciÃ³n para una tabla XOr</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig17.png" width="60%" style="display: block; margin: auto;" /></p>
<p><strong>Funciones de activaciÃ³n comunes</strong></p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig16.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="notas-adicionales" class="section level2 unnumbered hasAnchor">
<h2>Notas adicionales<a href="redes-neuronales-lineales-para-clasificaciÃ³n.html#notas-adicionales" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Curso de Git: <a href="https://www.youtube.com/watch?v=3GymExBkKjE&amp;t=195s" class="uri">https://www.youtube.com/watch?v=3GymExBkKjE&amp;t=195s</a></p>

<!-- ::: watermark -->
<!-- <img src="img/aif-logo.png" width="400"/> -->
<!-- ::: -->
</div>
</div>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script>

$('.pipehover_incremental tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).prevAll().addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});


$('.pipehover_select_one_row tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});

</script>
            </section>

          </div>
        </div>
      </div>
<a href="redes-neuronales-lineales-para-regresiÃ³n.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="perceptrÃ³n-multicapa.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["deep_learning_course_python.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
