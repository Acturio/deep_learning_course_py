<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Cap√≠tulo 4 Redes neuronales Lineales para Clasificaci√≥n | Deep Learning</title>
  <meta name="description" content="Cap√≠tulo 4 Redes neuronales Lineales para Clasificaci√≥n | Deep Learning" />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="Cap√≠tulo 4 Redes neuronales Lineales para Clasificaci√≥n | Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Cap√≠tulo 4 Redes neuronales Lineales para Clasificaci√≥n | Deep Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Cap√≠tulo 4 Redes neuronales Lineales para Clasificaci√≥n | Deep Learning" />
  
  <meta name="twitter:description" content="Cap√≠tulo 4 Redes neuronales Lineales para Clasificaci√≥n | Deep Learning" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="redes-neuronales-lineales-para-regresi√≥n.html"/>
<link rel="next" href="perceptr√≥n-multicapa.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="img/aif-logo.png" width="280"></a></li|
|:-:|  
<center>Curso de Redes Neuronales Artificiales</center>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>BIENVENIDA</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#objetivo"><i class="fa fa-check"></i>Objetivo</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#alcances-del-programa"><i class="fa fa-check"></i>Alcances del Programa</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#temario"><i class="fa fa-check"></i>Temario:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#c%C3%B3digo"><i class="fa fa-check"></i>C√≥digo</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#duraci%C3%B3n-y-evaluaci%C3%B3n-del-programa"><i class="fa fa-check"></i>Duraci√≥n y evaluaci√≥n del programa</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recursos-y-din%C3%A1mica"><i class="fa fa-check"></i>Recursos y din√°mica</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agenda"><i class="fa fa-check"></i>Agenda</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bibliograf%C3%ADa"><i class="fa fa-check"></i>Bibliograf√≠a</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introducci√≥n-a-deep-learning.html"><a href="introducci√≥n-a-deep-learning.html"><i class="fa fa-check"></i><b>1</b> Introducci√≥n a Deep Learning</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introducci√≥n-a-deep-learning.html"><a href="introducci√≥n-a-deep-learning.html#introducci%C3%B3n-al-aprendizaje-autom%C3%A1tico"><i class="fa fa-check"></i><b>1.1</b> Introducci√≥n al aprendizaje autom√°tico</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introducci√≥n-a-deep-learning.html"><a href="introducci√≥n-a-deep-learning.html#motivaci%C3%B3n-los-paradigmas-del-conocimiento"><i class="fa fa-check"></i><b>1.1.1</b> Motivaci√≥n: Los paradigmas del conocimiento</a></li>
<li class="chapter" data-level="1.1.2" data-path="introducci√≥n-a-deep-learning.html"><a href="introducci√≥n-a-deep-learning.html#machine-learning-supervisado"><i class="fa fa-check"></i><b>1.1.2</b> Machine learning supervisado</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introducci√≥n-a-deep-learning.html"><a href="introducci√≥n-a-deep-learning.html#por-qu%C3%A9-deep-learning"><i class="fa fa-check"></i><b>1.2</b> ¬øPor qu√© Deep Learning?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introducci√≥n-a-deep-learning.html"><a href="introducci√≥n-a-deep-learning.html#dimensi%C3%B3n-vc"><i class="fa fa-check"></i><b>1.2.1</b> Dimensi√≥n VC</a></li>
<li class="chapter" data-level="1.2.2" data-path="introducci√≥n-a-deep-learning.html"><a href="introducci√≥n-a-deep-learning.html#el-truco-del-kernel-svm"><i class="fa fa-check"></i><b>1.2.2</b> El truco del Kernel (SVM)</a></li>
<li class="chapter" data-level="1.2.3" data-path="introducci√≥n-a-deep-learning.html"><a href="introducci√≥n-a-deep-learning.html#ingenier%C3%ADa-de-caracteristicas"><i class="fa fa-check"></i><b>1.2.3</b> Ingenier√≠a de caracteristicas</a></li>
<li class="chapter" data-level="1.2.4" data-path="introducci√≥n-a-deep-learning.html"><a href="introducci√≥n-a-deep-learning.html#beneficios-del-deep-learning"><i class="fa fa-check"></i><b>1.2.4</b> Beneficios del deep learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="preliminares.html"><a href="preliminares.html"><i class="fa fa-check"></i><b>2</b> Preliminares</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminares.html"><a href="preliminares.html#algebra-lineal"><i class="fa fa-check"></i><b>2.1</b> Algebra Lineal</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="preliminares.html"><a href="preliminares.html#introducci%C3%B3n"><i class="fa fa-check"></i><b>2.1.1</b> Introducci√≥n</a></li>
<li class="chapter" data-level="2.1.2" data-path="preliminares.html"><a href="preliminares.html#motivaci%C3%B3n"><i class="fa fa-check"></i><b>2.1.2</b> Motivaci√≥n</a></li>
<li class="chapter" data-level="2.1.3" data-path="preliminares.html"><a href="preliminares.html#escalares"><i class="fa fa-check"></i><b>2.1.3</b> Escalares</a></li>
<li class="chapter" data-level="2.1.4" data-path="preliminares.html"><a href="preliminares.html#vectores-1"><i class="fa fa-check"></i><b>2.1.4</b> Vectores</a></li>
<li class="chapter" data-level="2.1.5" data-path="preliminares.html"><a href="preliminares.html#matrices-1"><i class="fa fa-check"></i><b>2.1.5</b> Matrices</a></li>
<li class="chapter" data-level="2.1.6" data-path="preliminares.html"><a href="preliminares.html#tensores"><i class="fa fa-check"></i><b>2.1.6</b> Tensores</a></li>
<li class="chapter" data-level="2.1.7" data-path="preliminares.html"><a href="preliminares.html#reducciones-sobre-tensores"><i class="fa fa-check"></i><b>2.1.7</b> Reducciones sobre tensores</a></li>
<li class="chapter" data-level="2.1.8" data-path="preliminares.html"><a href="preliminares.html#producto-punto"><i class="fa fa-check"></i><b>2.1.8</b> Producto punto</a></li>
<li class="chapter" data-level="2.1.9" data-path="preliminares.html"><a href="preliminares.html#producto-entre-matrices-y-vectores"><i class="fa fa-check"></i><b>2.1.9</b> Producto entre matrices y vectores</a></li>
<li class="chapter" data-level="2.1.10" data-path="preliminares.html"><a href="preliminares.html#multiplicaci%C3%B3n-matricial"><i class="fa fa-check"></i><b>2.1.10</b> Multiplicaci√≥n matricial</a></li>
<li class="chapter" data-level="2.1.11" data-path="preliminares.html"><a href="preliminares.html#normas"><i class="fa fa-check"></i><b>2.1.11</b> Normas</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="preliminares.html"><a href="preliminares.html#c%C3%A1lculo-diferencial"><i class="fa fa-check"></i><b>2.2</b> C√°lculo Diferencial</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="preliminares.html"><a href="preliminares.html#introducci%C3%B3n-1"><i class="fa fa-check"></i><b>2.2.1</b> Introducci√≥n</a></li>
<li class="chapter" data-level="2.2.2" data-path="preliminares.html"><a href="preliminares.html#motivaci%C3%B3n-1"><i class="fa fa-check"></i><b>2.2.2</b> Motivaci√≥n</a></li>
<li class="chapter" data-level="2.2.3" data-path="preliminares.html"><a href="preliminares.html#derivadas-y-diferenciaci%C3%B3n"><i class="fa fa-check"></i><b>2.2.3</b> Derivadas y diferenciaci√≥n</a></li>
<li class="chapter" data-level="2.2.4" data-path="preliminares.html"><a href="preliminares.html#regla-de-la-cadena"><i class="fa fa-check"></i><b>2.2.4</b> Regla de la cadena</a></li>
<li class="chapter" data-level="2.2.5" data-path="preliminares.html"><a href="preliminares.html#interpretaci%C3%B3n-geom%C3%A9trica-de-la-derivada"><i class="fa fa-check"></i><b>2.2.5</b> Interpretaci√≥n geom√©trica de la derivada</a></li>
<li class="chapter" data-level="2.2.6" data-path="preliminares.html"><a href="preliminares.html#teorema-fundamental-del-c%C3%A1lculo"><i class="fa fa-check"></i><b>2.2.6</b> Teorema fundamental del c√°lculo</a></li>
<li class="chapter" data-level="2.2.7" data-path="preliminares.html"><a href="preliminares.html#autodiferenciaci%C3%B3n"><i class="fa fa-check"></i><b>2.2.7</b> Autodiferenciaci√≥n</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html"><i class="fa fa-check"></i><b>3</b> Redes neuronales Lineales para Regresi√≥n</a>
<ul>
<li class="chapter" data-level="3.1" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#introducci%C3%B3n-2"><i class="fa fa-check"></i><b>3.1</b> Introducci√≥n</a></li>
<li class="chapter" data-level="3.2" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#regresiones-lineales-simples"><i class="fa fa-check"></i><b>3.2</b> Regresiones lineales simples</a></li>
<li class="chapter" data-level="3.3" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#funci%C3%B3n-de-p%C3%A9rdida-y-funci%C3%B3n-de-costo"><i class="fa fa-check"></i><b>3.3</b> Funci√≥n de p√©rdida y funci√≥n de costo</a></li>
<li class="chapter" data-level="3.4" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#supuestos-en-la-regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>3.4</b> Supuestos en la regresi√≥n lineal</a></li>
<li class="chapter" data-level="3.5" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#regresi%C3%B3n-lineal-m%C3%BAltiple"><i class="fa fa-check"></i><b>3.5</b> Regresi√≥n Lineal M√∫ltiple</a></li>
<li class="chapter" data-level="3.6" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#estimaci%C3%B3n-de-los-par%C3%A1metros"><i class="fa fa-check"></i><b>3.6</b> Estimaci√≥n de los par√°metros</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#derivaci%C3%B3n-paso-a-paso"><i class="fa fa-check"></i><b>3.6.1</b> Derivaci√≥n paso a paso</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#bondad-de-ajuste"><i class="fa fa-check"></i><b>3.7</b> Bondad de ajuste</a></li>
<li class="chapter" data-level="3.8" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#supuestos-del-modelo-lineal-m%C3%BAltiple"><i class="fa fa-check"></i><b>3.8</b> Supuestos del modelo lineal m√∫ltiple</a></li>
<li class="chapter" data-level="3.9" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#regularizaci%C3%B3n-en-la-regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>3.9</b> Regularizaci√≥n en la Regresi√≥n Lineal</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#ridge-regression"><i class="fa fa-check"></i><b>3.9.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="3.9.2" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#lasso-regression"><i class="fa fa-check"></i><b>3.9.2</b> Lasso Regression</a></li>
<li class="chapter" data-level="3.9.3" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#elastic-net-regression"><i class="fa fa-check"></i><b>3.9.3</b> Elastic Net Regression</a></li>
<li class="chapter" data-level="3.9.4" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#comparaci%C3%B3n-de-m%C3%A9todos"><i class="fa fa-check"></i><b>3.9.4</b> Comparaci√≥n de m√©todos</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#consideraciones"><i class="fa fa-check"></i><b>3.10</b> Consideraciones</a></li>
<li class="chapter" data-level="3.11" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#sesgo-y-varianza"><i class="fa fa-check"></i><b>3.11</b> Sesgo y Varianza</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#sesgo"><i class="fa fa-check"></i><b>3.11.1</b> Sesgo</a></li>
<li class="chapter" data-level="3.11.2" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#varianza"><i class="fa fa-check"></i><b>3.11.2</b> Varianza</a></li>
<li class="chapter" data-level="3.11.3" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#compromiso-sesgovarianza"><i class="fa fa-check"></i><b>3.11.3</b> Compromiso sesgo‚Äìvarianza</a></li>
<li class="chapter" data-level="3.11.4" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#en-regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>3.11.4</b> En regresi√≥n lineal</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="redes-neuronales-lineales-para-regresi√≥n.html"><a href="redes-neuronales-lineales-para-regresi√≥n.html#conclusi%C3%B3n-de-la-secci%C3%B3n"><i class="fa fa-check"></i><b>3.12</b> Conclusi√≥n de la secci√≥n</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="redes-neuronales-lineales-para-clasificaci√≥n.html"><a href="redes-neuronales-lineales-para-clasificaci√≥n.html"><i class="fa fa-check"></i><b>4</b> Redes neuronales Lineales para Clasificaci√≥n</a>
<ul>
<li class="chapter" data-level="4.1" data-path="redes-neuronales-lineales-para-clasificaci√≥n.html"><a href="redes-neuronales-lineales-para-clasificaci√≥n.html#modelos-lineales-generalizados"><i class="fa fa-check"></i><b>4.1</b> Modelos Lineales Generalizados</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="redes-neuronales-lineales-para-clasificaci√≥n.html"><a href="redes-neuronales-lineales-para-clasificaci√≥n.html#historia-1"><i class="fa fa-check"></i><b>4.1.1</b> Historia</a></li>
<li class="chapter" data-level="4.1.2" data-path="redes-neuronales-lineales-para-clasificaci√≥n.html"><a href="redes-neuronales-lineales-para-clasificaci√≥n.html#definici%C3%B3n"><i class="fa fa-check"></i><b>4.1.2</b> Definici√≥n</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="redes-neuronales-lineales-para-clasificaci√≥n.html"><a href="redes-neuronales-lineales-para-clasificaci√≥n.html#regresi%C3%B3n-log%C3%ADstica-para-clasificaci%C3%B3n"><i class="fa fa-check"></i><b>4.2</b> Regresi√≥n Log√≠stica para Clasificaci√≥n</a></li>
<li class="chapter" data-level="4.3" data-path="redes-neuronales-lineales-para-clasificaci√≥n.html"><a href="redes-neuronales-lineales-para-clasificaci√≥n.html#redes-neuronales"><i class="fa fa-check"></i><b>4.3</b> Redes neuronales</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="redes-neuronales-lineales-para-clasificaci√≥n.html"><a href="redes-neuronales-lineales-para-clasificaci√≥n.html#historia-2"><i class="fa fa-check"></i><b>4.3.1</b> Historia</a></li>
<li class="chapter" data-level="4.3.2" data-path="redes-neuronales-lineales-para-clasificaci√≥n.html"><a href="redes-neuronales-lineales-para-clasificaci√≥n.html#definici%C3%B3n-1"><i class="fa fa-check"></i><b>4.3.2</b> Definici√≥n</a></li>
<li class="chapter" data-level="4.3.3" data-path="redes-neuronales-lineales-para-clasificaci√≥n.html"><a href="redes-neuronales-lineales-para-clasificaci√≥n.html#neurona"><i class="fa fa-check"></i><b>4.3.3</b> Neurona</a></li>
<li class="chapter" data-level="4.3.4" data-path="redes-neuronales-lineales-para-clasificaci√≥n.html"><a href="redes-neuronales-lineales-para-clasificaci√≥n.html#redes-neuronales-1"><i class="fa fa-check"></i><b>4.3.4</b> Redes Neuronales</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="redes-neuronales-lineales-para-clasificaci√≥n.html"><a href="redes-neuronales-lineales-para-clasificaci√≥n.html#notas-adicionales"><i class="fa fa-check"></i>Notas adicionales</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html"><i class="fa fa-check"></i><b>5</b> Perceptr√≥n Multicapa</a>
<ul>
<li class="chapter" data-level="5.1" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#perceptrones-multicapa"><i class="fa fa-check"></i><b>5.1</b> Perceptrones multicapa</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#capas-ocultas"><i class="fa fa-check"></i><b>5.1.1</b> Capas ocultas</a></li>
<li class="chapter" data-level="5.1.2" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#funciones-de-activaci%C3%B3n"><i class="fa fa-check"></i><b>5.1.2</b> Funciones de activaci√≥n</a></li>
<li class="chapter" data-level="5.1.3" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#discusi%C3%B3n"><i class="fa fa-check"></i><b>5.1.3</b> Discusi√≥n</a></li>
<li class="chapter" data-level="5.1.4" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#ejercicios"><i class="fa fa-check"></i><b>5.1.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#implementaci%C3%B3n-de-perceptrones-multicapa"><i class="fa fa-check"></i><b>5.2</b> Implementaci√≥n de Perceptrones Multicapa</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#implementaci%C3%B3n-desde-cero"><i class="fa fa-check"></i><b>5.2.1</b> Implementaci√≥n desde Cero</a></li>
<li class="chapter" data-level="5.2.2" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#implementaci%C3%B3n"><i class="fa fa-check"></i><b>5.2.2</b> Implementaci√≥n</a></li>
<li class="chapter" data-level="5.2.3" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#ejercicios-1"><i class="fa fa-check"></i><b>5.2.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#forward-propagation-backward-propagation"><i class="fa fa-check"></i><b>5.3</b> Forward Propagation, Backward Propagation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#forward-propagation"><i class="fa fa-check"></i><b>5.3.1</b> Forward Propagation</a></li>
<li class="chapter" data-level="5.3.2" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#backpropagation"><i class="fa fa-check"></i><b>5.3.2</b> Backpropagation</a></li>
<li class="chapter" data-level="5.3.3" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#resumen-6"><i class="fa fa-check"></i><b>5.3.3</b> Resumen</a></li>
<li class="chapter" data-level="5.3.4" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#ejercicios-2"><i class="fa fa-check"></i><b>5.3.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#estabilidad-num%C3%A9rica-e-inicializaci%C3%B3n"><i class="fa fa-check"></i><b>5.4</b> Estabilidad Num√©rica e Inicializaci√≥n</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#explotaci%C3%B3n-y-desvanecimiento-de-gradientes"><i class="fa fa-check"></i><b>5.4.1</b> Explotaci√≥n y Desvanecimiento de Gradientes</a></li>
<li class="chapter" data-level="5.4.2" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#inicializaci%C3%B3n-param%C3%A9trica"><i class="fa fa-check"></i><b>5.4.2</b> Inicializaci√≥n param√©trica</a></li>
<li class="chapter" data-level="5.4.3" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#ejercicios-3"><i class="fa fa-check"></i><b>5.4.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#generalizaci%C3%B3n-en-deep-learning"><i class="fa fa-check"></i><b>5.5</b> Generalizaci√≥n en Deep Learning</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#sobreajuste-y-regularizaci%C3%B3n"><i class="fa fa-check"></i><b>5.5.1</b> Sobreajuste y Regularizaci√≥n</a></li>
<li class="chapter" data-level="5.5.2" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#inspiraci%C3%B3n-de-los-no-param%C3%A9tricos"><i class="fa fa-check"></i><b>5.5.2</b> Inspiraci√≥n de los no param√©tricos</a></li>
<li class="chapter" data-level="5.5.3" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#early-stopping"><i class="fa fa-check"></i><b>5.5.3</b> Early Stopping</a></li>
<li class="chapter" data-level="5.5.4" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#m%C3%A9todos-cl%C3%A1sicos-de-regularizaci%C3%B3n-para-redes-profundas"><i class="fa fa-check"></i><b>5.5.4</b> M√©todos cl√°sicos de regularizaci√≥n para redes profundas</a></li>
<li class="chapter" data-level="5.5.5" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#ejercicios-4"><i class="fa fa-check"></i><b>5.5.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#dropout"><i class="fa fa-check"></i><b>5.6</b> Dropout</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#dropout-en-la-pr%C3%A1ctica"><i class="fa fa-check"></i><b>5.6.1</b> Dropout en la pr√°ctica</a></li>
<li class="chapter" data-level="5.6.2" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#implementaci%C3%B3n-desde-cero-1"><i class="fa fa-check"></i><b>5.6.2</b> Implementaci√≥n desde cero</a></li>
<li class="chapter" data-level="5.6.3" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#definici%C3%B3n-del-modelo"><i class="fa fa-check"></i><b>5.6.3</b> Definici√≥n del modelo</a></li>
<li class="chapter" data-level="5.6.4" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#entrenamiento-2"><i class="fa fa-check"></i><b>5.6.4</b> Entrenamiento</a></li>
<li class="chapter" data-level="5.6.5" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#implementaci%C3%B3n-concisa"><i class="fa fa-check"></i><b>5.6.5</b> Implementaci√≥n concisa</a></li>
<li class="chapter" data-level="5.6.6" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#resumen-7"><i class="fa fa-check"></i><b>5.6.6</b> Resumen</a></li>
<li class="chapter" data-level="5.6.7" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#ejercicios-5"><i class="fa fa-check"></i><b>5.6.7</b> Ejercicios</a></li>
<li class="chapter" data-level="5.6.8" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#dropout-en-la-pr%C3%A1ctica-1"><i class="fa fa-check"></i><b>5.6.8</b> Dropout en la pr√°ctica</a></li>
<li class="chapter" data-level="5.6.9" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#implementaci%C3%B3n-desde-cero-2"><i class="fa fa-check"></i><b>5.6.9</b> Implementaci√≥n desde cero</a></li>
<li class="chapter" data-level="5.6.10" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#definici%C3%B3n-del-modelo-1"><i class="fa fa-check"></i><b>5.6.10</b> Definici√≥n del modelo</a></li>
<li class="chapter" data-level="5.6.11" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#entrenamiento-3"><i class="fa fa-check"></i><b>5.6.11</b> Entrenamiento</a></li>
<li class="chapter" data-level="5.6.12" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#implementaci%C3%B3n-concisa-1"><i class="fa fa-check"></i><b>5.6.12</b> Implementaci√≥n concisa</a></li>
<li class="chapter" data-level="5.6.13" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#resumen-8"><i class="fa fa-check"></i><b>5.6.13</b> Resumen</a></li>
<li class="chapter" data-level="5.6.14" data-path="perceptr√≥n-multicapa.html"><a href="perceptr√≥n-multicapa.html#ejercicios-6"><i class="fa fa-check"></i><b>5.6.14</b> Ejercicios</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gpus.html"><a href="gpus.html"><i class="fa fa-check"></i><b>6</b> GPUs</a>
<ul>
<li class="chapter" data-level="6.1" data-path="gpus.html"><a href="gpus.html#paralelismo-y-concurrencia"><i class="fa fa-check"></i><b>6.1</b> Paralelismo y concurrencia</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="gpus.html"><a href="gpus.html#reflexi%C3%B3n-concurrencia-y-paralelismo"><i class="fa fa-check"></i><b>6.1.1</b> Reflexi√≥n: Concurrencia y Paralelismo</a></li>
<li class="chapter" data-level="6.1.2" data-path="gpus.html"><a href="gpus.html#relaci%C3%B3n-entre-concurrencia-y-paralelismo"><i class="fa fa-check"></i><b>6.1.2</b> Relaci√≥n entre concurrencia y paralelismo</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gpus.html"><a href="gpus.html#complejidad-computacional"><i class="fa fa-check"></i><b>6.2</b> Complejidad Computacional</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="gpus.html"><a href="gpus.html#analizando-la-complejidad-computacional-en-el-problema-de-ordenamiento-de-n%C3%BAmeros"><i class="fa fa-check"></i><b>6.2.1</b> Analizando la complejidad computacional en el problema de ordenamiento de n√∫meros</a></li>
<li class="chapter" data-level="6.2.2" data-path="gpus.html"><a href="gpus.html#notaci%C3%B3n-big-o"><i class="fa fa-check"></i><b>6.2.2</b> Notaci√≥n Big O</a></li>
<li class="chapter" data-level="6.2.3" data-path="gpus.html"><a href="gpus.html#bubble-sort-vs-merge-sort."><i class="fa fa-check"></i><b>6.2.3</b> Bubble sort vs Merge sort.</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gpus.html"><a href="gpus.html#el-papel-del-hardware"><i class="fa fa-check"></i><b>6.3</b> El papel del hardware</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="gpus.html"><a href="gpus.html#implementaci%C3%B3n-de-multiplicaci%C3%B3n-de-matrices-en-distintas-arquitecturas"><i class="fa fa-check"></i><b>6.3.1</b> Implementaci√≥n de multiplicaci√≥n de matrices en distintas arquitecturas</a></li>
<li class="chapter" data-level="6.3.2" data-path="gpus.html"><a href="gpus.html#es-viable-acelerar-cualquier-algoritmo-en-gpu-tpu"><i class="fa fa-check"></i><b>6.3.2</b> ¬øEs viable acelerar cualquier algoritmo en GPU / TPU?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="redes-neuronales-convolucionales.html"><a href="redes-neuronales-convolucionales.html"><i class="fa fa-check"></i><b>7</b> Redes Neuronales Convolucionales</a></li>
<li class="chapter" data-level="8" data-path="redes-neuronales-convolucionales-modernas.html"><a href="redes-neuronales-convolucionales-modernas.html"><i class="fa fa-check"></i><b>8</b> Redes Neuronales Convolucionales Modernas</a></li>
<li class="chapter" data-level="9" data-path="redes-neuronales-recurrentes.html"><a href="redes-neuronales-recurrentes.html"><i class="fa fa-check"></i><b>9</b> Redes Neuronales Recurrentes</a></li>
<li class="chapter" data-level="10" data-path="redes-neuronales-recurrentes-modernas.html"><a href="redes-neuronales-recurrentes-modernas.html"><i class="fa fa-check"></i><b>10</b> Redes Neuronales Recurrentes Modernas</a></li>
<li class="chapter" data-level="11" data-path="mecanismos-de-atenci√≥n-y-transformers.html"><a href="mecanismos-de-atenci√≥n-y-transformers.html"><i class="fa fa-check"></i><b>11</b> Mecanismos de Atenci√≥n y Transformers</a></li>
<li class="part"><span><b>Parte 3: Escalabilidad, Eficiencia y Aplicaciones</b></span></li>
<li class="chapter" data-level="12" data-path="algoritmos-de-optimizaci√≥n.html"><a href="algoritmos-de-optimizaci√≥n.html"><i class="fa fa-check"></i><b>12</b> Algoritmos de Optimizaci√≥n</a></li>
<li class="chapter" data-level="" data-path="miscellanea-intro-to-graph-neural-networks.html"><a href="miscellanea-intro-to-graph-neural-networks.html"><i class="fa fa-check"></i>Miscellanea: Intro to Graph Neural Networks</a></li>
<li class="chapter" data-level="13" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><i class="fa fa-check"></i><b>13</b> Introducci√≥n a la Teor√≠a de Gr√°ficas.</a>
<ul>
<li class="chapter" data-level="13.1" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#qu%C3%A9-es-una-gr%C3%A1fica"><i class="fa fa-check"></i><b>13.1</b> ¬øQu√© es una gr√°fica?</a></li>
<li class="chapter" data-level="13.2" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#problemas-cl%C3%A1sicos-de-teor%C3%ADa-de-gr%C3%A1ficas-selecci%C3%B3n"><i class="fa fa-check"></i><b>13.2</b> Problemas cl√°sicos de teor√≠a de gr√°ficas (selecci√≥n)</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#caminos-y-conectividad"><i class="fa fa-check"></i><b>13.2.1</b> Caminos y conectividad</a></li>
<li class="chapter" data-level="13.2.2" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#detecci%C3%B3n-de-comunidades"><i class="fa fa-check"></i><b>13.2.2</b> Detecci√≥n de comunidades</a></li>
<li class="chapter" data-level="13.2.3" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#centralidad-e-influencia"><i class="fa fa-check"></i><b>13.2.3</b> Centralidad e influencia</a></li>
<li class="chapter" data-level="13.2.4" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#emparejamiento-y-asignaci%C3%B3n"><i class="fa fa-check"></i><b>13.2.4</b> Emparejamiento y asignaci√≥n</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#ecosistema-de-herramientas-para-el-trabajo-con-grafos"><i class="fa fa-check"></i><b>13.3</b> Ecosistema de Herramientas para el Trabajo con Grafos</a></li>
<li class="chapter" data-level="13.4" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#por-qu%C3%A9-combinar-graficas-y-deep-learning"><i class="fa fa-check"></i><b>13.4</b> ¬øPor qu√© combinar Graficas y Deep Learning?</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#a.-multilayer-percepton-en-las-features-tabulares-de-cora"><i class="fa fa-check"></i><b>13.4.1</b> A. Multilayer Percepton en las features tabulares de Cora</a></li>
<li class="chapter" data-level="13.4.2" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#b.-modelo-basado-en-una-capa-lineal-que-se-multiplica-por-la-matriz-de-adyacencia."><i class="fa fa-check"></i><b>13.4.2</b> B. Modelo basado en una capa lineal que se multiplica por la matriz de adyacencia.</a></li>
<li class="chapter" data-level="13.4.3" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#qu%C3%A9-esta-haciendo-la-red"><i class="fa fa-check"></i><b>13.4.3</b> ¬øQu√© esta haciendo la red?</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#tipos-de-problemas-de-gnn"><i class="fa fa-check"></i><b>13.5</b> Tipos de problemas de GNN</a></li>
<li class="chapter" data-level="13.6" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#existe-un-teorema-de-aproximaci%C3%B3n-univeral-tau-para-gnns"><i class="fa fa-check"></i><b>13.6</b> ¬øExiste un Teorema de Aproximaci√≥n Univeral (TAU) para GNN‚Äôs?</a></li>
<li class="chapter" data-level="13.7" data-path="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html"><a href="introducci√≥n-a-la-teor√≠a-de-gr√°ficas..html#enfoques-de-aprendizaje-transductivo-e-inductivo"><i class="fa fa-check"></i><b>13.7</b> Enfoques de aprendizaje transductivo e inductivo</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="graph-neural-network.html"><a href="graph-neural-network.html"><i class="fa fa-check"></i><b>14</b> Graph Neural Network</a>
<ul>
<li class="chapter" data-level="14.1" data-path="graph-neural-network.html"><a href="graph-neural-network.html#pytorch-geometric-y-graph-neural-networks"><i class="fa fa-check"></i><b>14.1</b> PyTorch Geometric y Graph Neural Networks</a></li>
<li class="chapter" data-level="14.2" data-path="graph-neural-network.html"><a href="graph-neural-network.html#graph-convolutional-networks-gnn"><i class="fa fa-check"></i><b>14.2</b> Graph Convolutional Networks (GNN)</a></li>
<li class="chapter" data-level="14.3" data-path="graph-neural-network.html"><a href="graph-neural-network.html#c%C3%B3mo-funcionan"><i class="fa fa-check"></i><b>14.3</b> ¬øC√≥mo funcionan?</a></li>
<li class="chapter" data-level="14.4" data-path="graph-neural-network.html"><a href="graph-neural-network.html#implementaci%C3%B3n-en-pytorch-geometric"><i class="fa fa-check"></i><b>14.4</b> Implementaci√≥n en PyTorch Geometric</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="graph-neural-network.html"><a href="graph-neural-network.html#ejemplo-predicci%C3%B3n-del-volumen-de-tr%C3%A1fico-en-wikipedia-regresi%C3%B3n"><i class="fa fa-check"></i><b>14.4.1</b> Ejemplo: Predicci√≥n del volumen de tr√°fico en Wikipedia (Regresi√≥n)</a></li>
<li class="chapter" data-level="14.4.2" data-path="graph-neural-network.html"><a href="graph-neural-network.html#ejemplo-clasificaci%C3%B3n-de-art%C3%ADculos-de-investigaci%C3%B3n-por-categor%C3%ADa"><i class="fa fa-check"></i><b>14.4.2</b> Ejemplo: Clasificaci√≥n de art√≠culos de investigaci√≥n por categor√≠a</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="graph-neural-network.html"><a href="graph-neural-network.html#graph-attention-network-gat"><i class="fa fa-check"></i><b>14.5</b> Graph Attention Network (GAT)</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="graph-neural-network.html"><a href="graph-neural-network.html#c%C3%B3mo-funcionan-1"><i class="fa fa-check"></i><b>14.5.1</b> ¬øC√≥mo funcionan?</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="graph-neural-network.html"><a href="graph-neural-network.html#implementaci%C3%B3n-en-pytorch-geometric-1"><i class="fa fa-check"></i><b>14.6</b> Implementaci√≥n en PyTorch Geometric</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="graph-neural-network.html"><a href="graph-neural-network.html#ejemplo-clasificaci%C3%B3n-de-art%C3%ADculos-de-investigaci%C3%B3n-por-categor%C3%ADa-parte-ii"><i class="fa fa-check"></i><b>14.6.1</b> Ejemplo: Clasificaci√≥n de art√≠culos de investigaci√≥n por categor√≠a (Parte II)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html"><i class="fa fa-check"></i><b>15</b> Embeddings y Graph Neural Network</a>
<ul>
<li class="chapter" data-level="15.1" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#qu%C3%A9-es-un-embedding"><i class="fa fa-check"></i><b>15.1</b> ¬øQu√© es un embedding?</a></li>
<li class="chapter" data-level="15.2" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#word2vec"><i class="fa fa-check"></i><b>15.2</b> Word2Vec</a></li>
<li class="chapter" data-level="15.3" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#node2vec"><i class="fa fa-check"></i><b>15.3</b> Node2Vec</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#ejemplo-aplicando-el-modelo-de-skipgrams-a-random-walks"><i class="fa fa-check"></i><b>15.3.1</b> Ejemplo: Aplicando el modelo de SkipGrams a Random Walks</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#sageconv"><i class="fa fa-check"></i><b>15.4</b> SageConv</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#ejemplo-clasificaci%C3%B3n-de-art%C3%ADculos-de-investigaci%C3%B3n-por-categor%C3%ADa-parte-ii-1"><i class="fa fa-check"></i><b>15.4.1</b> Ejemplo: Clasificaci√≥n de art√≠culos de investigaci√≥n por categor√≠a (Parte II)</a></li>
<li class="chapter" data-level="15.4.2" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#ejemplo-clasificaci%C3%B3n-de-art%C3%ADculos-de-investigaci%C3%B3n-por-categor%C3%ADa-parte-ii-2"><i class="fa fa-check"></i><b>15.4.2</b> Ejemplo: Clasificaci√≥n de art√≠culos de investigaci√≥n por categor√≠a (Parte II)</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="./"><img src="img/aif-logo.png" width="280"></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="redes-neuronales-lineales-para-clasificaci√≥n" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Cap√≠tulo 4</span> Redes neuronales Lineales para Clasificaci√≥n<a href="redes-neuronales-lineales-para-clasificaci√≥n.html#redes-neuronales-lineales-para-clasificaci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="modelos-lineales-generalizados" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Modelos Lineales Generalizados<a href="redes-neuronales-lineales-para-clasificaci√≥n.html#modelos-lineales-generalizados" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="historia-1" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Historia<a href="redes-neuronales-lineales-para-clasificaci√≥n.html#historia-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Regresi√≥n lineal</strong></p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig1.png" alt="" width="50%" style="display: block; margin: auto;" />
El primer m√©todo de regresi√≥n lineal documentado es el m√©todo de los m√≠nimos cuadrados, publicado por Legendre en 1805.
Posteriormente, Gauss public√≥ un trabajo donde se desarrolla con mayor detalle el tema.</p>
<p><strong>Modelo Lineal Generalizado (GLM)</strong></p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig2.png" alt="" width="60%" style="display: block; margin: auto;" />
Los GLM fueron formulados por John Nelder y Robert Wedderburn en 1972 como una manera de unificar modelos estad√≠sticos.</p>
<p>Un modelo lineal generalizado es una generalizaci√≥n flexible de la regresi√≥n lineal ordinaria.</p>
</div>
<div id="definici√≥n" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Definici√≥n<a href="redes-neuronales-lineales-para-clasificaci√≥n.html#definici%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Los Modelos Lineales Generalizados (GLM) son una extensi√≥n de los modelos lineales tradicionales que permiten modelar variables respuesta que no necesariamente siguen una distribuci√≥n normal (por ejemplo, binarias, de conteo o proporciones). Relacionan la distribuci√≥n aleatoria de la variable dependiente (variable objetivo) en el experimento, con la parte sistem√°tica a trav√©s de una funci√≥n llamada funci√≥n de enlace.</p>
<p>Por lo tanto, un modelo lineal generalizado, tiene tres componentes b√°sicos:</p>
<ul>
<li><strong>Componente aleatorio</strong> <em>(Y ~ Distribuci√≥n de probabilidad)</em>: Identifica la variable de objetivo y su distribuci√≥n de probabilidad.</li>
<li><strong>Componente sistem√°tica</strong> <em>(ùúÇ = xW)</em>: Especifica las variables explicativas de la funci√≥n predictora lineal.</li>
<li><strong>Funci√≥n de enlace</strong> <em>(ùëî(ùùÅ) = ùúÇ)</em>: Es una funci√≥n del valor esperado de Y como una combinaci√≥n lineal de las variables explicativas</li>
</ul>
<p>Algebraicamente:
<span class="math display">\[
    ùîº(ùíÄ)=ùùÅ=ùëî^{‚àí1}(WX)
\]</span></p>
<p>Los estudios de Wedderburn se limitaron los componentes aleatorios a la familia exponencial, cuya aplicaci√≥n se puede dar de la siguiente manera:</p>
<ul>
<li><strong>Normal</strong>: regresi√≥n lineal</li>
<li><strong>Binomial</strong>: regresi√≥n log√≠stica</li>
<li><strong>Poisson</strong>: regresi√≥n de conteo</li>
<li><strong>Gamma</strong>: regresi√≥n para tiempos o tasas positivas</li>
<li>Inversa Gaussiana, etc.</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Distribuci√≥n</th>
<th>Funci√≥n de Enlace</th>
<th>Nombre de modelo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Normal</td>
<td>g(Œº) = Œº</td>
<td>Regresi√≥n lineal</td>
</tr>
<tr class="even">
<td>Binomial</td>
<td>g(Œº) = log(Œº /(1‚àíŒº))</td>
<td>Regresi√≥n log√≠stica</td>
</tr>
<tr class="odd">
<td>Poisson</td>
<td>g(Œº) = log(Œº)</td>
<td>Regresi√≥n de Poisson</td>
</tr>
</tbody>
</table>
<p><strong>Representaci√≥n gr√°fica</strong></p>
<p>Podemos apreciar una regresi√≥n lineal como una neurona:
<img src="img/04_Linear_Neural_Networks_for_Classification/fig18.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
<p>Haciendo uso de una regresi√≥n log√≠stica, podemos pasar de <em>how much?</em> a <em>wich class?</em>, el diagrama ser√≠a el siguiente:
<img src="img/04_Linear_Neural_Networks_for_Classification/fig19.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="regresi√≥n-log√≠stica-para-clasificaci√≥n" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Regresi√≥n Log√≠stica para Clasificaci√≥n<a href="redes-neuronales-lineales-para-clasificaci√≥n.html#regresi%C3%B3n-log%C3%ADstica-para-clasificaci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Iniciemos recordando que los algoritmos de Machine Learning pueden ser clasificados como:
<img src="img/04_Linear_Neural_Networks_for_Classification/fig5.png" alt="" width="40%" style="display: block; margin: auto;" /></p>
<p>Los <strong><em>m√©todos de clasificaci√≥n, m√©todos predictivos, reconocimiento de patrones, aprendizaje supervisado</em></strong> son aquellos algoritmos en los cuales vamos a clasificar a los objetos en un conjunto de categor√≠as previamente definidas.</p>
<p>En estos m√©todos existe una variable, conocida como variable objetivo con la cual se buscar√°n dos resultados:</p>
<ul>
<li><strong>Evidenciar</strong> la raz√≥n por la cual existen las clases en cuesti√≥n, es decir, encontrar factores que puedan agrupar a los individuos en sus respectivos grupos.</li>
<li>Proyectar el ingreso de <strong>nuevos individuos</strong> en alguna de las caracter√≠sticas anteriores.</li>
</ul>
<p><strong>Definici√≥n</strong></p>
<p>La clasificaci√≥n es la tarea de <strong>aprender una funci√≥n, f,</strong> que pueda mapear cada conjunto de atributos a una categor√≠a predefinida.
Busca predecir la clase a la cual pertenece un registro.
La diferencia principal contra el an√°lisis de conglomerados es el deseo de predecir a nuevos individuos.</p>
<p>Buscamos una funci√≥n que vaya de D a C
<span class="math display">\[
ùëì:ùê∑‚Üí Y
\]</span>
Donde:</p>
<ul>
<li><span class="math inline">\(ùê∑={(ùë•_ùëñ,ùë¶_ùëñ) | ùëñ=1,2,‚Ä¶,ùëÅ}\)</span> es el conjunto de individuos, incluyendo <span class="math inline">\(ùë•_ùëñ\)</span> corresponde a las caracter√≠sticas explicativas de un individuo y <span class="math inline">\(ùë¶_ùëñ\)</span> corresponde al valor de la clase</li>
<li><span class="math inline">\(Y={y _1,  y_2,‚Ä¶, y_ùëö}\)</span> es el conjunto de clases.</li>
</ul>
<p>La funci√≥n resultante puede ser un √°rbol de decisi√≥n, SVM, etc.</p>
<p>La funci√≥n que clasificar√° el conjunto X a la clase Y, com√∫nmente es llamada modelo de clasificaci√≥n.
<img src="img/04_Linear_Neural_Networks_for_Classification/fig6.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
<p>Una vez recordado lo anterior, podemos definir a una regresi√≥n log√≠stica como un modelo en el cual se recibe una tabla relacional con <em>d</em> caracter√≠sticas descriptivas y una clase a la cual pertenece cada individuo de la tabla.</p>
<p>Como resultado, el modelo nos proporciona la <em>probabilidad de pertenecer</em> a alguna clase.</p>
<p><strong>Planteamiento del problema</strong></p>
<p>El primer paso, consistir√≠a en construir el modelo de regresi√≥n, de tal forma que el componente aleatorio (<span class="math inline">\(y_i\)</span>) se conecte con la componente sist√©mica (regresi√≥n lineal):
<span class="math display">\[
ùë¶_ùëñ‚ÜîùõΩ_0+ùõΩ_1 ùë•_ùëñ+ùúñ_ùëñ
\]</span></p>
<hr />
<p>Vamos a suponer un modelo de <strong>regresi√≥n lineal cl√°sica</strong> de tal forma que:
<span class="math display">\[
ùîº(ùë¶_ùëñ | ùë•_ùëñ )=ùõΩ_0+ùõΩ_1 ùë•_ùëñ
\]</span>
Al intentar buscar probabilidades a trav√©s de una regresi√≥n lineal, obtendr√≠amos lo siguiente:</p>
<p>Sea <span class="math inline">\(ùëù_ùëñ\)</span> la probabilidad de pertenecer a la clase 1 dadas las caracter√≠sticas del individuo:
<span class="math display">\[
ùëù_ùëñ=‚Ñô(ùë¶=1|ùë•_ùëñ )
\]</span></p>
<p>Podemos asumir que <span class="math inline">\(ùëù_ùëñ\)</span> sigue una distribuci√≥n Bernoulli, de tal forma que su esperanza sea:
<span class="math display">\[
ùîº(ùë¶|ùë•_ùëñ )=ùëù_ùëñ√ó1+(1‚àíùëù_ùëñ )√ó0=ùëù_ùëñ
\]</span>
De tal forma que:
<span class="math display">\[
ùëù_ùëñ= ùõΩ_0+ùõΩ_1 ùë•_ùëñ
\]</span>
No obstante, los resultados pueden resultar absurdos debido a que nuestra f√≥rmula nos puede proporcionar <strong>probabilidades negativas</strong> o <strong>mayores a cero</strong> <span class="math inline">\(!\)</span></p>
<hr />
<p>Las probabilidades no tienen valores superiores a 1 ni negativas, pero el concepto de momios <span class="math inline">\(p / (1-p)\)</span> tiene como imagen de cero a infinito. La funci√≥n exponencial tiene la misma imagen, entonces, la regresi√≥n log√≠stica relaciona ambos concpetos a trav√©s de la siguiente igualdad
<span class="math display">\[
‚Ñô(ùë¶=1|ùë•_ùëñ )/(1‚àí‚Ñô(ùë¶=1|ùë•_ùëñ ) )=ùëí^{(ùõΩ_0+ùõΩ_1 ùë•_ùëñ )}
\]</span>
Al aplicar el logaritmo natual, podemos apreciar la definici√≥n de la funci√≥n logit:
<span class="math display">\[
logit[ùëù_ùëñ ]=ln[ùëù_ùëñ/(1‚àíùëù_ùëñ )]=ùõΩ_0+ùõΩ_1 ùë•_ùëñ
\]</span>
Recordando que la funci√≥n logit es la inversa de la funci√≥n logistica (o sigmoide)</p>
<p><img src="deep_learning_course_python_files/figure-html/unnamed-chunk-31-1.png" alt="" width="576" style="display: block; margin: auto;" /></p>
<p>La funci√≥n logit es apropiada para los problemas de clasificaci√≥n con dos clases. No obstante, la generalizaci√≥n para m√∫ltiples clases se hace con la funci√≥n Softmax()
<span class="math display">\[
\sigma_{softmax} = \frac{e^{z_m}}{\sum_{m=1}^M e^{z_m}}
\]</span></p>
<p>La estructura aplicada en Redes Neuronales es la siguiente:
<img src="img/04_Linear_Neural_Networks_for_Classification/fig20.png" alt="" width="70%" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="redes-neuronales" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Redes neuronales<a href="redes-neuronales-lineales-para-clasificaci√≥n.html#redes-neuronales" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="historia-2" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Historia<a href="redes-neuronales-lineales-para-clasificaci√≥n.html#historia-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Redes Neuronales</strong></p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig3.png" alt="" width="50%" style="display: block; margin: auto;" />
En 1943 Warren McCulloch (neurocient√≠fico, m√©dico, neur√≥logo y fisi√≥logo) y Walter Pitts (matem√°tico, psic√≥logo, fil√≥sofo y neurocient√≠fico) crearon un modelo para redes neuronales basados en la l√≥gica de umbral. Este modelo se√±al√≥ el camino para que la investigaci√≥n de redes neuronales se divida en dos enfoques distintos:</p>
<ol style="list-style-type: decimal">
<li>Un enfoque centrado en los procesos biol√≥gicos en el cerebro.</li>
<li>Otro en la aplicaci√≥n de redes neuronales para la inteligencia artificial.</li>
</ol>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig4.png" alt="" width="40%" style="display: block; margin: auto;" />
Para la aplicaci√≥n de la inteligencia artificial, fue Marvin Minsky quien dio una gran aportaci√≥n con el libro <strong>‚ÄúPerceptrones‚Äù</strong> en 1969.
Minsky es considerado como uno de los padres de las ciencias de la computaci√≥n y fue cofundador del laboratorio de inteligencia artificial del MIT</p>
</div>
<div id="definici√≥n-1" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Definici√≥n<a href="redes-neuronales-lineales-para-clasificaci√≥n.html#definici%C3%B3n-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Las redes neuronales son algoritmos que tienen como objetivo el simular en una computadora la forma en la cual funcionan las redes neuronales en los humanos.
Puede recibir variables cuantitativas y cualitativas. Es un algoritmo sensible a cambios en los datos y par√°metros</p>
</div>
<div id="neurona" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Neurona<a href="redes-neuronales-lineales-para-clasificaci√≥n.html#neurona" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>C√©lula del sistema nerviosos formada por un n√∫cleo y una serie de prolongaciones, una de las cuales es m√°s larga que las dem√°s.
Est√°n especializadas en la recepci√≥n de est√≠mulos y conducci√≥n del impulso nerviosos entre ellas o con otros tipos celulas</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig7.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
<p><br></p>
<p><strong>Dentrita:</strong> Es la fuente de un impulso nerviosos. De hecho el impulso nervioso es unidireccional, es decir, solamente se transmite desde las dendritas hacia el ax√≥n (canal de entrada)</p>
<p><strong>Ax√≥n:</strong> Prolongaci√≥n que arranca del cuerpo de la neurona y termina en una ramificaci√≥n que est√° en contacto con otras c√©lulas. Es la v√≠a por la cual circulan los impulsos el√©ctricos (canal de salida)</p>
<p><strong>Sinapsis:</strong> Regi√≥n de comunicaci√≥n entre el ax√≥n de una neurona y las dendritas o el cuerpo de otra</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig8.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
<p><strong>Emulaci√≥n:</strong></p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig9.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
<p>Las redes neuronales, tratan de modelar una red neuronal donde cada nodo es una neurona, y los arcos representan la sinapsis entre las neuronas.
Como algoritmo, las redes neuronales reciben n entradas y pueden otorgar n salidas.</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig10.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
<p>Un perceptr√≥n puede entenderse como la unidad b√°sica de inferencia en forma de discriminador lineal. Un perceptr√≥n va a separar a los puntos a trav√©s de un hiperplano
Es el modelo biol√≥gico m√°s sencillo hace referencia a una sola neurona. El perceptr√≥n consiste de dos tipos de nodos:</p>
<ul>
<li>Nodos de entrada: ingreso de los atributos</li>
<li>Nodos de salida: representa a los resultados del modelo</li>
</ul>
<p>Un perceptr√≥n genera sus resultados realizando una suma ponderada de sus atributos de entrada, restando un valor t y examinando el signo del resultado.
Supongamos la siguiente tabla:</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig11.png" alt="" width="30%" style="display: block; margin: auto;" /></p>
<p>El perceptr√≥n ser√≠a aquella red neuronal que recibiera de entrada cada uno de los valores x y diera como resultado y</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig12.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
<p><span class="math display">\[
y =
\begin{cases}
    \text{1, si:  } 0.3ùë•_1+0.3ùë•_1+0.3ùë•_1‚àí0.4 ‚â• 0 \\
    \text{-1, si:  } 0.3ùë•_1+0.3ùë•_1+0.3ùë•_1‚àí0.4 &lt; 0 \\
\end{cases}
\]</span>
Como el resultado depende del signo, la funci√≥n se representa como:
<span class="math display">\[
ùë¶=ùë†ùëñùëîùëõ(ùë§,ùë•)
\]</span></p>
<p>Cuando intentamos hacer la separaci√≥n de un XOr, podemos notar que necesitamos dos discriminadores lineales:</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig13.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="redes-neuronales-1" class="section level3 hasAnchor" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Redes Neuronales<a href="redes-neuronales-lineales-para-clasificaci√≥n.html#redes-neuronales-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Las redes neuronales, son una estructura m√°s compleja que un perceptr√≥n. Algunas de las complejidades son:</p>
<ul>
<li>Las redes neuronales tienen <strong>varios nodos intermedios</strong> entre los nodos de entrada y los nodos de salida.</li>
<li>Las capas ocultas pueden utilizar <strong>funciones de activaci√≥n diferentes</strong></li>
</ul>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig14.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig15.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
<p>Con una capa oculta, es posible generar una funci√≥n de discriminaci√≥n para una tabla XOr</p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig17.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
<p><strong>Funciones de activaci√≥n comunes</strong></p>
<p><img src="img/04_Linear_Neural_Networks_for_Classification/fig16.png" alt="" width="60%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="notas-adicionales" class="section level2 unnumbered hasAnchor">
<h2>Notas adicionales<a href="redes-neuronales-lineales-para-clasificaci√≥n.html#notas-adicionales" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Curso de Git: <a href="https://www.youtube.com/watch?v=3GymExBkKjE&amp;t=195s" class="uri">https://www.youtube.com/watch?v=3GymExBkKjE&amp;t=195s</a></p>

<!-- ::: watermark -->
<!-- <img src="img/aif-logo.png" width="400"/> -->
<!-- ::: -->
</div>
</div>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script>

$('.pipehover_incremental tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).prevAll().addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});


$('.pipehover_select_one_row tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});

</script>
            </section>

          </div>
        </div>
      </div>
<a href="redes-neuronales-lineales-para-regresi√≥n.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="perceptr√≥n-multicapa.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["deep_learning_course_python.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
