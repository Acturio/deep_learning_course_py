<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 2 Preliminares | Deep Learning</title>
  <meta name="description" content="Capítulo 2 Preliminares | Deep Learning" />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 2 Preliminares | Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Capítulo 2 Preliminares | Deep Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 2 Preliminares | Deep Learning" />
  
  <meta name="twitter:description" content="Capítulo 2 Preliminares | Deep Learning" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introducción-a-deep-learning.html"/>
<link rel="next" href="redes-neuronales-lineales-para-regresión.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="img/aif-logo.png" width="280"></a></li|
|:-:|  
<center>Curso de Redes Neuronales Artificiales</center>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>BIENVENIDA</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#objetivo"><i class="fa fa-check"></i>Objetivo</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#alcances-del-programa"><i class="fa fa-check"></i>Alcances del Programa</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#temario"><i class="fa fa-check"></i>Temario:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#c%C3%B3digo"><i class="fa fa-check"></i>Código</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#duraci%C3%B3n-y-evaluaci%C3%B3n-del-programa"><i class="fa fa-check"></i>Duración y evaluación del programa</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recursos-y-din%C3%A1mica"><i class="fa fa-check"></i>Recursos y dinámica</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agenda"><i class="fa fa-check"></i>Agenda</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bibliograf%C3%ADa"><i class="fa fa-check"></i>Bibliografía</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Parte 1: Bases y Preeliminares</b></span></li>
<li class="chapter" data-level="1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html"><i class="fa fa-check"></i><b>1</b> Introducción a Deep Learning</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#introducci%C3%B3n-al-aprendizaje-autom%C3%A1tico"><i class="fa fa-check"></i><b>1.1</b> Introducción al aprendizaje automático</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#motivaci%C3%B3n-los-paradigmas-del-conocimiento"><i class="fa fa-check"></i><b>1.1.1</b> Motivación: Los paradigmas del conocimiento</a></li>
<li class="chapter" data-level="1.1.2" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#machine-learning-supervisado"><i class="fa fa-check"></i><b>1.1.2</b> Machine learning supervisado</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#por-qu%C3%A9-deep-learning"><i class="fa fa-check"></i><b>1.2</b> ¿Por qué Deep Learning?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#dimensi%C3%B3n-vc"><i class="fa fa-check"></i><b>1.2.1</b> Dimensión VC</a></li>
<li class="chapter" data-level="1.2.2" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#el-truco-del-kernel-svm"><i class="fa fa-check"></i><b>1.2.2</b> El truco del Kernel (SVM)</a></li>
<li class="chapter" data-level="1.2.3" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#ingenier%C3%ADa-de-caracteristicas"><i class="fa fa-check"></i><b>1.2.3</b> Ingeniería de caracteristicas</a></li>
<li class="chapter" data-level="1.2.4" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#beneficios-del-deep-learning"><i class="fa fa-check"></i><b>1.2.4</b> Beneficios del deep learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="preliminares.html"><a href="preliminares.html"><i class="fa fa-check"></i><b>2</b> Preliminares</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminares.html"><a href="preliminares.html#algebra-lineal"><i class="fa fa-check"></i><b>2.1</b> Algebra Lineal</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="preliminares.html"><a href="preliminares.html#introducci%C3%B3n"><i class="fa fa-check"></i><b>2.1.1</b> Introducción</a></li>
<li class="chapter" data-level="2.1.2" data-path="preliminares.html"><a href="preliminares.html#motivaci%C3%B3n"><i class="fa fa-check"></i><b>2.1.2</b> Motivación</a></li>
<li class="chapter" data-level="2.1.3" data-path="preliminares.html"><a href="preliminares.html#escalares"><i class="fa fa-check"></i><b>2.1.3</b> Escalares</a></li>
<li class="chapter" data-level="2.1.4" data-path="preliminares.html"><a href="preliminares.html#vectores-1"><i class="fa fa-check"></i><b>2.1.4</b> Vectores</a></li>
<li class="chapter" data-level="2.1.5" data-path="preliminares.html"><a href="preliminares.html#matrices-1"><i class="fa fa-check"></i><b>2.1.5</b> Matrices</a></li>
<li class="chapter" data-level="2.1.6" data-path="preliminares.html"><a href="preliminares.html#tensores"><i class="fa fa-check"></i><b>2.1.6</b> Tensores</a></li>
<li class="chapter" data-level="2.1.7" data-path="preliminares.html"><a href="preliminares.html#reducciones-sobre-tensores"><i class="fa fa-check"></i><b>2.1.7</b> Reducciones sobre tensores</a></li>
<li class="chapter" data-level="2.1.8" data-path="preliminares.html"><a href="preliminares.html#producto-punto"><i class="fa fa-check"></i><b>2.1.8</b> Producto punto</a></li>
<li class="chapter" data-level="2.1.9" data-path="preliminares.html"><a href="preliminares.html#producto-entre-matrices-y-vectores"><i class="fa fa-check"></i><b>2.1.9</b> Producto entre matrices y vectores</a></li>
<li class="chapter" data-level="2.1.10" data-path="preliminares.html"><a href="preliminares.html#multiplicaci%C3%B3n-matricial"><i class="fa fa-check"></i><b>2.1.10</b> Multiplicación matricial</a></li>
<li class="chapter" data-level="2.1.11" data-path="preliminares.html"><a href="preliminares.html#normas"><i class="fa fa-check"></i><b>2.1.11</b> Normas</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="preliminares.html"><a href="preliminares.html#c%C3%A1lculo-diferencial"><i class="fa fa-check"></i><b>2.2</b> Cálculo Diferencial</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="preliminares.html"><a href="preliminares.html#introducci%C3%B3n-1"><i class="fa fa-check"></i><b>2.2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2.2" data-path="preliminares.html"><a href="preliminares.html#motivaci%C3%B3n-1"><i class="fa fa-check"></i><b>2.2.2</b> Motivación</a></li>
<li class="chapter" data-level="2.2.3" data-path="preliminares.html"><a href="preliminares.html#derivadas-y-diferenciaci%C3%B3n"><i class="fa fa-check"></i><b>2.2.3</b> Derivadas y diferenciación</a></li>
<li class="chapter" data-level="2.2.4" data-path="preliminares.html"><a href="preliminares.html#regla-de-la-cadena"><i class="fa fa-check"></i><b>2.2.4</b> Regla de la cadena</a></li>
<li class="chapter" data-level="2.2.5" data-path="preliminares.html"><a href="preliminares.html#interpretaci%C3%B3n-geom%C3%A9trica-de-la-derivada"><i class="fa fa-check"></i><b>2.2.5</b> Interpretación geométrica de la derivada</a></li>
<li class="chapter" data-level="2.2.6" data-path="preliminares.html"><a href="preliminares.html#teorema-fundamental-del-c%C3%A1lculo"><i class="fa fa-check"></i><b>2.2.6</b> Teorema fundamental del cálculo</a></li>
<li class="chapter" data-level="2.2.7" data-path="preliminares.html"><a href="preliminares.html#autodiferenciaci%C3%B3n"><i class="fa fa-check"></i><b>2.2.7</b> Autodiferenciación</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html"><i class="fa fa-check"></i><b>3</b> Redes neuronales Lineales para Regresión</a></li>
<li class="chapter" data-level="4" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html"><i class="fa fa-check"></i><b>4</b> Redes neuronales Lineales para Clasificación</a></li>
<li class="chapter" data-level="5" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html"><i class="fa fa-check"></i><b>5</b> Perceptrón Multicapa</a></li>
<li class="part"><span><b>Parte 2: Técnicas Modernas de Deep Learning</b></span></li>
<li class="chapter" data-level="6" data-path="guía-del-constructor.html"><a href="guía-del-constructor.html"><i class="fa fa-check"></i><b>6</b> Guía del Constructor</a></li>
<li class="chapter" data-level="7" data-path="redes-neuronales-convolucionales.html"><a href="redes-neuronales-convolucionales.html"><i class="fa fa-check"></i><b>7</b> Redes Neuronales Convolucionales</a></li>
<li class="chapter" data-level="8" data-path="redes-neuronales-convolucionales-modernas.html"><a href="redes-neuronales-convolucionales-modernas.html"><i class="fa fa-check"></i><b>8</b> Redes Neuronales Convolucionales Modernas</a></li>
<li class="chapter" data-level="9" data-path="redes-neuronales-recurrentes.html"><a href="redes-neuronales-recurrentes.html"><i class="fa fa-check"></i><b>9</b> Redes Neuronales Recurrentes</a></li>
<li class="chapter" data-level="10" data-path="redes-neuronales-recurrentes-modernas.html"><a href="redes-neuronales-recurrentes-modernas.html"><i class="fa fa-check"></i><b>10</b> Redes Neuronales Recurrentes Modernas</a></li>
<li class="chapter" data-level="11" data-path="mecanismos-de-atención-y-transformers.html"><a href="mecanismos-de-atención-y-transformers.html"><i class="fa fa-check"></i><b>11</b> Mecanismos de Atención y Transformers</a></li>
<li class="part"><span><b>Parte 3: Escalabilidad, Eficiencia y Aplicaciones</b></span></li>
<li class="chapter" data-level="12" data-path="algoritmos-de-optimización.html"><a href="algoritmos-de-optimización.html"><i class="fa fa-check"></i><b>12</b> Algoritmos de Optimización</a></li>
<li class="divider"></li>
<li><a href="./"><img src="img/aif-logo.png" width="280"></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="preliminares" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Capítulo 2</span> Preliminares<a href="preliminares.html#preliminares" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="algebra-lineal" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Algebra Lineal<a href="preliminares.html#algebra-lineal" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="introducción" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Introducción<a href="preliminares.html#introducci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El álgebra lineal es la rama de las matemáticas que estudia los espacios vectoriales y transformaciones lineales así como los elementos que son parte de estas como son los vectores, matrices, bases, operadores así como las propiedades geométricas que ocurren en los espacios vectoriales.</p>
<p>Enfocandose en como se representan y manipulan los datos que pueden tener múltiples dimensiones o características de una forma estructurada y dando así, interpretaciones de estos con una serie de observaciones, estructura y rigor matemático.</p>
</div>
<div id="motivación" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Motivación<a href="preliminares.html#motivaci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El aprendizaje profundo se basa en procesar y transformar datos numéricos (imágenes, sonidos, textos, etc) y dichas transformaciones son operaciones lineales y no lineales aplicadas repetidamente, encontramos una forma muy natural de hacer uso del álgebra lineal para el aprendizaje profundo, representado como el procesamiento de redes neuronales.</p>
<p>De forma natural, podemos ver como las entradas, salidas, neuronas y funciones de activación hacen uso de estructuras como vectores, matrices y transformaciones lineales:</p>
<div id="vectores" class="section level4 hasAnchor" number="2.1.2.1">
<h4><span class="header-section-number">2.1.2.1</span> Vectores<a href="preliminares.html#vectores" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Representan entradas, salidas y parámetros.</p>
</div>
<div id="matrices" class="section level4 hasAnchor" number="2.1.2.2">
<h4><span class="header-section-number">2.1.2.2</span> Matrices<a href="preliminares.html#matrices" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Representan conexiones entre neuronas así como transformaciones entre espacios. Esto porque una red neuronal puede expresarse como
<span class="math display">\[
    y = Wx + b
\]</span>
donde:</p>
<ul>
<li><span class="math inline">\(x\)</span> es el vector de entrada,</li>
<li><span class="math inline">\(W\)</span> es una matriz de pesos.</li>
<li><span class="math inline">\(b\)</span> es el sesgo,</li>
<li><span class="math inline">\(y\)</span> es la salida.</li>
</ul>
</div>
<div id="transformaciones-lineales" class="section level4 hasAnchor" number="2.1.2.3">
<h4><span class="header-section-number">2.1.2.3</span> Transformaciones lineales<a href="preliminares.html#transformaciones-lineales" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Cada capa lineal en una red neuronal transforma el espacio de las entradas. Estas transformaciones cambian orientación, posición, o escala de los datos en un espacio multidimensional.</p>
</div>
</div>
<div id="escalares" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Escalares<a href="preliminares.html#escalares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="campos-algebraicos" class="section level4 hasAnchor" number="2.1.3.1">
<h4><span class="header-section-number">2.1.3.1</span> Campos Algebraicos<a href="preliminares.html#campos-algebraicos" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Un <strong>campo algebraico</strong> (o simplemente <em>campo</em>) es una estructura matemática formada por un conjunto de elementos en el que se pueden realizar las operaciones de <strong>suma</strong>, <strong>resta</strong>, <strong>multiplicación</strong> y <strong>división</strong> (excepto por cero), cumpliendo ciertas reglas de consistencia.</p>
<p>Algunos ejemplos de campos algebraicos son los números reales (denotado por <span class="math inline">\(\mathbb{R}\)</span>) y los números complejos (denotado por <span class="math inline">\(\mathbb{C}\)</span>).</p>
<p>En este contexto, cuando hablamos de escalares, nos referiremos a un elemento del campo sobre el que trabajamos, usualmente sobre los números reales, o en ocasiones particulares, números imaginarios.</p>
</div>
<div id="dato-curioso" class="section level4 hasAnchor" number="2.1.3.2">
<h4><span class="header-section-number">2.1.3.2</span> Dato curioso<a href="preliminares.html#dato-curioso" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Si en lugar de un campo tenemos un anillo algebraico (a diferencia del campo, no necesariamente existe un inverso multiplicativo), entonces los escalares perteneceran al anillo, y en lugar de tener un espacio vectorial, diremos que tendremos un <strong>módulo</strong> y eso es campo de estudio de otra rama de las matemáticas. Por lo que en este contexto, únicamente trataremos con los números reales como campo y posiblemente con complejos.</p>
</div>
</div>
<div id="vectores-1" class="section level3 hasAnchor" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Vectores<a href="preliminares.html#vectores-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Un vector es un objeto matemático que posee magnitud y dirección. Estos pertenecen a un espacio vectorial que está definido sobre un campo y por ende, las entradas de un vector, son elementos de este campo (denotado como escalares), así como dichas entradas es una lista ordenada.
<span class="math display">\[
v =
\begin{pmatrix}
    v_1 \\
    v_2 \\
    \vdots \\
    v_n
\end{pmatrix}
\]</span>
Estos números son las coordenadas del vector respecto a la base del espacio.</p>
</div>
<div id="matrices-1" class="section level3 hasAnchor" number="2.1.5">
<h3><span class="header-section-number">2.1.5</span> Matrices<a href="preliminares.html#matrices-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Una matríz es un arreglo rectangular de escalares (o con entradas en el campo vectorial), organizada en filas y columnas de la siguiente forma:
<span class="math display">\[
A =
\begin{pmatrix}
    a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\
    a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn}
\end{pmatrix}
\]</span>
donde <span class="math inline">\(A\)</span> tiene <span class="math inline">\(m\)</span> filas y <span class="math inline">\(n\)</span> columnas, y se dice que es de tamaño <span class="math inline">\(m \times n\)</span>.</p>
<div id="alternativas-conceptuales-para-entenderlas" class="section level4 hasAnchor" number="2.1.5.1">
<h4><span class="header-section-number">2.1.5.1</span> Alternativas conceptuales para entenderlas<a href="preliminares.html#alternativas-conceptuales-para-entenderlas" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Una matríz puede entenderse como:</p>
<ul>
<li>Un conjunto de vectores organizados de manera estructurada.</li>
<li>Una herramienta para transformar un vector en otro mediante operaciones lineales.</li>
<li>Una representación compacta de un sistema de ecuaciones lineales.</li>
</ul>
<p>Por dar un ejemplo, la expresión <span class="math inline">\(y = Ax\)</span> con <span class="math inline">\(x, y\)</span> como vectores y <span class="math inline">\(A\)</span> como matríz, denota como se transforma <span class="math inline">\(x\)</span> en <span class="math inline">\(y\)</span> por medio de <span class="math inline">\(A\)</span>, o un sistema de ecuaciones lineales.</p>
</div>
<div id="propiedades-matemáticas" class="section level4 hasAnchor" number="2.1.5.2">
<h4><span class="header-section-number">2.1.5.2</span> Propiedades matemáticas<a href="preliminares.html#propiedades-matem%C3%A1ticas" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Las matrices permiten realizar:</p>
<ul>
<li>Suma y multiplicación de transformaciones lineales.</li>
<li>Cálculo de determinante e inversas de matrices (en caso de existir).</li>
<li>Cambios de base, rotaciones, escalamientos y proyecciones.</li>
</ul>
<p>En esencia podemos pensar que son la forma algebraica de expresar operaciones lineales entre espacios vectoriales.</p>
</div>
<div id="en-el-aprendizaje-profundo" class="section level4 hasAnchor" number="2.1.5.3">
<h4><span class="header-section-number">2.1.5.3</span> En el aprendizaje profundo<a href="preliminares.html#en-el-aprendizaje-profundo" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Las matrices son un componente clave en el núcleo operativo de las redes neuronales considerando las siguientes observaciones:</p>
<ul>
<li>Cada capa lineal se representa por una matríz de pesos que transforma las entradas en salidas.</li>
<li>Las operaciones de multiplicación matricial permite combinar miles de variables de entrada con miles de parámetros a la vez.</li>
<li>Durante el entrenamiento, las matrices cambian, siendo ajustadas para minimizar el error de predicción.</li>
</ul>
</div>
<div id="resumen" class="section level4 hasAnchor" number="2.1.5.4">
<h4><span class="header-section-number">2.1.5.4</span> Resumen<a href="preliminares.html#resumen" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Las matrices son arreglos ordenados de escalares o vectores que suelen representar transformaciones lineales así como bases vectoriales. Esta es la forma matemática que se usa para describir el procesamiento y transformación numérica que ocurre en una red neuronal.</p>
</div>
</div>
<div id="tensores" class="section level3 hasAnchor" number="2.1.6">
<h3><span class="header-section-number">2.1.6</span> Tensores<a href="preliminares.html#tensores" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>De forma simétrica a las estructuras de escalares, vectores y matrices, un tensor es una estructura matemática que generaliza estos conceptos para describir relaciones lineales y multidimensionales entre conjuntos de datos numéricos.</p>
<p>Una forma de ejemplificar usando como base la relación entre puntos, líneas, cuadrados y cubos puede ser la siguiente: una matríz es a un tensor (arreglo de matrices), como un vector es a una matríz (arreglo de vectores), como un escalar es a un vector (arreglo de escalares).</p>
<div id="intuición-geométrica" class="section level4 hasAnchor" number="2.1.6.1">
<h4><span class="header-section-number">2.1.6.1</span> Intuición geométrica<a href="preliminares.html#intuici%C3%B3n-geom%C3%A9trica" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Un tensor puede verse como un objeto que transforma o relaciona vectores y covectores de manera multilineal, conservando coherencia bajo cambios de coordenadas. Esto en espacios físicos permite describir propiedades como:</p>
<ul>
<li>Fuerza y dirección (vectores).</li>
<li>Tensiones, deformaciones o inercia (tensores de segundo orden).</li>
<li>Curvaturas o transformaciones complejas (tensores de orden superior).</li>
</ul>
</div>
<div id="su-rol-en-el-aprendizaje-profundo" class="section level4 hasAnchor" number="2.1.6.2">
<h4><span class="header-section-number">2.1.6.2</span> Su rol en el aprendizaje profundo<a href="preliminares.html#su-rol-en-el-aprendizaje-profundo" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>En el contexto del aprendizaje profundo, los tensores suelen usarse en un sentido computacional más que geométrico. Por ejemplo:</p>
<ul>
<li>Una imagen puede representarse como un arreglo dimensional de dos dimensiones, pero donde sus elementos son vectores de dimensión 3 (pixeles con sus 3 colores).</li>
<li>Un lote de imágenes forma un tensor de orden 4: número o etiqueta de la imagen, alto, ancho y colores.</li>
<li>Los pesos de una red neuronal y sus gradientes también son tensores.</li>
</ul>
</div>
<div id="propiedades-básicas-de-la-aritmética-de-tensores" class="section level4 hasAnchor" number="2.1.6.3">
<h4><span class="header-section-number">2.1.6.3</span> Propiedades básicas de la aritmética de tensores<a href="preliminares.html#propiedades-b%C3%A1sicas-de-la-aritm%C3%A9tica-de-tensores" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Escalares, vectores, matrices y tensores de orden superior tienen propiedades que son útiles, por ejemplo operaciones elemento a elemento producen salidas que tienen las mismas dimensiones que sus entradas.</p>
<p>El producto de elemento a elemento de dos matrices es llamado <em>producto Hadamard</em> y es denotado con <span class="math inline">\(\odot\)</span>. Podemos ver esto de la siguiente forma para dos matrices <span class="math inline">\(A, B \in \mathbb{R}^{m \times n}\)</span>:
<span class="math display">\[
A \odot B =
\begin{pmatrix}
    a_{11}  b_{11} &amp; a_{12}  b_{12} &amp; \dots  &amp; a_{1n}  b_{1n} \\
    a_{21}  b_{21} &amp; a_{22}  b_{22} &amp; \dots  &amp; a_{2n}  b_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m1}  b_{m1} &amp; a_{m2}  b_{m2} &amp; \dots  &amp; a_{mn}  b_{mn}
\end{pmatrix}
\]</span></p>
</div>
</div>
<div id="reducciones-sobre-tensores" class="section level3 hasAnchor" number="2.1.7">
<h3><span class="header-section-number">2.1.7</span> Reducciones sobre tensores<a href="preliminares.html#reducciones-sobre-tensores" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Una reducción es una operación que toma un tensor y “reduce” una o más de sus dimensiones, combinando los elementos mediante una operación como la suma, el promedio, máximo o cualquier otra función agregadora</p>
<div id="ejemplos" class="section level4 hasAnchor" number="2.1.7.1">
<h4><span class="header-section-number">2.1.7.1</span> Ejemplos<a href="preliminares.html#ejemplos" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suponiendo que tenemos la siguiente matriz
<span class="math display">\[
A =
\begin{pmatrix}
    1 &amp; 2 &amp; 3 \\
    4 &amp; 5 &amp; 6
\end{pmatrix}
\]</span>
Tenemos las siguientes posibilidades de sumas:</p>
<ul>
<li>Suma total: <span class="math inline">\(\mbox{sum}(A) = 1  + 2 + 3 + 4 + 5 + 6 = 21\)</span></li>
<li>Suma por filas: <span class="math inline">\(\mbox{sum}(A, \mbox{axis} = 1) = [6, 16]\)</span> que es un vector (tensor de orden 1).#</li>
<li>Suma por columnas: <span class="math inline">\(\mbox{sum}(A, \mbox{axis} = 0) = [5, 7, 9]\)</span></li>
</ul>
</div>
<div id="intuición-geométrica-1" class="section level4 hasAnchor" number="2.1.7.2">
<h4><span class="header-section-number">2.1.7.2</span> Intuición geométrica<a href="preliminares.html#intuici%C3%B3n-geom%C3%A9trica-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Una reducción colapsa una dirección del espacio del tensor.</p>
<ul>
<li>Dada una matriz, reducir sobre las filas equivale a proyectar todo el espacio de datos sobre el eje de las columnas.</li>
<li>En tensores de orden mayor, esto trata de aplanar parte de la estructura multidimensional.</li>
</ul>
</div>
<div id="en-aprendizaje-profundo" class="section level4 hasAnchor" number="2.1.7.3">
<h4><span class="header-section-number">2.1.7.3</span> En aprendizaje profundo<a href="preliminares.html#en-aprendizaje-profundo" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Las reducciones son operaciones fundamentales en el cálculo dentro de las redes neuronales. Consideremos las siguientes:</p>
<ul>
<li>Función de pérdida:
Se calcula reduciendo las diferencias entre predicciones y etiquetas. Por ejemplo:
<span class="math display">\[
      \mbox{loss} = \mbox{mean}((y_{\mbox{pred}} - y_{\mbox{true}})^2)
  \]</span>
reducción con media sobre todos los ejemplos.</li>
<li>Normalización:
Reducir sobre ciertas dimensiones (por ejemplo, el canal o lote) para calcular medias y desviaciones estándar.</li>
<li>Gradientes:
Durante el entrenamiento, los gradientes a menudo se <em>acumulan</em> (se reducen) sobre los lotes de datos.</li>
</ul>
</div>
<div id="tipos-comunes-de-reducciones" class="section level4 hasAnchor" number="2.1.7.4">
<h4><span class="header-section-number">2.1.7.4</span> Tipos comunes de reducciones<a href="preliminares.html#tipos-comunes-de-reducciones" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<table>
<colgroup>
<col width="20%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead>
<tr class="header">
<th>Operación</th>
<th>Resultado</th>
<th>Uso típico</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>suma</td>
<td>Suma de elementos</td>
<td>Energía total, acumulaciones</td>
</tr>
<tr class="even">
<td>promedio</td>
<td>Promedio</td>
<td>Pérdidas, normalización</td>
</tr>
<tr class="odd">
<td>máximo, mínimo</td>
<td>Valor extremo</td>
<td>Selección, agrupamiento, submuestreo</td>
</tr>
<tr class="even">
<td>producto</td>
<td>Producto total</td>
<td>Escalados, combinaciones</td>
</tr>
<tr class="odd">
<td>norma</td>
<td>Magnitud del tensor</td>
<td>Regularización, análisis geométrico</td>
</tr>
</tbody>
</table>
</div>
<div id="resumen-1" class="section level4 hasAnchor" number="2.1.7.5">
<h4><span class="header-section-number">2.1.7.5</span> Resumen<a href="preliminares.html#resumen-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Podemos pensar en la reducción como el proceso de combinar las entradas a lo largo de ciertas dimensiones, para producir una representación más simple o resumida del tensor original.</p>
<p>Matemáticamente, una reducción es una contracción parcial de índices y computacionalmente, una operación de agregación que permite manejar de forma eficiente grandes volúmenes de datos en redes neuronales.</p>
</div>
</div>
<div id="producto-punto" class="section level3 hasAnchor" number="2.1.8">
<h3><span class="header-section-number">2.1.8</span> Producto punto<a href="preliminares.html#producto-punto" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>De momento hemos estado revisando operaciones tanto de reducción como operaciones elemento a elemento. Pero para el producto punto, tenemos una combinación muy interesante. Ya que por un lado, es una operación binaria que toma dos vectores, y nos regresa una agregación, siendo esta la suma total del producto elemento a elemento. O más matemáticamente, sea <span class="math inline">\(d \in \mathbb{N}\)</span>, dado <span class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^d\)</span> y <span class="math inline">\(\mathbf{x}^\top\)</span> la transpuesta de <span class="math inline">\(\mathbf{x}\)</span>, entonces su producto punto <span class="math inline">\(\mathbf{x} \cdot \mathbf{y}\)</span> ó <span class="math inline">\(\mathbf{x}^\top \mathbf{y}\)</span> (o también conocido como producto interno <span class="math inline">\(\langle \mathbf{x}, \mathbf{y} \rangle\)</span>) se define así:
<span class="math display">\[
    \mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i
\]</span>
Este resultado sin embargo, tiene repercusiones importantes así como interpretaciones geométricas de gran importancia. A este producto también se puede ver como una forma bilineal, es decir, que en ambas entradas, es linear (abre sumas y saca escalares: <span class="math inline">\(f(ax + by) = af(x) + bf(y)\)</span>).</p>
<div id="interpretación-geométrica" class="section level4 hasAnchor" number="2.1.8.1">
<h4><span class="header-section-number">2.1.8.1</span> Interpretación geométrica<a href="preliminares.html#interpretaci%C3%B3n-geom%C3%A9trica" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El producto punto mide <em>cuanto apunta un vector en la dirección de otro</em> o el ángulo que hay entre estos:
<span class="math display">\[
    \mathbf{a} \cdot \mathbf{b} = \|a\| \|b\| \cos(\theta)
\]</span>
donde:</p>
<ul>
<li>Las longitudes de los vectores están representadas por <span class="math inline">\(\|a\|\)</span> y <span class="math inline">\(\|b\|\)</span>,</li>
<li>el ángulo entre estos es <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p>Si:</p>
<ul>
<li>Apuntan en direcciones similares: <span class="math inline">\(\mathbf{a} \cdot \mathbf{b} &gt; 0\)</span>.</li>
<li>Son perpendiculares <span class="math inline">\(\mathbf{a} \cdot \mathbf{b} = 0\)</span>.</li>
<li>Apuntan en direcciones opuestas: <span class="math inline">\(\mathbf{a} \cdot \mathbf{b} &lt; 0\)</span>.</li>
</ul>
</div>
<div id="en-el-aprendizaje-profundo-1" class="section level4 hasAnchor" number="2.1.8.2">
<h4><span class="header-section-number">2.1.8.2</span> En el aprendizaje profundo<a href="preliminares.html#en-el-aprendizaje-profundo-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El producto punto aparece de forma constante en redes neuronales:</p>
<ul>
<li>En cada neurona, el valor de activación se calcula como un producto punto entre el vector de entrada y el vector de pesos:
<span class="math display">\[
      z = \mathbf{w} \cdot \mathbf{x} + b
  \]</span></li>
<li>En el contexto de atención (<em>transformers</em>), se usa para medir la similitud entre representaciones:
<span class="math display">\[
      \mathbf{q} \sim \mathbf{k} = \mbox{sim}(\mathbf{q}, \mathbf{k}) = \mathbf{q} \cdot \mathbf{k}
  \]</span></li>
</ul>
</div>
<div id="resumen-2" class="section level4 hasAnchor" number="2.1.8.3">
<h4><span class="header-section-number">2.1.8.3</span> Resumen<a href="preliminares.html#resumen-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El producto punto es la operación fundamental para medir la relación de direcciones y magnitud entre dos vectores, y en redes neuronales, es la base matemática de como las neuronas evalúan la información que procesan.</p>
</div>
</div>
<div id="producto-entre-matrices-y-vectores" class="section level3 hasAnchor" number="2.1.9">
<h3><span class="header-section-number">2.1.9</span> Producto entre matrices y vectores<a href="preliminares.html#producto-entre-matrices-y-vectores" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Para definir el producto entre matrices y vectores, tomemos <span class="math inline">\(m, n \in \mathbb{N}\)</span>, una matriz <span class="math inline">\(A \in \mathbb{R}^{n \times m}\)</span> y un vector <span class="math inline">\(x \in \mathbb{R}^m\)</span>, entonces lo definiremos de la siguiente forma:
<span class="math display">\[
    A\cdot x =
    \begin{pmatrix}
        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1m} \\
        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2m} \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nm} \\
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        x_1 \\ x_2 \\ \vdots \\ x_m
    \end{pmatrix}
    =
    \begin{pmatrix}
        a_{11}x_1 + a_{12}x_2 + \cdots + a_{1m}x_m \\
        a_{21}x_1 + a_{22}x_2 + \cdots + a_{2m}x_m \\
        \vdots \\
        a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nm}x_m \\
    \end{pmatrix}
\]</span>
Si recordamos que <span class="math inline">\(A\)</span> puede verse como un arreglo o lista de vectores <span class="math inline">\(n\)</span> vectores <span class="math inline">\(\mathbf{a}_j\)</span> con <span class="math inline">\(j \in {1, \cdots, n}\)</span>, entonces lo anterior lo podemos reescribir de la siguiente forma:
<span class="math display">\[
    A \cdot x =
    \begin{pmatrix}
        \mathbf{a}_1^\top \cdot x \\
        \mathbf{a}_2^\top \cdot x \\
        \vdots \\
        \mathbf{a}_n^\top \cdot x \\
    \end{pmatrix}
    =
    \begin{pmatrix}
        \mathbf{a}_1^\top \\
        \mathbf{a}_2^\top \\
        \vdots \\
        \mathbf{a}_n^\top \\
    \end{pmatrix} \cdot x
\]</span>
donde podemos ver como esta multiplicación matricial es una transformación que proyecta vectores de <span class="math inline">\(\mathbb{R}^m\)</span> hacia <span class="math inline">\(\mathbb{R}^n\)</span> (viendo como toma <span class="math inline">\(x\)</span>, y lo transforma en <span class="math inline">\(A\cdot x\)</span>).</p>
<div id="interpretación-geométrica-1" class="section level4 hasAnchor" number="2.1.9.1">
<h4><span class="header-section-number">2.1.9.1</span> Interpretación geométrica<a href="preliminares.html#interpretaci%C3%B3n-geom%C3%A9trica-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Las transformaciones que puede hacer <span class="math inline">\(A\)</span> sobre <span class="math inline">\(x\)</span> son:</p>
<ul>
<li>Rotarlo, escalarlo, reflejarlo o cambiar su dimensión.</li>
<li>Si <span class="math inline">\(A\)</span> es una matriz cuadrada (<span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>), entonces la transformación es hacia el mismo espacio</li>
<li>Cuando <span class="math inline">\(A\cdot x = 0\)</span>, hablaremos del núcleo de la transformación (<span class="math inline">\(\ker(A)\)</span>), o espacio nulo y este será todos los vectores <span class="math inline">\(x\)</span> tales que satisfacen esa condición.</li>
</ul>
<div id="teorema-del-rango-y-nulidad" class="section level5 hasAnchor" number="2.1.9.1.1">
<h5><span class="header-section-number">2.1.9.1.1</span> Teorema del rango y nulidad<a href="preliminares.html#teorema-del-rango-y-nulidad" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>El teorema del rango y nulidad nos habla de la relación que existe entre el núcleo y la imagen de <span class="math inline">\(A\)</span>: Sea <span class="math inline">\(A: V \rightarrow W\)</span> una transformación lineal entre espacios vectoriales de dimensión finita, donde:</p>
<ul>
<li><span class="math inline">\(\dim(V) = n\)</span>.</li>
<li><span class="math inline">\(\mbox{rango}(A)\)</span> es la imagen (también llamada rango),</li>
<li><span class="math inline">\(\ker(A)\)</span> es el núcleo o kernel (también llamado espacio nulo).
entonces:
<span class="math display">\[
      \dim(V) = dim(\mbox{rango(A)}) + \dim(\ker(A))
  \]</span>
o de forma equivalente:
<span class="math display">\[
      n = \mbox{rango}(A) + \ker(A)
  \]</span></li>
</ul>
</div>
</div>
<div id="en-aprendizaje-profundo-1" class="section level4 hasAnchor" number="2.1.9.2">
<h4><span class="header-section-number">2.1.9.2</span> En aprendizaje profundo<a href="preliminares.html#en-aprendizaje-profundo-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>En una neurona o capa lineal, el cálculo principal es precisamente este producto:
<span class="math display">\[
    \mathbf{y} = A\mathbf{x} + \mathbf{b}
\]</span>
donde los elementos son los siguientes:</p>
<ul>
<li>vector de entradas: <span class="math inline">\(\mathbf{x}\)</span>,</li>
<li>matriz de pesos: <span class="math inline">\(A\)</span>,</li>
<li>vector de sesgos: <span class="math inline">\(\mathbf{b}\)</span>,</li>
<li>salida o activación previa: <span class="math inline">\(\mathbf{y}\)</span>.</li>
</ul>
<p>Cada multiplicación entre matriz y vector permite a la red combinar las características y extraer patrones de los datos, siendo una de las operaciones más repetidas en todo el aprendizaje profundo.</p>
<p>En relación al teorema de rango y nulidad en el contexto de redes neuronales, podemos ver lo siguiente:</p>
<ul>
<li>Qué parte de la información de entrada puede conservarse (rango).</li>
<li>Qué parte se pierde o colapsa (nulidad).</li>
<li>Como el modelo puede aprender representaciones comprimidas al pasar de un espacio con alta dimensión a otro más pequeño.</li>
</ul>
</div>
<div id="resumen-3" class="section level4 hasAnchor" number="2.1.9.3">
<h4><span class="header-section-number">2.1.9.3</span> Resumen<a href="preliminares.html#resumen-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El producto de matriz con vector aplica una transformación lineal al vector, combinando sus componentes mediante productos punto con las filas de la matriz. Esta es la base matemática de como las redes neuronales procesan la información y generan nuevas representaciones.</p>
<p>En conjunto con el teorema de rango y nulidad, podemos ver que en toda transformación lineal, la suma del número de direcciones que se conservan y las que se anulan es igual a la dimensión del espacio original.</p>
</div>
</div>
<div id="multiplicación-matricial" class="section level3 hasAnchor" number="2.1.10">
<h3><span class="header-section-number">2.1.10</span> Multiplicación matricial<a href="preliminares.html#multiplicaci%C3%B3n-matricial" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Siguiendo el patrón de partir de lo particular y empezar a generalizar, y teniendo presente que una matriz es un arreglo ordenado de vectores, naturalmente podemos preguntarnos por el producto matricial, y respondernos de forma directa.</p>
<p>Sean <span class="math inline">\(n, k, m \in \mathbb{N}\)</span> y dos matrices <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{n \times k}\)</span> y <span class="math inline">\(\mathbf{B} \in \mathbb{R}^{k \times m}\)</span> con la estructura que ya conocemos:
<span class="math display">\[
\begin{split}\mathbf{A}=\begin{pmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1k} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nk} \\
\end{pmatrix},\quad
\mathbf{B}=\begin{pmatrix}
b_{11} &amp; b_{12} &amp; \cdots &amp; b_{1m} \\
b_{21} &amp; b_{22} &amp; \cdots &amp; b_{2m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
b_{k1} &amp; b_{k2} &amp; \cdots &amp; b_{km} \\
\end{pmatrix}.\end{split}
\]</span>
Sea <span class="math inline">\(\mathbf{a}_i^\top \in \mathbb{R}^k\)</span> el <span class="math inline">\(i\)</span>-ésimo vector fila de la matriz <span class="math inline">\(\mathbf{A}\)</span> y sea <span class="math inline">\(\mathbf{b}_j \in \mathbb{R}^k\)</span> el <span class="math inline">\(j\)</span>-ésimo vector columna de <span class="math inline">\(\mathbf{B}\)</span>, entonces:
<span class="math display">\[
\begin{split}\mathbf{A}=
\begin{pmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{pmatrix},
\quad \mathbf{B}=\begin{pmatrix}
\mathbf{b}_{1} &amp; \mathbf{b}_{2} &amp; \cdots &amp; \mathbf{b}_{m} \\
\end{pmatrix}.\end{split}
\]</span>
Para formar el producto matricial <span class="math inline">\(\mathbf{C} \in \mathbb{R}^{n \times m}\)</span>, vamos a calcular el elemento <span class="math inline">\(c_{ij}\)</span> como el producto punto entre la <span class="math inline">\(i\)</span>-ésima fila de <span class="math inline">\(\mathbf{A}\)</span> y la <span class="math inline">\(j\)</span>-ésima columna de <span class="math inline">\(\mathbf{B}\)</span>, es decir <span class="math inline">\(\mathbf{a}_i^\top \mathbf{b}_j\)</span>:
<span class="math display">\[
\begin{split}\mathbf{C} = \mathbf{AB} = \begin{pmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{pmatrix}
\begin{pmatrix}
\mathbf{b}_{1} &amp; \mathbf{b}_{2} &amp; \cdots &amp; \mathbf{b}_{m} \\
\end{pmatrix}
= \begin{pmatrix}
\mathbf{a}^\top_{1} \mathbf{b}_1 &amp; \mathbf{a}^\top_{1}\mathbf{b}_2&amp; \cdots &amp; \mathbf{a}^\top_{1} \mathbf{b}_m \\
\mathbf{a}^\top_{2}\mathbf{b}_1 &amp; \mathbf{a}^\top_{2} \mathbf{b}_2 &amp; \cdots &amp; \mathbf{a}^\top_{2} \mathbf{b}_m \\
\vdots &amp; \vdots &amp; \ddots &amp;\vdots\\
\mathbf{a}^\top_{n} \mathbf{b}_1 &amp; \mathbf{a}^\top_{n}\mathbf{b}_2&amp; \cdots&amp; \mathbf{a}^\top_{n} \mathbf{b}_m
\end{pmatrix}.\end{split}
\]</span>
De esta forma, podemos pensar en el producto matricial como realizar <span class="math inline">\(m\)</span> productos entre matrices y vectores o <span class="math inline">\(m \times n\)</span> productos punto y recolectandolos todos para formar una matriz de <span class="math inline">\(n \times m\)</span>. Cabe mencionar que este no es el producto Hadamard mencionado previamente.</p>
<div id="interpretación-geométrica-2" class="section level4 hasAnchor" number="2.1.10.1">
<h4><span class="header-section-number">2.1.10.1</span> Interpretación geométrica<a href="preliminares.html#interpretaci%C3%B3n-geom%C3%A9trica-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Dado que cada elemento del producto matricial, es el producto punto entre las filas de la matriz de la izquierda y las columnas de la matriz de la derecha, entonces el producto punto puede verse como una composición de transformaciones lineales:
<span class="math display">\[
    (AB)x = A(Bx).
\]</span>
Esto significa que aplicar primero <span class="math inline">\(B\)</span> y luego <span class="math inline">\(A\)</span> a un vector es equivalente a aplicar una sola transformación representada por <span class="math inline">\(AB\)</span>.</p>
</div>
<div id="propiedades-importantes" class="section level4 hasAnchor" number="2.1.10.2">
<h4><span class="header-section-number">2.1.10.2</span> Propiedades importantes<a href="preliminares.html#propiedades-importantes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Sean <span class="math inline">\(A, B, C\)</span> matrices con las dimensiones compatibles para el producto matricial, hay algunos puntos muy importantes que se tienen que mencionar:</p>
<ul>
<li>Las matrices no necesariamente conmutan: <span class="math inline">\(AB \neq BA\)</span>.</li>
<li>Las matrices son asociativas: <span class="math inline">\((AB)C = A(BC)\)</span>.</li>
<li>Las matrices son distributivas: <span class="math inline">\(A(B+C) = AB + AC\)</span>.</li>
<li>Compatibilidad con escalares: <span class="math inline">\((\alpha A)B = A(\alpha B) = \alpha(AB)\)</span> para <span class="math inline">\(\alpha\)</span> escalar.</li>
</ul>
</div>
<div id="en-el-aprendizaje-profundo-2" class="section level4 hasAnchor" number="2.1.10.3">
<h4><span class="header-section-number">2.1.10.3</span> En el aprendizaje profundo<a href="preliminares.html#en-el-aprendizaje-profundo-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El producto matricial es la operación más esencial en el cálculo de redes neuronales:</p>
<ul>
<li>Cada capa lineal o densa se expresa como:
<span class="math display">\[
      \mathbf{Y} = \mathbf{A}\mathbf{X} + \mathbf{B}
  \]</span>
donde <span class="math inline">\(\mathbf{A}\)</span> (matriz de pesos) multiplica la matriz o vector de entrada <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>En redes convolucionales, transformadores o encajes vectoriales, los productos matriciales generalizan a productos tensoriales u operaciones de atención.</li>
</ul>
</div>
<div id="producto-matricial-como-cambio-de-base-del-espacio-vectorial" class="section level4 hasAnchor" number="2.1.10.4">
<h4><span class="header-section-number">2.1.10.4</span> Producto matricial como cambio de base del espacio vectorial<a href="preliminares.html#producto-matricial-como-cambio-de-base-del-espacio-vectorial" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Una interpretación adicional al producto matricial es la de cambio de base del espacio vectorial y esto puede verse de la siguiente forma</p>
</div>
<div id="resumen-4" class="section level4 hasAnchor" number="2.1.10.5">
<h4><span class="header-section-number">2.1.10.5</span> Resumen<a href="preliminares.html#resumen-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El producto entre dos matrices es la combinación de filas y columnas entre la primera y segunda matriz para formar una nueva matriz, que representa la composición de transformaciones lineales: una idea central en álgebra lineal y por lo tanto, en aprendizaje profundo.</p>
</div>
</div>
<div id="normas" class="section level3 hasAnchor" number="2.1.11">
<h3><span class="header-section-number">2.1.11</span> Normas<a href="preliminares.html#normas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Una norma es una función que asigna a cada vector un número real no negativo que representa su longitud, magnitud o tamaño dentro de un espacio vectorial.</p>
<p>Descrito de forma matemática, sea <span class="math inline">\(V\)</span> un espacio vectorial sobre un campo <span class="math inline">\(\mathbb{R}\)</span> ó <span class="math inline">\(\mathbb{C}\)</span>. Una forma es una función tal que:
<span class="math display">\[
    \|\cdot\|:V \rightarrow \mathbb{R}_{\ge0}
\]</span>
que a cada vector <span class="math inline">\(v \in V\)</span> le asigna el número <span class="math inline">\(\|v\|\)</span>, cumpliendo las siguientes tres propiedades:
1. No negatividad y definitud:
<span class="math display">\[
        \|v\| \ge 0, \space \mbox{y} \space \|v\| = 0 \Leftrightarrow v = 0
    \]</span>
(la longitud nunca es negativa y solamente es cero con el vector cero).
2. Homogeneidad (o multiplicación por escalar):
<span class="math display">\[
    \|\alpha v\| = |\alpha| \|v\|
    \]</span>
(Escalar un vector cambia su longitud en el mismo factor absoluto).
3. Desigualdad triangular:
<span class="math display">\[
        \|u + v\| \le \|u\| + \|v\|
    \]</span>
(la longitud de la suma nunca excede la suma de las longitudes, como ocurre en un triángulo).</p>
<div id="ejemplos-de-normas" class="section level4 hasAnchor" number="2.1.11.1">
<h4><span class="header-section-number">2.1.11.1</span> Ejemplos de normas<a href="preliminares.html#ejemplos-de-normas" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Sea <span class="math inline">\(V\)</span> un espacio vectorial de dimensión <span class="math inline">\(n \in \mathbb{N}\)</span>, con entradas en los reales o complejos, y sea <span class="math inline">\(v \in V\)</span> un vector, entonces las siguientes normas están definidas de la siguiente forma</p>
<div id="norma-euclidiana-o-l2" class="section level5 hasAnchor" number="2.1.11.1.1">
<h5><span class="header-section-number">2.1.11.1.1</span> Norma Euclidiana (o <span class="math inline">\(L^2\)</span>):<a href="preliminares.html#norma-euclidiana-o-l2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[
    \|v\|_2 = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}
\]</span>
Observa que:
<span class="math display">\[
     \langle v, v \rangle= v^\top \cdot v = v_1^2 + v_2^2 + \cdots + v_n^2 = \|v\|_2^2,
\]</span>
por lo tanto:
<span class="math display">\[
    \|v\|_2 = \sqrt{\langle v, v\rangle} = \sqrt{v^\top \cdot v}.
\]</span></p>
</div>
<div id="norma-manhattan-o-l1" class="section level5 hasAnchor" number="2.1.11.1.2">
<h5><span class="header-section-number">2.1.11.1.2</span> Norma Manhattan (o <span class="math inline">\(L^1\)</span>)<a href="preliminares.html#norma-manhattan-o-l1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[
    \|v\|_1 = |v_1| + |v_2| + \cdots + |v_n|
\]</span>
Esta norma también es conocida como la norma del taxista.</p>
</div>
<div id="norma-máxima-linfty" class="section level5 hasAnchor" number="2.1.11.1.3">
<h5><span class="header-section-number">2.1.11.1.3</span> Norma máxima (<span class="math inline">\(L^{\infty}\)</span>)<a href="preliminares.html#norma-m%C3%A1xima-linfty" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[
    \|v\|_{\infty} = \max_{i}|v_i|
\]</span></p>
</div>
<div id="norma-frobenius" class="section level5 hasAnchor" number="2.1.11.1.4">
<h5><span class="header-section-number">2.1.11.1.4</span> Norma Frobenius<a href="preliminares.html#norma-frobenius" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>En el caso de las matrices, la situación es un poco más compleja. Esto porque las matrices pueden ser vistas como arreglos de vectores, o arreglos de números, al mismo tiempo que son transformaciones lineales. Por ejemplo, podríamos preguntarnos cuál es la relación de distancia para un producto de matriz y vector <span class="math inline">\(Xv\)</span> con relación a <span class="math inline">\(v\)</span>. Esta línea de pensamiento nos lleva a algo llamado <em>norma espectral</em>. Pero por ahora presentamos la norma Frobenius, la cuál es mucho más fácil de calcular y queda definida de la siguiente forma
<span class="math display">\[
    \|X\|_F = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}x_{ij}^2}.
\]</span>
Esta norma se comporta como la norma <span class="math inline">\(L^2\)</span> para vectores, pero en caso matricial.</p>
</div>
</div>
<div id="interpretación-geométrica-3" class="section level4 hasAnchor" number="2.1.11.2">
<h4><span class="header-section-number">2.1.11.2</span> Interpretación geométrica<a href="preliminares.html#interpretaci%C3%B3n-geom%C3%A9trica-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Una norma define la noción de la distancia y magnitud (o topología) dentro del espacio vectorial. Diferentes normas producen distintas geometrías del espacio (círculos, cuadrados, octágonos, etc).</p>
</div>
<div id="en-aprendizaje-profundo-2" class="section level4 hasAnchor" number="2.1.11.3">
<h4><span class="header-section-number">2.1.11.3</span> En aprendizaje profundo<a href="preliminares.html#en-aprendizaje-profundo-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Las normas son esenciales para:</p>
<ul>
<li>Medir errores (como ejemplo, usando la norma <span class="math inline">\(L^2\)</span> para el error cuadrático medio).</li>
<li>Regularización de modelos (normas como <span class="math inline">\(L^1\)</span> y <span class="math inline">\(L^2\)</span> reducen el sobreajuste penalizando los pesos grandes).</li>
<li>Normalizar datos o vectores de características, para estabilizar el entrenamiento y mejorar la interpretación de la similitud entre vectores.</li>
</ul>
</div>
<div id="resumen-5" class="section level4 hasAnchor" number="2.1.11.4">
<h4><span class="header-section-number">2.1.11.4</span> Resumen<a href="preliminares.html#resumen-5" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Una norma mide la longitud o tamaño de un vector de forma consistente con las reglas del espacio vectorial, permitiendo cuantificar distancias, magnitudes y relaciones entre vectores.</p>
</div>
</div>
</div>
<div id="cálculo-diferencial" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Cálculo Diferencial<a href="preliminares.html#c%C3%A1lculo-diferencial" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="introducción-1" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Introducción<a href="preliminares.html#introducci%C3%B3n-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Él cálculo diferencial es la rama de la matemática que surge para estudiar los cambios que ocurren en la naturaleza representado como fenómenos dinámicos, partiendo de las leyes Newtonianas estudiando el cambio de movimiento para definir así la velocidad, y posteriormente el cambio de la velocidad para entender la aceleración.</p>
<p>En el contexto del aprendizaje profundo, este tema es de vital importancia ya que las redes neuronales se calibran optimizando funciones mediante la medición y ajuste de cambios infinitesimales usando conceptos como mínimos locales y globales por medio de un concepto llamado <em>gradiente</em>, minimizando algo conocido como función de pérdida y recurriendo a técnicas modernas como la autodiferenciación para optimizar estas funciones usando la propagación hacia atrás.</p>
<p>Podemos pensar que una red neuronal aprende observando como las variaciones pequeñas en los pesos afectan el error o la función de pérdida. Esta relación, se describe con derivadas y gradientes.</p>
<p>De esta forma, el cálculo diferencial proporciona el marco de trabajo para:</p>
<ul>
<li>comprender como fluye la información y el error a través de la red.</li>
<li>Analizar la convergencia y estabilidad de los algoritmos de optimización.</li>
<li>Desarrollar modelos más eficientes y estables desde el punto de vista numérico y teórico.</li>
</ul>
</div>
<div id="motivación-1" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Motivación<a href="preliminares.html#motivaci%C3%B3n-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La motivación directa es precisamente el optimizar las funciones de pérdida que están relacionadas con los pesos de las redes neuronales para mejorar como desempeña nuestra red neuronal.</p>
</div>
<div id="derivadas-y-diferenciación" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Derivadas y diferenciación<a href="preliminares.html#derivadas-y-diferenciaci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Para proceder con las derivadas, primero definiremos rápidamente (sin entrar en tanto detalle) el concepto de función para una variable y posteriormente, como función de múltiples variables.</p>
<div id="funciones" class="section level4 hasAnchor" number="2.2.3.1">
<h4><span class="header-section-number">2.2.3.1</span> Funciones<a href="preliminares.html#funciones" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Sean <span class="math inline">\(X, Y\)</span> dos conjuntos, una función es una relación que existe entre dos conjuntos, de tal forma que para <span class="math inline">\(x \in X\)</span> y <span class="math inline">\(y \in Y\)</span>, <span class="math inline">\(f(x) = y\)</span>. Esto podemos pensarlo como una regla o mapeo para relacionar ambos conjuntos y lo denotamos como <span class="math inline">\(f: X \rightarrow Y\)</span>.</p>
<p>Para el cálculo de una variable, usualmente nuestro dominio será algún subconjunto <span class="math inline">\(X\)</span> de los números reales, y este estará relacionado con otro número real que de momento diremos pertenecerá a un subconjunto <span class="math inline">\(Y\)</span> de los reales, el cuál será el contradominio. Es decir: <span class="math inline">\(f: X \subset\mathbb{R} \rightarrow Y \subset\mathbb{R}\)</span>.</p>
<p>Para cuando trabajemos con cálculo vectorial, diremos que el dominio será precisamente un espacio vectorial <span class="math inline">\(\mathbb{R}^n\)</span>, para <span class="math inline">\(n \in \mathbb{N}\)</span>. Pero el contradominio será en los números reales. Es decir, las funciones para el cálculo vectorial serán: <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>.</p>
</div>
<div id="derivada" class="section level4 hasAnchor" number="2.2.3.2">
<h4><span class="header-section-number">2.2.3.2</span> Derivada<a href="preliminares.html#derivada" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La derivada de una función <span class="math inline">\(f\)</span> la vamos a definir de la siguiente forma:
<span class="math display">\[
    f&#39;(x) = \lim_{h \rightarrow 0}\frac{f(x + h) - f(x)}{h}.
\]</span></p>
<p>A esta expresión se le llama límite y se interpreta como hacer una pequeña perturbación que tiende a cero en el parámetro <span class="math inline">\(h\)</span> y cuando este límite existe, se dice que <span class="math inline">\(f\)</span> es diferenciable en <span class="math inline">\(x\)</span>, siendo así <span class="math inline">\(f&#39;(x)\)</span> su derivada. Esto se dice que debe ocurrir en al menos un subconjunto ya que al considerar las perturbaciones del parámetro <span class="math inline">\(h\)</span>, estamos considerando números del orden infinitesimal ya que estamos tratando con los números reales, y estos poseen una propiedad de densidad que nos permite encontrar estos elementos para realizar dicho límite.</p>
<p>Para mantener el foco en el aprendizaje profundo y no desviarnos a un curso tradicional de ciencias o ingeniería, de momento lo dejaremos hasta aquí en relación a las propiedades matemáticas de los reales, límites, y otros temas que tienen mucho material para ser desarrollados.</p>
<p>Como dato útil, podemos pensar también en la derivada de la derivada, nombrando esta como la segunda derivada y sus propiedades serán las mismas por definición. Sin embargo, esta recursividad nos permite poder obtener información a partir de una función con sus diferentes derivadas de acuerdo al contexto. Por ejemplo, si registramos la aceleración de un objeto, la derivada nos indicará la velocidad del objeto, y su segunda derivada, la posición del objeto.</p>
</div>
<div id="derivada-vectorial" class="section level4 hasAnchor" number="2.2.3.3">
<h4><span class="header-section-number">2.2.3.3</span> Derivada vectorial<a href="preliminares.html#derivada-vectorial" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>De forma simétrica a como hemos definido la derivada en una variable, podemos ver para funciones escalar de varias variables <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>, que el gradiente queda definido de la siguiente forma:
<span class="math display">\[
    \nabla f(\vec{x}) =
    \begin{pmatrix}
        \frac{\partial}{\partial x_1}f(\vec{x}) \\
        \frac{\partial}{\partial x_2}f(\vec{x}) \\
        \vdots \\
        \frac{\partial}{\partial x_n}f(\vec{x})
    \end{pmatrix}
\]</span>
donde para <span class="math inline">\(0 \le i \le n\)</span>, tenemos que:
<span class="math display">\[
    \frac{\partial}{\partial x_i}f(\vec{x})
\]</span>
es la derivada de <span class="math inline">\(f\)</span> en la <span class="math inline">\(i\)</span>-ésima dirección (o coordenada o variable) definida de la siguiente forma considerando <span class="math inline">\(y = f(x_1, x_2, \dots, x_n)\)</span>:
<span class="math display">\[
    \frac{\partial y}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.
\]</span></p>
<p>Como ejemplo, consideremos la función <span class="math inline">\(f(x, y) = 3x^2 y\)</span>. Entonces tenemos lo siguiente:
<span class="math display">\[
    \begin{align}
        \frac{\partial}{\partial x}3x^2y = 6xy \\
        \frac{\partial}{\partial y}3x^2y = 3x^2 \\
    \end{align}
\]</span>
Por lo tanto, su gradiente es:
<span class="math display">\[
    \nabla f(x, y) =
    \begin{pmatrix}
        6xy \\
        3x^2
    \end{pmatrix}
\]</span>
y este nos dirá como se comporta <span class="math inline">\(f\)</span> de forma análoga al caso de 1 variable. Otros casos como funciones vectoriales de una variable real, o funciones vectoriales de varias variables quedan fuera por el momento de este estudio.</p>
</div>
<div id="algunas-formulas-y-reglas" class="section level4 hasAnchor" number="2.2.3.4">
<h4><span class="header-section-number">2.2.3.4</span> Algunas formulas y reglas<a href="preliminares.html#algunas-formulas-y-reglas" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A continuación hay algunas formulas que nos pueden ayudar a calcular algunas derivadas:
<span class="math display">\[
\begin{split}\begin{aligned} \frac{d}{dx} C &amp; = 0 &amp;&amp; \textrm{para cualquier constante $C$} \\ \frac{d}{dx} x^n &amp; = n x^{n-1} &amp;&amp; \textrm{para } n \neq 0 \\ \frac{d}{dx} e^x &amp; = e^x \\ \frac{d}{dx} \ln x &amp; = x^{-1}. \end{aligned}\end{split}
\]</span>
Y continuamos con algunas reglas que nos permitirán trabajar con funciones compuestas:
<span class="math display">\[
\begin{split}\begin{aligned} \frac{d}{dx} [C f(x)] &amp; = C \frac{d}{dx} f(x) &amp;&amp; \textrm{La constante sale} \\ \frac{d}{dx} [f(x) + g(x)] &amp; = \frac{d}{dx} f(x) + \frac{d}{dx} g(x) &amp;&amp; \textrm{Suma de derivadas} \\ \frac{d}{dx} [f(x) g(x)] &amp; = f(x) \frac{d}{dx} g(x) + g(x) \frac{d}{dx} f(x) &amp;&amp; \textrm{Producto o multiplicación de derivadas} \\ \frac{d}{dx} \frac{f(x)}{g(x)} &amp; = \frac{g(x) \frac{d}{dx} f(x) - f(x) \frac{d}{dx} g(x)}{g^2(x)} &amp;&amp; \textrm{Cociente o división de derivadas} \end{aligned}\end{split}
\]</span>
De esta forma, podemos hacer uso de diferentes reglas para calcular derivadas. Más aún, con el teorema fundamental del cálculo, y al ver la integral como anti-derivada, podemos aprovechar algunos resultados del cálculo integral.</p>
<div id="reglas-útiles-para-derivar-funciones-multivariable" class="section level5 hasAnchor" number="2.2.3.4.1">
<h5><span class="header-section-number">2.2.3.4.1</span> Reglas útiles para derivar funciones multivariable<a href="preliminares.html#reglas-%C3%BAtiles-para-derivar-funciones-multivariable" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Para toda <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, tenemos <span class="math inline">\(\nabla_x Ax = A^\top\)</span> y <span class="math inline">\(\nabla_x x^\top A = A\)</span>. Así como para matrices cuadradas <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>, tenemos que <span class="math inline">\(\nabla_x x^\top A x = (A + A^\top)x\)</span>, y en particular: <span class="math inline">\(\nabla_x \|x\|^2 = \nabla_x x^\top x = 2x\)</span>.</p>
<p>De forma similar para cualquier matriz <span class="math inline">\(X\)</span>, tenemos <span class="math inline">\(\nabla_x \|X\|_F^2 = 2X\)</span>.</p>
</div>
</div>
</div>
<div id="regla-de-la-cadena" class="section level3 hasAnchor" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Regla de la cadena<a href="preliminares.html#regla-de-la-cadena" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>En general, las funciones suelen tener composiciones más elaboradas y sofisticadas de lo que hemos enlistado antes, haciendo que sea complicado realizar el cálculo de la derivada en estos casos. Por fortuna existe algo llamado <em>regla de la cadena</em> y que nos apoya completamente en estos escenarios.</p>
<p>De momento pensando en derivadas de una variable, partamos de la siguiente función <span class="math inline">\(y = f(g(x))\)</span>, resaltando que <span class="math inline">\(y = f(u)\)</span> así como <span class="math inline">\(u = g(x)\)</span> son ambas diferenciables. Entonces la regla de la cadena afirma que:
<span class="math display">\[
    \frac{dy}{dx} = \frac{dy}{du}\frac{du}{dx}.
\]</span>
De esta forma, viendo el escenario desde el contexto de funciones multivariable, supón que <span class="math inline">\(y = f(\vec{u})\)</span> tiene variables <span class="math inline">\(u_1, u_2, \dots, u_m\)</span> donde para cada <span class="math inline">\(u_i = g_i(\vec{x})\)</span> tiene variables <span class="math inline">\(x_1, x_2, \dots, x_n\)</span>, es decir <span class="math inline">\(\vec{u} = g(\vec{x})\)</span>, entonces la regla de la cadena afirma que:
<span class="math display">\[
\frac{\partial y}{\partial x_{i}} = \frac{\partial y}{\partial u_{1}} \frac{\partial u_{1}}{\partial x_{i}} + \frac{\partial y}{\partial u_{2}} \frac{\partial u_{2}}{\partial x_{i}} + \ldots + \frac{\partial y}{\partial u_{m}} \frac{\partial u_{m}}{\partial x_{i}} \ \textrm{ y así } \ \nabla_{\mathbf{x}} y =  \mathbf{A} \nabla_{\mathbf{u}} y,
\]</span>
donde <span class="math inline">\(A \in \mathbb{R}^{n \times m}\)</span> es una matriz que contiene la derivada del vector <span class="math inline">\(\vec{u}\)</span> con respecto al vector <span class="math inline">\(\vec{x}\)</span>. Por lo tanto, para evaluar el gradiente requiere calcular un producto entre vector y matriz.</p>
<p>Esta es una de las razones principales de por qué el álgebra lineal es un componente vital en la construcción de sistemas de aprendizaje profundo.</p>
</div>
<div id="interpretación-geométrica-de-la-derivada" class="section level3 hasAnchor" number="2.2.5">
<h3><span class="header-section-number">2.2.5</span> Interpretación geométrica de la derivada<a href="preliminares.html#interpretaci%C3%B3n-geom%C3%A9trica-de-la-derivada" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Calcular la derivada representa más que la definición que hemos dado, y esta sin duda tiene una interpretación geométrica: el ángulo de la línea tangente en el punto <span class="math inline">\(x\)</span> se le asocia con <span class="math inline">\(f&#39;(x)\)</span>. De esta forma podemos ver otras interpretaciones geométricas ya que a medida que las tangentes son positivas, podemos ver que la función se incrementa en valor, así como con tangentes negativas, significa que la función decrementa en valor. De esta forma, cuando encontramos que las tangentes son cero, significa que hay una meseta o valor constante en la función en ese punto.</p>
<p>También podemos ver que para funciones escalares de varias variables, que su gradiente tiene una estructura similar, diciendonos estos comportamientos en sus variables o direcciones, pudiendo así conceptualizar como el pensar en <em>gradiente descendiente</em> nos lleva a buscar las direcciones de decrecimiento de una función para así, buscar sus valores mínimos, pensando que al llegar a estas mesetas hundidas o valles, encontraremos los mínimos que buscamos.</p>
</div>
<div id="teorema-fundamental-del-cálculo" class="section level3 hasAnchor" number="2.2.6">
<h3><span class="header-section-number">2.2.6</span> Teorema fundamental del cálculo<a href="preliminares.html#teorema-fundamental-del-c%C3%A1lculo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Un teorema muy importante en el cálculo es el teorema fundamental de este, estableciendo la conexión entre derivadas e integrales. Sirviendo así como puente para conectar las derivadas con las integrales, viendo a una integral como <em>antiderivada</em>.</p>
<p>Sea <span class="math inline">\(f: [a, b]: \rightarrow \mathbb{R}\)</span> una función continua. Definamos una nueva función <span class="math inline">\(F(x)\)</span> como:
<span class="math display">\[
    F(x) = \int_{a}^{x}f(t)dt,
\]</span>
entonces
<span class="math display">\[
    F&#39;(x) = f(x).
\]</span>
Es decir, derivar la integral de una función continua nos regresa la función original. Las interpretaciones de este resultado son:</p>
<ul>
<li><span class="math inline">\(F(x)\)</span> acumula el área bajo la curva de <span class="math inline">\(f\)</span> desde <span class="math inline">\(a\)</span> hasta <span class="math inline">\(x\)</span>.</li>
<li>La derivada <span class="math inline">\(F&#39;(x)\)</span> mide cuán rápido crece esa área al mover el límite superior <span class="math inline">\(x\)</span>.</li>
<li>De esta forma, la derivada y la integral son operaciones inversas.</li>
</ul>
</div>
<div id="autodiferenciación" class="section level3 hasAnchor" number="2.2.7">
<h3><span class="header-section-number">2.2.7</span> Autodiferenciación<a href="preliminares.html#autodiferenciaci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Esta idea es un concepto poderoso que conecta el cálculo diferencial con la computación moderna y el aprendizaje profundo. Esto porque computacionalmente, no es eficiente realizar cálculos simbólicos, así como la forma de calcular derivadas numéricas solía ser con una forma más antigua en un contexto de diferencias finitas.</p>
<div id="historia" class="section level4 hasAnchor" number="2.2.7.1">
<h4><span class="header-section-number">2.2.7.1</span> Historia<a href="preliminares.html#historia" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Mientras tanto, el concepto de autodiferenciación es un tema más reciente en la historia de las matemáticas, encontrando las primeras referencias en el trabajo de Wengert en 1964 <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> . Las ideas núcleo para la propagación hacia atrás se pueden trazar a una tesis de doctorado (PhD) de Speelpenning en 1980 <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> y fueron trabajadas y desarrolladas a finales de los 80’s por Griewank <a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. Aunque la propagación hacia atrás es el método por eleccción para calcular gradientes, no es la única opción que existe. Un ejemplo lo encontramos en el lenguaje de programación Julia, que emplea una propagación hacia adelante <a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>.</p>
</div>
<div id="qué-es" class="section level4 hasAnchor" number="2.2.7.2">
<h4><span class="header-section-number">2.2.7.2</span> ¿Qué es?<a href="preliminares.html#qu%C3%A9-es" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Es un conjunto de técnicas que permiten calcular derivadas de funciones expresadas como programas de manera exacta y eficiente. Utilizando la regla de la cadena a nivel de operaciones elementales. No tratándose de:</p>
<ul>
<li>Diferenciación simbólica, donde se manipulan las expresiones algebraicamente (como en el caso de <em>SymPy</em>), resultando altamente ineficiente.</li>
<li>Ni de diferenciación numérica, que aproxima derivadas con diferencias finitas y es propensa a errores de redondeo.</li>
</ul>
<p>De esta forma la autodiferenciación combina la precisión del cálculo analítico con la eficiencia computacional del cálculo numérico.</p>
</div>
<div id="fundamento-matemático" class="section level4 hasAnchor" number="2.2.7.3">
<h4><span class="header-section-number">2.2.7.3</span> Fundamento matemático<a href="preliminares.html#fundamento-matem%C3%A1tico" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Toda función implementada computacionalmente se puede descomponer como una composición de operaciones elementales (suma, multiplicación, exponencial, etc). Consideremos el siguiente ejemplo:
<span class="math display">\[
    y = f(x_1, x_2) = e^{x_1x_2} + \sin(x_1).
\]</span>
El cálculo de su derivada implica aplicar sistemáticamente la regla de la cadena:
<span class="math display">\[
    \frac{dy}{dx_i} = \frac{\partial f}{\partial x_i}.
\]</span>
La autodiferenciación recorre ese mismo proceso de forma automática, propagando derivadas locales a lo largo de la gráfica o grafo computacional que representa dicha función.</p>
</div>
<div id="dos-modos-principales" class="section level4 hasAnchor" number="2.2.7.4">
<h4><span class="header-section-number">2.2.7.4</span> Dos modos principales<a href="preliminares.html#dos-modos-principales" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="modo-directo" class="section level5 hasAnchor" number="2.2.7.4.1">
<h5><span class="header-section-number">2.2.7.4.1</span> Modo directo<a href="preliminares.html#modo-directo" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Calcula las derivadas de salida respecto a una entrada. Ideal cuando tienes pocas entradas y muchas salidas.</p>
<p>Consideremos el siguiente ejemplo:
<span class="math display">\[
    f(x, y) =
    \begin{pmatrix}
        x^2 + \sin(y) \\
        e^{xy}
    \end{pmatrix}
\]</span></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb2-1"><a href="preliminares.html#cb2-1" tabindex="-1"></a><span class="im">using</span> <span class="bu">ForwardDiff</span></span>
<span id="cb2-2"><a href="preliminares.html#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="preliminares.html#cb2-3" tabindex="-1"></a><span class="co">### f: ℝ² → ℝ²</span></span>
<span id="cb2-4"><a href="preliminares.html#cb2-4" tabindex="-1"></a><span class="fu">f</span>(x<span class="op">::</span><span class="dt">AbstractVector</span>) <span class="op">=</span> [</span>
<span id="cb2-5"><a href="preliminares.html#cb2-5" tabindex="-1"></a>    x[<span class="fl">1</span>]<span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fu">sin</span>(x[<span class="fl">2</span>]);</span>
<span id="cb2-6"><a href="preliminares.html#cb2-6" tabindex="-1"></a>    <span class="fu">exp</span>(x[<span class="fl">1</span>] <span class="op">*</span> x[<span class="fl">2</span>])</span>
<span id="cb2-7"><a href="preliminares.html#cb2-7" tabindex="-1"></a>]</span>
<span id="cb2-8"><a href="preliminares.html#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="preliminares.html#cb2-9" tabindex="-1"></a><span class="co">### g: ℝ² → ℝ (para gradiente y Hessiano)</span></span>
<span id="cb2-10"><a href="preliminares.html#cb2-10" tabindex="-1"></a><span class="fu">g</span>(x<span class="op">::</span><span class="dt">AbstractVector</span>) <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (x[<span class="fl">1</span>]<span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">3</span>x[<span class="fl">2</span>]<span class="op">^</span><span class="fl">2</span>) <span class="op">+</span> <span class="fu">sin</span>(x[<span class="fl">1</span>]<span class="op">*</span>x[<span class="fl">2</span>])</span>
<span id="cb2-11"><a href="preliminares.html#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="preliminares.html#cb2-12" tabindex="-1"></a>x <span class="op">=</span> [<span class="fl">1.0</span>, <span class="fl">0.5</span>]</span>
<span id="cb2-13"><a href="preliminares.html#cb2-13" tabindex="-1"></a></span>
<span id="cb2-14"><a href="preliminares.html#cb2-14" tabindex="-1"></a><span class="co">### Jacobiano de f en x</span></span>
<span id="cb2-15"><a href="preliminares.html#cb2-15" tabindex="-1"></a>J <span class="op">=</span> ForwardDiff.<span class="fu">jacobian</span>(f, x)</span>
<span id="cb2-16"><a href="preliminares.html#cb2-16" tabindex="-1"></a></span>
<span id="cb2-17"><a href="preliminares.html#cb2-17" tabindex="-1"></a><span class="co">### Gradiente de g en x</span></span>
<span id="cb2-18"><a href="preliminares.html#cb2-18" tabindex="-1"></a>∇g <span class="op">=</span> ForwardDiff.<span class="fu">gradient</span>(g, x)</span>
<span id="cb2-19"><a href="preliminares.html#cb2-19" tabindex="-1"></a></span>
<span id="cb2-20"><a href="preliminares.html#cb2-20" tabindex="-1"></a><span class="co">### Hessiano de g en x</span></span>
<span id="cb2-21"><a href="preliminares.html#cb2-21" tabindex="-1"></a>H <span class="op">=</span> ForwardDiff.<span class="fu">hessian</span>(g, x)</span>
<span id="cb2-22"><a href="preliminares.html#cb2-22" tabindex="-1"></a></span>
<span id="cb2-23"><a href="preliminares.html#cb2-23" tabindex="-1"></a><span class="fu">println</span>(<span class="st">&quot;J =</span><span class="sc">\n</span><span class="st">&quot;</span>, J)</span>
<span id="cb2-24"><a href="preliminares.html#cb2-24" tabindex="-1"></a><span class="fu">println</span>(<span class="st">&quot;∇g = &quot;</span>, ∇g)</span>
<span id="cb2-25"><a href="preliminares.html#cb2-25" tabindex="-1"></a><span class="fu">println</span>(<span class="st">&quot;H =</span><span class="sc">\n</span><span class="st">&quot;</span>, H)</span>
<span id="cb2-26"><a href="preliminares.html#cb2-26" tabindex="-1"></a><span class="co">###=</span></span>
<span id="cb2-27"><a href="preliminares.html#cb2-27" tabindex="-1"></a>Resultados<span class="op">:</span></span>
<span id="cb2-28"><a href="preliminares.html#cb2-28" tabindex="-1"></a>J <span class="op">=</span></span>
<span id="cb2-29"><a href="preliminares.html#cb2-29" tabindex="-1"></a>[<span class="fl">2.0</span> <span class="fl">0.8775825618903728</span>;</span>
<span id="cb2-30"><a href="preliminares.html#cb2-30" tabindex="-1"></a> <span class="fl">0.8243606353500641</span> <span class="fl">1.6487212707001282</span>]</span>
<span id="cb2-31"><a href="preliminares.html#cb2-31" tabindex="-1"></a>∇g <span class="op">=</span> [<span class="fl">1.4387912809451864</span>, <span class="fl">2.3775825618903728</span>]</span>
<span id="cb2-32"><a href="preliminares.html#cb2-32" tabindex="-1"></a>H <span class="op">=</span></span>
<span id="cb2-33"><a href="preliminares.html#cb2-33" tabindex="-1"></a>[<span class="fl">0.8801436153489492</span> <span class="fl">0.6378697925882713</span>;</span>
<span id="cb2-34"><a href="preliminares.html#cb2-34" tabindex="-1"></a> <span class="fl">0.6378697925882713</span> <span class="fl">2.520574461395797</span>]</span>
<span id="cb2-35"><a href="preliminares.html#cb2-35" tabindex="-1"></a><span class="op">=</span><span class="co">#</span></span></code></pre></div>
<p>Los resultados son los siguientes:</p>
<div id="el-jacobiano-j_fx-y" class="section level6 hasAnchor" number="2.2.7.4.1.1">
<h6><span class="header-section-number">2.2.7.4.1.1</span> El Jacobiano <span class="math inline">\(J_f(x, y)\)</span>:<a href="preliminares.html#el-jacobiano-j_fx-y" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p><span class="math display">\[
    \left.J_f(x_0, y_0)\right|_{x_0 = 1,\ y_0 = 0.5} =
    \begin{pmatrix}
        2.0 &amp; 0.8775825618903728 \\
         0.8243606353500641 &amp; 1.6487212707001282
    \end{pmatrix}
\]</span>
Lo que nos dice la sensibilidad al variar cerca del punto <span class="math inline">\((x_0, y_0)\)</span>.</p>
</div>
<div id="el-gradiente-nabla-gx-y" class="section level6 hasAnchor" number="2.2.7.4.1.2">
<h6><span class="header-section-number">2.2.7.4.1.2</span> El gradiente <span class="math inline">\(\nabla g(x, y)\)</span>:<a href="preliminares.html#el-gradiente-nabla-gx-y" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p><span class="math display">\[
    \left.\nabla g(x_0, y_0)\right|_{(x_0, y_0) = (1, 0.5)} =
    \begin{pmatrix}
        1.4387912809451864 \\
        2.3775825618903728
    \end{pmatrix}
\]</span></p>
<p>Esto nos dice la dirección de máximo incremento local de <span class="math inline">\(g\)</span>; para un paso pequeño <span class="math inline">\(\Delta x\)</span>, tenemos que <span class="math inline">\(g(x + \Delta x) \approx g(x) + \nabla g(x) \cdot \Delta x\)</span>.</p>
</div>
<div id="el-hessiano-h_gx-y" class="section level6 hasAnchor" number="2.2.7.4.1.3">
<h6><span class="header-section-number">2.2.7.4.1.3</span> El hessiano <span class="math inline">\(H_g(x, y)\)</span><a href="preliminares.html#el-hessiano-h_gx-y" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p><span class="math display">\[
    \left.H_g(x_0, y_0)\right|_{(x_0, y_0) = (1, 0.5)} =
    \begin{pmatrix}
        0.8801436153489492 &amp; 0.6378697925882713 \\
        0.6378697925882713 &amp; 2.520574461395797
    \end{pmatrix}
\]</span>
Lo que nos dice esto es la curvatura local de <span class="math inline">\(g\)</span>. Si <span class="math inline">\(H\)</span> es definida positiva en <span class="math inline">\((x, y)\)</span>, el punto es un mínimo local.</p>
</div>
<div id="uso-de-estos-resultados" class="section level6 hasAnchor" number="2.2.7.4.1.4">
<h6><span class="header-section-number">2.2.7.4.1.4</span> Uso de estos resultados<a href="preliminares.html#uso-de-estos-resultados" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>Estos cálculos nos sirven para los siguientes conceptos en la práctica:</p>
<ul>
<li>Linealizar <span class="math inline">\(f\)</span> alrededor de <span class="math inline">\(x\)</span>: <span class="math inline">\(f(x + \Delta x) \approx f(x) + J \Delta x\)</span>. La derivada es una buena aproximación que tiene un comportamiento lineal de forma local.</li>
<li>Buscar descenso de <span class="math inline">\(g\)</span>: mover <span class="math inline">\(x\)</span> en la dirección <span class="math inline">\(-\nabla g\)</span>,</li>
<li>Aprovechar la curvatura: <span class="math inline">\(\Delta x = -H^{-1} \nabla g\)</span> si <span class="math inline">\(H\)</span> es bien condicionada y positiva definida (<span class="math inline">\(H \succ 0\)</span>).</li>
</ul>
<p><strong>Qué pasa:</strong> <code>ForwardDiff</code> propaga <strong>números duales</strong> (pares valor-derivada) por cada operación elemental y arma automáticamente el Jacobiano/gradiente/Hessiano.</p>
</div>
</div>
<div id="modo-inverso" class="section level5 hasAnchor" number="2.2.7.4.2">
<h5><span class="header-section-number">2.2.7.4.2</span> Modo inverso<a href="preliminares.html#modo-inverso" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Calcula derivadas de una salida respecto a todas las entradas. Este modo es ideal cuando tienes una salida edscalar (como es el caso de una función de pérdida en aprendizaje profundo) y muchas entradas o parámetros.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb3-1"><a href="preliminares.html#cb3-1" tabindex="-1"></a><span class="im">using</span> <span class="bu">Zygote</span>, <span class="bu">LinearAlgebra</span>, <span class="bu">Statistics</span>, <span class="bu">Random</span></span>
<span id="cb3-2"><a href="preliminares.html#cb3-2" tabindex="-1"></a><span class="bu">Random</span>.<span class="fu">seed!</span>(<span class="fl">0</span>)</span>
<span id="cb3-3"><a href="preliminares.html#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="preliminares.html#cb3-4" tabindex="-1"></a><span class="co">### Definimos un MLP Perceptrón Multicapa o Multilayer Perceptron</span></span>
<span id="cb3-5"><a href="preliminares.html#cb3-5" tabindex="-1"></a><span class="co">### sencillo &quot;a mano&quot;</span></span>
<span id="cb3-6"><a href="preliminares.html#cb3-6" tabindex="-1"></a><span class="kw">struct</span> MLP</span>
<span id="cb3-7"><a href="preliminares.html#cb3-7" tabindex="-1"></a>    W1<span class="op">::</span><span class="dt">Matrix{Float64}</span></span>
<span id="cb3-8"><a href="preliminares.html#cb3-8" tabindex="-1"></a>    b1<span class="op">::</span><span class="dt">Vector{Float64}</span></span>
<span id="cb3-9"><a href="preliminares.html#cb3-9" tabindex="-1"></a>    W2<span class="op">::</span><span class="dt">Matrix{Float64}</span></span>
<span id="cb3-10"><a href="preliminares.html#cb3-10" tabindex="-1"></a>    b2<span class="op">::</span><span class="dt">Vector{Float64}</span></span>
<span id="cb3-11"><a href="preliminares.html#cb3-11" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb3-12"><a href="preliminares.html#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a href="preliminares.html#cb3-13" tabindex="-1"></a><span class="co">### Inicialización</span></span>
<span id="cb3-14"><a href="preliminares.html#cb3-14" tabindex="-1"></a><span class="kw">function</span> <span class="fu">MLP</span>(inDim<span class="op">::</span><span class="dt">Int</span>, hid<span class="op">::</span><span class="dt">Int</span>, outDim<span class="op">::</span><span class="dt">Int</span>; σ <span class="op">=</span> <span class="fl">0.1</span>)</span>
<span id="cb3-15"><a href="preliminares.html#cb3-15" tabindex="-1"></a>    W1 <span class="op">=</span> σ <span class="op">.*</span> <span class="fu">randn</span>(hid, inDim)</span>
<span id="cb3-16"><a href="preliminares.html#cb3-16" tabindex="-1"></a>    b1 <span class="op">=</span> <span class="fu">zeros</span>(hid)</span>
<span id="cb3-17"><a href="preliminares.html#cb3-17" tabindex="-1"></a>    W2 <span class="op">=</span> σ <span class="op">.*</span> <span class="fu">randn</span>(outDim, hid)</span>
<span id="cb3-18"><a href="preliminares.html#cb3-18" tabindex="-1"></a>    b2 <span class="op">=</span> <span class="fu">zeros</span>(outDim)</span>
<span id="cb3-19"><a href="preliminares.html#cb3-19" tabindex="-1"></a>    <span class="fu">MLP</span>(W1, b1, W2, b2)</span>
<span id="cb3-20"><a href="preliminares.html#cb3-20" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb3-21"><a href="preliminares.html#cb3-21" tabindex="-1"></a></span>
<span id="cb3-22"><a href="preliminares.html#cb3-22" tabindex="-1"></a><span class="co">### Forward pass</span></span>
<span id="cb3-23"><a href="preliminares.html#cb3-23" tabindex="-1"></a><span class="fu">predict</span>(m<span class="op">::</span><span class="dt">MLP</span>, X<span class="op">::</span><span class="dt">Matrix</span>) <span class="op">=</span> m.W2 <span class="op">*</span> <span class="fu">tanh</span>.(m.W1 <span class="op">*</span> X <span class="op">.+</span> m.b1) <span class="op">.+</span> m.b2</span>
<span id="cb3-24"><a href="preliminares.html#cb3-24" tabindex="-1"></a></span>
<span id="cb3-25"><a href="preliminares.html#cb3-25" tabindex="-1"></a><span class="co">### Pérdida MSE escalar</span></span>
<span id="cb3-26"><a href="preliminares.html#cb3-26" tabindex="-1"></a><span class="fu">mse</span>(m<span class="op">::</span><span class="dt">MLP</span>, X<span class="op">::</span><span class="dt">Matrix</span>, Y<span class="op">::</span><span class="dt">Matrix</span>) <span class="op">=</span> <span class="fu">mean</span>((<span class="fu">predict</span>(m, X) <span class="op">.-</span> Y)<span class="op">.^</span><span class="fl">2</span>)</span>
<span id="cb3-27"><a href="preliminares.html#cb3-27" tabindex="-1"></a></span>
<span id="cb3-28"><a href="preliminares.html#cb3-28" tabindex="-1"></a><span class="co">### Datos de juguete: 2 entradas, 1 salida, N muestras</span></span>
<span id="cb3-29"><a href="preliminares.html#cb3-29" tabindex="-1"></a>N <span class="op">=</span> <span class="fl">5</span></span>
<span id="cb3-30"><a href="preliminares.html#cb3-30" tabindex="-1"></a>X <span class="op">=</span> <span class="fu">randn</span>(<span class="fl">2</span>, N)</span>
<span id="cb3-31"><a href="preliminares.html#cb3-31" tabindex="-1"></a>trueW <span class="op">=</span> [<span class="fl">2.0</span> <span class="op">-</span><span class="fl">1.0</span>]             <span class="co"># fila 1x2</span></span>
<span id="cb3-32"><a href="preliminares.html#cb3-32" tabindex="-1"></a>Y <span class="op">=</span> trueW <span class="op">*</span> X <span class="op">.+</span> <span class="fl">0.1</span> <span class="op">.*</span> <span class="fu">randn</span>(<span class="fl">1</span>, N)  <span class="co"># 1xN</span></span>
<span id="cb3-33"><a href="preliminares.html#cb3-33" tabindex="-1"></a></span>
<span id="cb3-34"><a href="preliminares.html#cb3-34" tabindex="-1"></a><span class="co">### Modelo: 2 neuronas de entrada, 8 en la capa oculta y 1 de salida.</span></span>
<span id="cb3-35"><a href="preliminares.html#cb3-35" tabindex="-1"></a>m <span class="op">=</span> <span class="fu">MLP</span>(<span class="fl">2</span>, <span class="fl">8</span>, <span class="fl">1</span>)</span>
<span id="cb3-36"><a href="preliminares.html#cb3-36" tabindex="-1"></a></span>
<span id="cb3-37"><a href="preliminares.html#cb3-37" tabindex="-1"></a><span class="co">### Gradiente de la pérdida respecto a todos los parámetros</span></span>
<span id="cb3-38"><a href="preliminares.html#cb3-38" tabindex="-1"></a><span class="fu">loss</span>(m) <span class="op">=</span> <span class="fu">mse</span>(m, X, Y)</span>
<span id="cb3-39"><a href="preliminares.html#cb3-39" tabindex="-1"></a>grads <span class="op">=</span> Zygote.<span class="fu">gradient</span>(loss, m)</span>
<span id="cb3-40"><a href="preliminares.html#cb3-40" tabindex="-1"></a></span>
<span id="cb3-41"><a href="preliminares.html#cb3-41" tabindex="-1"></a><span class="co">### grads es una tupla con ∂loss/∂(W1,b1,W2,b2) empaquetada como MLP</span></span>
<span id="cb3-42"><a href="preliminares.html#cb3-42" tabindex="-1"></a>∂W1 <span class="op">=</span> grads[<span class="fl">1</span>].W1</span>
<span id="cb3-43"><a href="preliminares.html#cb3-43" tabindex="-1"></a>∂b1 <span class="op">=</span> grads[<span class="fl">1</span>].b1</span>
<span id="cb3-44"><a href="preliminares.html#cb3-44" tabindex="-1"></a>∂W2 <span class="op">=</span> grads[<span class="fl">1</span>].W2</span>
<span id="cb3-45"><a href="preliminares.html#cb3-45" tabindex="-1"></a>∂b2 <span class="op">=</span> grads[<span class="fl">1</span>].b2</span>
<span id="cb3-46"><a href="preliminares.html#cb3-46" tabindex="-1"></a></span>
<span id="cb3-47"><a href="preliminares.html#cb3-47" tabindex="-1"></a><span class="fu">println</span>(<span class="st">&quot;‖∂W1‖ = &quot;</span>, <span class="fu">norm</span>(∂W1))</span>
<span id="cb3-48"><a href="preliminares.html#cb3-48" tabindex="-1"></a><span class="fu">println</span>(<span class="st">&quot;‖∂b1‖ = &quot;</span>, <span class="fu">norm</span>(∂b1))</span>
<span id="cb3-49"><a href="preliminares.html#cb3-49" tabindex="-1"></a><span class="fu">println</span>(<span class="st">&quot;‖∂W2‖ = &quot;</span>, <span class="fu">norm</span>(∂W2))</span>
<span id="cb3-50"><a href="preliminares.html#cb3-50" tabindex="-1"></a><span class="fu">println</span>(<span class="st">&quot;‖∂b2‖ = &quot;</span>, <span class="fu">norm</span>(∂b2))</span></code></pre></div>
<p>Las normas calculadas son las siguientes:</p>
<table>
<thead>
<tr class="header">
<th>Parámetro</th>
<th>Norma</th>
<th>Capa</th>
<th>Tipo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>‖∂W1‖</td>
<td>0.34</td>
<td>Oculta</td>
<td>Pesos</td>
</tr>
<tr class="even">
<td>‖∂b1‖</td>
<td>0.53</td>
<td>Oculta</td>
<td>Sesgos</td>
</tr>
<tr class="odd">
<td>‖∂W2‖</td>
<td>0.25</td>
<td>Salida</td>
<td>Pesos</td>
</tr>
<tr class="even">
<td>‖∂b2‖</td>
<td>1.77</td>
<td>Salida</td>
<td>Sesgos</td>
</tr>
</tbody>
</table>
<p>Con lo que podemos ver que no existe explosión ni desvanecimiento de gradiente. Y parece indicar que el modelo comienza <em>cerca</em> de un mínimo local o en una zona de pérdida menos pronunciada.</p>

<!-- ::: watermark -->
<!-- <img src="img/aif-logo.png" width="400"/> -->
<!-- ::: -->
</div>
</div>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Wengert, R. E. (1964). A simple automatic derivative evaluation program. <em>Communications of the ACM</em>, <em>7</em>(8), 463–464.<a href="preliminares.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Speelpenning, B. (1980). <em>Compiling fast partial derivatives of functions given by algorithms</em> (Doctoral dissertation). University of Illinois at Urbana-Champaign.<a href="preliminares.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Griewank, A. (1989). On automatic differentiation. <em>Mathematical Programming: Recent Developments and Applications</em> (pp. 83–107). Kluwer.<a href="preliminares.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Revels, J., Lubin, M., &amp; Papamarkou, T. (2016). Forward-mode automatic differentiation in Julia. <em>ArXiv:1607.07892</em>.<a href="preliminares.html#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script>

$('.pipehover_incremental tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).prevAll().addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});


$('.pipehover_select_one_row tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});

</script>
            </section>

          </div>
        </div>
      </div>
<a href="introducción-a-deep-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="redes-neuronales-lineales-para-regresión.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["deep_learning_course_python.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
