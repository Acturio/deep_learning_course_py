<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 15 Embeddings y Graph Neural Network | Deep Learning</title>
  <meta name="description" content="Capítulo 15 Embeddings y Graph Neural Network | Deep Learning" />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 15 Embeddings y Graph Neural Network | Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Capítulo 15 Embeddings y Graph Neural Network | Deep Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 15 Embeddings y Graph Neural Network | Deep Learning" />
  
  <meta name="twitter:description" content="Capítulo 15 Embeddings y Graph Neural Network | Deep Learning" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="graph-neural-network.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="img/aif-logo.png" width="280"></a></li|
|:-:|  
<center>Curso de Redes Neuronales Artificiales</center>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>BIENVENIDA</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#objetivo"><i class="fa fa-check"></i>Objetivo</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#alcances-del-programa"><i class="fa fa-check"></i>Alcances del Programa</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#temario"><i class="fa fa-check"></i>Temario:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#c%C3%B3digo"><i class="fa fa-check"></i>Código</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#duraci%C3%B3n-y-evaluaci%C3%B3n-del-programa"><i class="fa fa-check"></i>Duración y evaluación del programa</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recursos-y-din%C3%A1mica"><i class="fa fa-check"></i>Recursos y dinámica</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agenda"><i class="fa fa-check"></i>Agenda</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bibliograf%C3%ADa"><i class="fa fa-check"></i>Bibliografía</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html"><i class="fa fa-check"></i><b>1</b> Introducción a Deep Learning</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#introducci%C3%B3n-al-aprendizaje-autom%C3%A1tico"><i class="fa fa-check"></i><b>1.1</b> Introducción al aprendizaje automático</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#motivaci%C3%B3n-los-paradigmas-del-conocimiento"><i class="fa fa-check"></i><b>1.1.1</b> Motivación: Los paradigmas del conocimiento</a></li>
<li class="chapter" data-level="1.1.2" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#machine-learning-supervisado"><i class="fa fa-check"></i><b>1.1.2</b> Machine learning supervisado</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#por-qu%C3%A9-deep-learning"><i class="fa fa-check"></i><b>1.2</b> ¿Por qué Deep Learning?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#dimensi%C3%B3n-vc"><i class="fa fa-check"></i><b>1.2.1</b> Dimensión VC</a></li>
<li class="chapter" data-level="1.2.2" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#el-truco-del-kernel-svm"><i class="fa fa-check"></i><b>1.2.2</b> El truco del Kernel (SVM)</a></li>
<li class="chapter" data-level="1.2.3" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#ingenier%C3%ADa-de-caracteristicas"><i class="fa fa-check"></i><b>1.2.3</b> Ingeniería de caracteristicas</a></li>
<li class="chapter" data-level="1.2.4" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#beneficios-del-deep-learning"><i class="fa fa-check"></i><b>1.2.4</b> Beneficios del deep learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="preliminares.html"><a href="preliminares.html"><i class="fa fa-check"></i><b>2</b> Preliminares</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminares.html"><a href="preliminares.html#algebra-lineal"><i class="fa fa-check"></i><b>2.1</b> Algebra Lineal</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="preliminares.html"><a href="preliminares.html#introducci%C3%B3n"><i class="fa fa-check"></i><b>2.1.1</b> Introducción</a></li>
<li class="chapter" data-level="2.1.2" data-path="preliminares.html"><a href="preliminares.html#motivaci%C3%B3n"><i class="fa fa-check"></i><b>2.1.2</b> Motivación</a></li>
<li class="chapter" data-level="2.1.3" data-path="preliminares.html"><a href="preliminares.html#escalares"><i class="fa fa-check"></i><b>2.1.3</b> Escalares</a></li>
<li class="chapter" data-level="2.1.4" data-path="preliminares.html"><a href="preliminares.html#vectores-1"><i class="fa fa-check"></i><b>2.1.4</b> Vectores</a></li>
<li class="chapter" data-level="2.1.5" data-path="preliminares.html"><a href="preliminares.html#matrices-1"><i class="fa fa-check"></i><b>2.1.5</b> Matrices</a></li>
<li class="chapter" data-level="2.1.6" data-path="preliminares.html"><a href="preliminares.html#tensores"><i class="fa fa-check"></i><b>2.1.6</b> Tensores</a></li>
<li class="chapter" data-level="2.1.7" data-path="preliminares.html"><a href="preliminares.html#reducciones-sobre-tensores"><i class="fa fa-check"></i><b>2.1.7</b> Reducciones sobre tensores</a></li>
<li class="chapter" data-level="2.1.8" data-path="preliminares.html"><a href="preliminares.html#producto-punto"><i class="fa fa-check"></i><b>2.1.8</b> Producto punto</a></li>
<li class="chapter" data-level="2.1.9" data-path="preliminares.html"><a href="preliminares.html#producto-entre-matrices-y-vectores"><i class="fa fa-check"></i><b>2.1.9</b> Producto entre matrices y vectores</a></li>
<li class="chapter" data-level="2.1.10" data-path="preliminares.html"><a href="preliminares.html#multiplicaci%C3%B3n-matricial"><i class="fa fa-check"></i><b>2.1.10</b> Multiplicación matricial</a></li>
<li class="chapter" data-level="2.1.11" data-path="preliminares.html"><a href="preliminares.html#normas"><i class="fa fa-check"></i><b>2.1.11</b> Normas</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="preliminares.html"><a href="preliminares.html#c%C3%A1lculo-diferencial"><i class="fa fa-check"></i><b>2.2</b> Cálculo Diferencial</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="preliminares.html"><a href="preliminares.html#introducci%C3%B3n-1"><i class="fa fa-check"></i><b>2.2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2.2" data-path="preliminares.html"><a href="preliminares.html#motivaci%C3%B3n-1"><i class="fa fa-check"></i><b>2.2.2</b> Motivación</a></li>
<li class="chapter" data-level="2.2.3" data-path="preliminares.html"><a href="preliminares.html#derivadas-y-diferenciaci%C3%B3n"><i class="fa fa-check"></i><b>2.2.3</b> Derivadas y diferenciación</a></li>
<li class="chapter" data-level="2.2.4" data-path="preliminares.html"><a href="preliminares.html#regla-de-la-cadena"><i class="fa fa-check"></i><b>2.2.4</b> Regla de la cadena</a></li>
<li class="chapter" data-level="2.2.5" data-path="preliminares.html"><a href="preliminares.html#interpretaci%C3%B3n-geom%C3%A9trica-de-la-derivada"><i class="fa fa-check"></i><b>2.2.5</b> Interpretación geométrica de la derivada</a></li>
<li class="chapter" data-level="2.2.6" data-path="preliminares.html"><a href="preliminares.html#teorema-fundamental-del-c%C3%A1lculo"><i class="fa fa-check"></i><b>2.2.6</b> Teorema fundamental del cálculo</a></li>
<li class="chapter" data-level="2.2.7" data-path="preliminares.html"><a href="preliminares.html#autodiferenciaci%C3%B3n"><i class="fa fa-check"></i><b>2.2.7</b> Autodiferenciación</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html"><i class="fa fa-check"></i><b>3</b> Redes neuronales Lineales para Regresión</a>
<ul>
<li class="chapter" data-level="3.1" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#introducci%C3%B3n-2"><i class="fa fa-check"></i><b>3.1</b> Introducción</a></li>
<li class="chapter" data-level="3.2" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#regresiones-lineales-simples"><i class="fa fa-check"></i><b>3.2</b> Regresiones lineales simples</a></li>
<li class="chapter" data-level="3.3" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#funci%C3%B3n-de-p%C3%A9rdida-y-funci%C3%B3n-de-costo"><i class="fa fa-check"></i><b>3.3</b> Función de pérdida y función de costo</a></li>
<li class="chapter" data-level="3.4" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#supuestos-en-la-regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>3.4</b> Supuestos en la regresión lineal</a></li>
<li class="chapter" data-level="3.5" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#regresi%C3%B3n-lineal-m%C3%BAltiple"><i class="fa fa-check"></i><b>3.5</b> Regresión Lineal Múltiple</a></li>
<li class="chapter" data-level="3.6" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#estimaci%C3%B3n-de-los-par%C3%A1metros"><i class="fa fa-check"></i><b>3.6</b> Estimación de los parámetros</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#derivaci%C3%B3n-paso-a-paso"><i class="fa fa-check"></i><b>3.6.1</b> Derivación paso a paso</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#bondad-de-ajuste"><i class="fa fa-check"></i><b>3.7</b> Bondad de ajuste</a></li>
<li class="chapter" data-level="3.8" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#supuestos-del-modelo-lineal-m%C3%BAltiple"><i class="fa fa-check"></i><b>3.8</b> Supuestos del modelo lineal múltiple</a></li>
<li class="chapter" data-level="3.9" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#regularizaci%C3%B3n-en-la-regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>3.9</b> Regularización en la Regresión Lineal</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#ridge-regression"><i class="fa fa-check"></i><b>3.9.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="3.9.2" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#lasso-regression"><i class="fa fa-check"></i><b>3.9.2</b> Lasso Regression</a></li>
<li class="chapter" data-level="3.9.3" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#elastic-net-regression"><i class="fa fa-check"></i><b>3.9.3</b> Elastic Net Regression</a></li>
<li class="chapter" data-level="3.9.4" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#comparaci%C3%B3n-de-m%C3%A9todos"><i class="fa fa-check"></i><b>3.9.4</b> Comparación de métodos</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#consideraciones"><i class="fa fa-check"></i><b>3.10</b> Consideraciones</a></li>
<li class="chapter" data-level="3.11" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#sesgo-y-varianza"><i class="fa fa-check"></i><b>3.11</b> Sesgo y Varianza</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#sesgo"><i class="fa fa-check"></i><b>3.11.1</b> Sesgo</a></li>
<li class="chapter" data-level="3.11.2" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#varianza"><i class="fa fa-check"></i><b>3.11.2</b> Varianza</a></li>
<li class="chapter" data-level="3.11.3" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#compromiso-sesgovarianza"><i class="fa fa-check"></i><b>3.11.3</b> Compromiso sesgo–varianza</a></li>
<li class="chapter" data-level="3.11.4" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#en-regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>3.11.4</b> En regresión lineal</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#conclusi%C3%B3n-de-la-secci%C3%B3n"><i class="fa fa-check"></i><b>3.12</b> Conclusión de la sección</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html"><i class="fa fa-check"></i><b>4</b> Redes neuronales Lineales para Clasificación</a>
<ul>
<li class="chapter" data-level="4.1" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#modelos-lineales-generalizados"><i class="fa fa-check"></i><b>4.1</b> Modelos Lineales Generalizados</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#historia-1"><i class="fa fa-check"></i><b>4.1.1</b> Historia</a></li>
<li class="chapter" data-level="4.1.2" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#definici%C3%B3n"><i class="fa fa-check"></i><b>4.1.2</b> Definición</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#regresi%C3%B3n-log%C3%ADstica-para-clasificaci%C3%B3n"><i class="fa fa-check"></i><b>4.2</b> Regresión Logística para Clasificación</a></li>
<li class="chapter" data-level="4.3" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#redes-neuronales"><i class="fa fa-check"></i><b>4.3</b> Redes neuronales</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#historia-2"><i class="fa fa-check"></i><b>4.3.1</b> Historia</a></li>
<li class="chapter" data-level="4.3.2" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#definici%C3%B3n-1"><i class="fa fa-check"></i><b>4.3.2</b> Definición</a></li>
<li class="chapter" data-level="4.3.3" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#neurona"><i class="fa fa-check"></i><b>4.3.3</b> Neurona</a></li>
<li class="chapter" data-level="4.3.4" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#redes-neuronales-1"><i class="fa fa-check"></i><b>4.3.4</b> Redes Neuronales</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#notas-adicionales"><i class="fa fa-check"></i>Notas adicionales</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html"><i class="fa fa-check"></i><b>5</b> Perceptrón Multicapa</a>
<ul>
<li class="chapter" data-level="5.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#perceptrones-multicapa"><i class="fa fa-check"></i><b>5.1</b> Perceptrones multicapa</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#capas-ocultas"><i class="fa fa-check"></i><b>5.1.1</b> Capas ocultas</a></li>
<li class="chapter" data-level="5.1.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#funciones-de-activaci%C3%B3n"><i class="fa fa-check"></i><b>5.1.2</b> Funciones de activación</a></li>
<li class="chapter" data-level="5.1.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#discusi%C3%B3n"><i class="fa fa-check"></i><b>5.1.3</b> Discusión</a></li>
<li class="chapter" data-level="5.1.4" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios"><i class="fa fa-check"></i><b>5.1.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n-de-perceptrones-multicapa"><i class="fa fa-check"></i><b>5.2</b> Implementación de Perceptrones Multicapa</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n-desde-cero"><i class="fa fa-check"></i><b>5.2.1</b> Implementación desde Cero</a></li>
<li class="chapter" data-level="5.2.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n"><i class="fa fa-check"></i><b>5.2.2</b> Implementación</a></li>
<li class="chapter" data-level="5.2.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios-1"><i class="fa fa-check"></i><b>5.2.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#forward-propagation-backward-propagation"><i class="fa fa-check"></i><b>5.3</b> Forward Propagation, Backward Propagation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#forward-propagation"><i class="fa fa-check"></i><b>5.3.1</b> Forward Propagation</a></li>
<li class="chapter" data-level="5.3.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#backpropagation"><i class="fa fa-check"></i><b>5.3.2</b> Backpropagation</a></li>
<li class="chapter" data-level="5.3.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#resumen-6"><i class="fa fa-check"></i><b>5.3.3</b> Resumen</a></li>
<li class="chapter" data-level="5.3.4" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios-2"><i class="fa fa-check"></i><b>5.3.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#estabilidad-num%C3%A9rica-e-inicializaci%C3%B3n"><i class="fa fa-check"></i><b>5.4</b> Estabilidad Numérica e Inicialización</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#explotaci%C3%B3n-y-desvanecimiento-de-gradientes"><i class="fa fa-check"></i><b>5.4.1</b> Explotación y Desvanecimiento de Gradientes</a></li>
<li class="chapter" data-level="5.4.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#inicializaci%C3%B3n-param%C3%A9trica"><i class="fa fa-check"></i><b>5.4.2</b> Inicialización paramétrica</a></li>
<li class="chapter" data-level="5.4.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios-3"><i class="fa fa-check"></i><b>5.4.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#generalizaci%C3%B3n-en-deep-learning"><i class="fa fa-check"></i><b>5.5</b> Generalización en Deep Learning</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#sobreajuste-y-regularizaci%C3%B3n"><i class="fa fa-check"></i><b>5.5.1</b> Sobreajuste y Regularización</a></li>
<li class="chapter" data-level="5.5.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#inspiraci%C3%B3n-de-los-no-param%C3%A9tricos"><i class="fa fa-check"></i><b>5.5.2</b> Inspiración de los no paramétricos</a></li>
<li class="chapter" data-level="5.5.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#early-stopping"><i class="fa fa-check"></i><b>5.5.3</b> Early Stopping</a></li>
<li class="chapter" data-level="5.5.4" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#m%C3%A9todos-cl%C3%A1sicos-de-regularizaci%C3%B3n-para-redes-profundas"><i class="fa fa-check"></i><b>5.5.4</b> Métodos clásicos de regularización para redes profundas</a></li>
<li class="chapter" data-level="5.5.5" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios-4"><i class="fa fa-check"></i><b>5.5.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#dropout"><i class="fa fa-check"></i><b>5.6</b> Dropout</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#dropout-en-la-pr%C3%A1ctica"><i class="fa fa-check"></i><b>5.6.1</b> Dropout en la práctica</a></li>
<li class="chapter" data-level="5.6.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n-desde-cero-1"><i class="fa fa-check"></i><b>5.6.2</b> Implementación desde cero</a></li>
<li class="chapter" data-level="5.6.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#definici%C3%B3n-del-modelo"><i class="fa fa-check"></i><b>5.6.3</b> Definición del modelo</a></li>
<li class="chapter" data-level="5.6.4" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#entrenamiento-2"><i class="fa fa-check"></i><b>5.6.4</b> Entrenamiento</a></li>
<li class="chapter" data-level="5.6.5" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n-concisa"><i class="fa fa-check"></i><b>5.6.5</b> Implementación concisa</a></li>
<li class="chapter" data-level="5.6.6" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#resumen-7"><i class="fa fa-check"></i><b>5.6.6</b> Resumen</a></li>
<li class="chapter" data-level="5.6.7" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios-5"><i class="fa fa-check"></i><b>5.6.7</b> Ejercicios</a></li>
<li class="chapter" data-level="5.6.8" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#dropout-en-la-pr%C3%A1ctica-1"><i class="fa fa-check"></i><b>5.6.8</b> Dropout en la práctica</a></li>
<li class="chapter" data-level="5.6.9" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n-desde-cero-2"><i class="fa fa-check"></i><b>5.6.9</b> Implementación desde cero</a></li>
<li class="chapter" data-level="5.6.10" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#definici%C3%B3n-del-modelo-1"><i class="fa fa-check"></i><b>5.6.10</b> Definición del modelo</a></li>
<li class="chapter" data-level="5.6.11" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#entrenamiento-3"><i class="fa fa-check"></i><b>5.6.11</b> Entrenamiento</a></li>
<li class="chapter" data-level="5.6.12" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n-concisa-1"><i class="fa fa-check"></i><b>5.6.12</b> Implementación concisa</a></li>
<li class="chapter" data-level="5.6.13" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#resumen-8"><i class="fa fa-check"></i><b>5.6.13</b> Resumen</a></li>
<li class="chapter" data-level="5.6.14" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios-6"><i class="fa fa-check"></i><b>5.6.14</b> Ejercicios</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gpus.html"><a href="gpus.html"><i class="fa fa-check"></i><b>6</b> GPUs</a>
<ul>
<li class="chapter" data-level="6.1" data-path="gpus.html"><a href="gpus.html#paralelismo-y-concurrencia"><i class="fa fa-check"></i><b>6.1</b> Paralelismo y concurrencia</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="gpus.html"><a href="gpus.html#reflexi%C3%B3n-concurrencia-y-paralelismo"><i class="fa fa-check"></i><b>6.1.1</b> Reflexión: Concurrencia y Paralelismo</a></li>
<li class="chapter" data-level="6.1.2" data-path="gpus.html"><a href="gpus.html#relaci%C3%B3n-entre-concurrencia-y-paralelismo"><i class="fa fa-check"></i><b>6.1.2</b> Relación entre concurrencia y paralelismo</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gpus.html"><a href="gpus.html#complejidad-computacional"><i class="fa fa-check"></i><b>6.2</b> Complejidad Computacional</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="gpus.html"><a href="gpus.html#analizando-la-complejidad-computacional-en-el-problema-de-ordenamiento-de-n%C3%BAmeros"><i class="fa fa-check"></i><b>6.2.1</b> Analizando la complejidad computacional en el problema de ordenamiento de números</a></li>
<li class="chapter" data-level="6.2.2" data-path="gpus.html"><a href="gpus.html#notaci%C3%B3n-big-o"><i class="fa fa-check"></i><b>6.2.2</b> Notación Big O</a></li>
<li class="chapter" data-level="6.2.3" data-path="gpus.html"><a href="gpus.html#bubble-sort-vs-merge-sort."><i class="fa fa-check"></i><b>6.2.3</b> Bubble sort vs Merge sort.</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gpus.html"><a href="gpus.html#el-papel-del-hardware"><i class="fa fa-check"></i><b>6.3</b> El papel del hardware</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="gpus.html"><a href="gpus.html#implementaci%C3%B3n-de-multiplicaci%C3%B3n-de-matrices-en-distintas-arquitecturas"><i class="fa fa-check"></i><b>6.3.1</b> Implementación de multiplicación de matrices en distintas arquitecturas</a></li>
<li class="chapter" data-level="6.3.2" data-path="gpus.html"><a href="gpus.html#es-viable-acelerar-cualquier-algoritmo-en-gpu-tpu"><i class="fa fa-check"></i><b>6.3.2</b> ¿Es viable acelerar cualquier algoritmo en GPU / TPU?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="redes-neuronales-convolucionales.html"><a href="redes-neuronales-convolucionales.html"><i class="fa fa-check"></i><b>7</b> Redes Neuronales Convolucionales</a></li>
<li class="chapter" data-level="8" data-path="redes-neuronales-convolucionales-modernas.html"><a href="redes-neuronales-convolucionales-modernas.html"><i class="fa fa-check"></i><b>8</b> Redes Neuronales Convolucionales Modernas</a></li>
<li class="chapter" data-level="9" data-path="redes-neuronales-recurrentes.html"><a href="redes-neuronales-recurrentes.html"><i class="fa fa-check"></i><b>9</b> Redes Neuronales Recurrentes</a></li>
<li class="chapter" data-level="10" data-path="redes-neuronales-recurrentes-modernas.html"><a href="redes-neuronales-recurrentes-modernas.html"><i class="fa fa-check"></i><b>10</b> Redes Neuronales Recurrentes Modernas</a></li>
<li class="chapter" data-level="11" data-path="mecanismos-de-atención-y-transformers.html"><a href="mecanismos-de-atención-y-transformers.html"><i class="fa fa-check"></i><b>11</b> Mecanismos de Atención y Transformers</a></li>
<li class="part"><span><b>Parte 3: Escalabilidad, Eficiencia y Aplicaciones</b></span></li>
<li class="chapter" data-level="12" data-path="algoritmos-de-optimización.html"><a href="algoritmos-de-optimización.html"><i class="fa fa-check"></i><b>12</b> Algoritmos de Optimización</a></li>
<li class="chapter" data-level="" data-path="miscellanea-intro-to-graph-neural-networks.html"><a href="miscellanea-intro-to-graph-neural-networks.html"><i class="fa fa-check"></i>Miscellanea: Intro to Graph Neural Networks</a></li>
<li class="chapter" data-level="13" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html"><i class="fa fa-check"></i><b>13</b> Introducción a la Teoría de Gráficas.</a>
<ul>
<li class="chapter" data-level="13.1" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#qu%C3%A9-es-una-gr%C3%A1fica"><i class="fa fa-check"></i><b>13.1</b> ¿Qué es una gráfica?</a></li>
<li class="chapter" data-level="13.2" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#problemas-cl%C3%A1sicos-de-teor%C3%ADa-de-gr%C3%A1ficas-selecci%C3%B3n"><i class="fa fa-check"></i><b>13.2</b> Problemas clásicos de teoría de gráficas (selección)</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#caminos-y-conectividad"><i class="fa fa-check"></i><b>13.2.1</b> Caminos y conectividad</a></li>
<li class="chapter" data-level="13.2.2" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#detecci%C3%B3n-de-comunidades"><i class="fa fa-check"></i><b>13.2.2</b> Detección de comunidades</a></li>
<li class="chapter" data-level="13.2.3" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#centralidad-e-influencia"><i class="fa fa-check"></i><b>13.2.3</b> Centralidad e influencia</a></li>
<li class="chapter" data-level="13.2.4" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#emparejamiento-y-asignaci%C3%B3n"><i class="fa fa-check"></i><b>13.2.4</b> Emparejamiento y asignación</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#ecosistema-de-herramientas-para-el-trabajo-con-grafos"><i class="fa fa-check"></i><b>13.3</b> Ecosistema de Herramientas para el Trabajo con Grafos</a></li>
<li class="chapter" data-level="13.4" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#por-qu%C3%A9-combinar-graficas-y-deep-learning"><i class="fa fa-check"></i><b>13.4</b> ¿Por qué combinar Graficas y Deep Learning?</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#a.-multilayer-percepton-en-las-features-tabulares-de-cora"><i class="fa fa-check"></i><b>13.4.1</b> A. Multilayer Percepton en las features tabulares de Cora</a></li>
<li class="chapter" data-level="13.4.2" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#b.-modelo-basado-en-una-capa-lineal-que-se-multiplica-por-la-matriz-de-adyacencia."><i class="fa fa-check"></i><b>13.4.2</b> B. Modelo basado en una capa lineal que se multiplica por la matriz de adyacencia.</a></li>
<li class="chapter" data-level="13.4.3" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#qu%C3%A9-esta-haciendo-la-red"><i class="fa fa-check"></i><b>13.4.3</b> ¿Qué esta haciendo la red?</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#tipos-de-problemas-de-gnn"><i class="fa fa-check"></i><b>13.5</b> Tipos de problemas de GNN</a></li>
<li class="chapter" data-level="13.6" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#existe-un-teorema-de-aproximaci%C3%B3n-univeral-tau-para-gnns"><i class="fa fa-check"></i><b>13.6</b> ¿Existe un Teorema de Aproximación Univeral (TAU) para GNN’s?</a></li>
<li class="chapter" data-level="13.7" data-path="introducción-a-la-teoría-de-gráficas..html"><a href="introducción-a-la-teoría-de-gráficas..html#enfoques-de-aprendizaje-transductivo-e-inductivo"><i class="fa fa-check"></i><b>13.7</b> Enfoques de aprendizaje transductivo e inductivo</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="graph-neural-network.html"><a href="graph-neural-network.html"><i class="fa fa-check"></i><b>14</b> Graph Neural Network</a>
<ul>
<li class="chapter" data-level="14.1" data-path="graph-neural-network.html"><a href="graph-neural-network.html#pytorch-geometric-y-graph-neural-networks"><i class="fa fa-check"></i><b>14.1</b> PyTorch Geometric y Graph Neural Networks</a></li>
<li class="chapter" data-level="14.2" data-path="graph-neural-network.html"><a href="graph-neural-network.html#graph-convolutional-networks-gnn"><i class="fa fa-check"></i><b>14.2</b> Graph Convolutional Networks (GNN)</a></li>
<li class="chapter" data-level="14.3" data-path="graph-neural-network.html"><a href="graph-neural-network.html#c%C3%B3mo-funcionan"><i class="fa fa-check"></i><b>14.3</b> ¿Cómo funcionan?</a></li>
<li class="chapter" data-level="14.4" data-path="graph-neural-network.html"><a href="graph-neural-network.html#implementaci%C3%B3n-en-pytorch-geometric"><i class="fa fa-check"></i><b>14.4</b> Implementación en PyTorch Geometric</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="graph-neural-network.html"><a href="graph-neural-network.html#ejemplo-predicci%C3%B3n-del-volumen-de-tr%C3%A1fico-en-wikipedia-regresi%C3%B3n"><i class="fa fa-check"></i><b>14.4.1</b> Ejemplo: Predicción del volumen de tráfico en Wikipedia (Regresión)</a></li>
<li class="chapter" data-level="14.4.2" data-path="graph-neural-network.html"><a href="graph-neural-network.html#ejemplo-clasificaci%C3%B3n-de-art%C3%ADculos-de-investigaci%C3%B3n-por-categor%C3%ADa"><i class="fa fa-check"></i><b>14.4.2</b> Ejemplo: Clasificación de artículos de investigación por categoría</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="graph-neural-network.html"><a href="graph-neural-network.html#graph-attention-network-gat"><i class="fa fa-check"></i><b>14.5</b> Graph Attention Network (GAT)</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="graph-neural-network.html"><a href="graph-neural-network.html#c%C3%B3mo-funcionan-1"><i class="fa fa-check"></i><b>14.5.1</b> ¿Cómo funcionan?</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="graph-neural-network.html"><a href="graph-neural-network.html#implementaci%C3%B3n-en-pytorch-geometric-1"><i class="fa fa-check"></i><b>14.6</b> Implementación en PyTorch Geometric</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="graph-neural-network.html"><a href="graph-neural-network.html#ejemplo-clasificaci%C3%B3n-de-art%C3%ADculos-de-investigaci%C3%B3n-por-categor%C3%ADa-parte-ii"><i class="fa fa-check"></i><b>14.6.1</b> Ejemplo: Clasificación de artículos de investigación por categoría (Parte II)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html"><i class="fa fa-check"></i><b>15</b> Embeddings y Graph Neural Network</a>
<ul>
<li class="chapter" data-level="15.1" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#qu%C3%A9-es-un-embedding"><i class="fa fa-check"></i><b>15.1</b> ¿Qué es un embedding?</a></li>
<li class="chapter" data-level="15.2" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#word2vec"><i class="fa fa-check"></i><b>15.2</b> Word2Vec</a></li>
<li class="chapter" data-level="15.3" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#node2vec"><i class="fa fa-check"></i><b>15.3</b> Node2Vec</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#ejemplo-aplicando-el-modelo-de-skipgrams-a-random-walks"><i class="fa fa-check"></i><b>15.3.1</b> Ejemplo: Aplicando el modelo de SkipGrams a Random Walks</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#sageconv"><i class="fa fa-check"></i><b>15.4</b> SageConv</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#ejemplo-clasificaci%C3%B3n-de-art%C3%ADculos-de-investigaci%C3%B3n-por-categor%C3%ADa-parte-ii-1"><i class="fa fa-check"></i><b>15.4.1</b> Ejemplo: Clasificación de artículos de investigación por categoría (Parte II)</a></li>
<li class="chapter" data-level="15.4.2" data-path="embeddings-y-graph-neural-network.html"><a href="embeddings-y-graph-neural-network.html#ejemplo-clasificaci%C3%B3n-de-art%C3%ADculos-de-investigaci%C3%B3n-por-categor%C3%ADa-parte-ii-2"><i class="fa fa-check"></i><b>15.4.2</b> Ejemplo: Clasificación de artículos de investigación por categoría (Parte II)</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="./"><img src="img/aif-logo.png" width="280"></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="embeddings-y-graph-neural-network" class="section level1 hasAnchor" number="15">
<h1><span class="header-section-number">Capítulo 15</span> Embeddings y Graph Neural Network<a href="embeddings-y-graph-neural-network.html#embeddings-y-graph-neural-network" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="qué-es-un-embedding" class="section level2 hasAnchor" number="15.1">
<h2><span class="header-section-number">15.1</span> ¿Qué es un embedding?<a href="embeddings-y-graph-neural-network.html#qu%C3%A9-es-un-embedding" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En el aprendizaje automático tradicional, las categorías (como nombres de
ciudades, tipos de átomos o IDs de usuarios) se representaban mediante representaciones
numéricas como One-Hot Encoding.</p>
<p>Aunque de utilidad, en la práctica dicha soluciones padecen de algunos problemas:
i) <strong>incapacidad semántica</strong>: en el caso palabras, las representaciones de pareja
que tiene un significado cercano, sus encondings se pueden encontrar muy alejados
(e.g. El vector de “perro” [1,0,0] y el de “cachorro” [0,1,0] son ortogonales;
no tienen ninguna relación); ii) <strong>explosión de memoria</strong>:al tener muchos elementos,
cada vector puede tener un número grande dimensiones, pero que la mayoría sean de ceros (sparse).</p>
<p>Para dar solucion a este tema, se emplean los embeddings, los cuale son
representaciones densas de datos discretos. Es decir, para representar los puntos
consolidamos una transformacion de los datos hacia un espacio, que en
lugar de crear un vector gigante de ceros y unos, representa a cada elemento
como un vector de números reales de tamaño fijo (por ejemplo, 128 o 300 dimensiones).</p>
<p>La idea esencial es proyectar los puntos en un espacio continuo de baja dimensión donde la
la similitud de los puntos (e.g semantica, geométrica, estructural) se conserve
geometricamente. Es decir puntos que son parecidos en el espacio original,
tengan una representacion cercana bajo dicha función.</p>
<p>En Deep Learning, los embeddding ajustando los pesos pesos de capaz de unaa red neuronal.
La red comienza con vectores aleatorios y a medida que la red intenta resolver
una tarea (e.g. predecir la siguiente palabra, image o clasificar un nodo), se
ajustan la representación vectorial.</p>
<p>Al final, elementos que aparecen en contextos similares terminan “empujados”
hacia la misma zona en el espacio vectorial.</p>
<p>Para ejemplificarlo podemos construir embeddings para los elementos
del Dataset Iris.</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb245-1"><a href="embeddings-y-graph-neural-network.html#cb245-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb245-2"><a href="embeddings-y-graph-neural-network.html#cb245-2" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb245-3"><a href="embeddings-y-graph-neural-network.html#cb245-3" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset</span>
<span id="cb245-4"><a href="embeddings-y-graph-neural-network.html#cb245-4" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb245-5"><a href="embeddings-y-graph-neural-network.html#cb245-5" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb245-6"><a href="embeddings-y-graph-neural-network.html#cb245-6" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb245-7"><a href="embeddings-y-graph-neural-network.html#cb245-7" tabindex="-1"></a></span>
<span id="cb245-8"><a href="embeddings-y-graph-neural-network.html#cb245-8" tabindex="-1"></a><span class="co"># Datos</span></span>
<span id="cb245-9"><a href="embeddings-y-graph-neural-network.html#cb245-9" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb245-10"><a href="embeddings-y-graph-neural-network.html#cb245-10" tabindex="-1"></a>X <span class="op">=</span> torch.tensor(iris.data, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb245-11"><a href="embeddings-y-graph-neural-network.html#cb245-11" tabindex="-1"></a>y <span class="op">=</span> torch.tensor(iris.target, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb245-12"><a href="embeddings-y-graph-neural-network.html#cb245-12" tabindex="-1"></a></span>
<span id="cb245-13"><a href="embeddings-y-graph-neural-network.html#cb245-13" tabindex="-1"></a>dataset <span class="op">=</span> TensorDataset(X, y)</span>
<span id="cb245-14"><a href="embeddings-y-graph-neural-network.html#cb245-14" tabindex="-1"></a>loader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb245-15"><a href="embeddings-y-graph-neural-network.html#cb245-15" tabindex="-1"></a></span>
<span id="cb245-16"><a href="embeddings-y-graph-neural-network.html#cb245-16" tabindex="-1"></a><span class="co"># Modelo</span></span>
<span id="cb245-17"><a href="embeddings-y-graph-neural-network.html#cb245-17" tabindex="-1"></a><span class="kw">class</span> Classifier(nn.Module):</span>
<span id="cb245-18"><a href="embeddings-y-graph-neural-network.html#cb245-18" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim<span class="op">=</span><span class="dv">4</span>, emb_dim<span class="op">=</span><span class="dv">8</span>, num_classes<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb245-19"><a href="embeddings-y-graph-neural-network.html#cb245-19" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb245-20"><a href="embeddings-y-graph-neural-network.html#cb245-20" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb245-21"><a href="embeddings-y-graph-neural-network.html#cb245-21" tabindex="-1"></a>            nn.Linear(input_dim, emb_dim),</span>
<span id="cb245-22"><a href="embeddings-y-graph-neural-network.html#cb245-22" tabindex="-1"></a>            nn.ReLU()</span>
<span id="cb245-23"><a href="embeddings-y-graph-neural-network.html#cb245-23" tabindex="-1"></a>        )</span>
<span id="cb245-24"><a href="embeddings-y-graph-neural-network.html#cb245-24" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(emb_dim, num_classes)</span>
<span id="cb245-25"><a href="embeddings-y-graph-neural-network.html#cb245-25" tabindex="-1"></a></span>
<span id="cb245-26"><a href="embeddings-y-graph-neural-network.html#cb245-26" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb245-27"><a href="embeddings-y-graph-neural-network.html#cb245-27" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb245-28"><a href="embeddings-y-graph-neural-network.html#cb245-28" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.classifier(z), z</span>
<span id="cb245-29"><a href="embeddings-y-graph-neural-network.html#cb245-29" tabindex="-1"></a></span>
<span id="cb245-30"><a href="embeddings-y-graph-neural-network.html#cb245-30" tabindex="-1"></a>model <span class="op">=</span> Classifier()</span>
<span id="cb245-31"><a href="embeddings-y-graph-neural-network.html#cb245-31" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb245-32"><a href="embeddings-y-graph-neural-network.html#cb245-32" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb245-33"><a href="embeddings-y-graph-neural-network.html#cb245-33" tabindex="-1"></a></span>
<span id="cb245-34"><a href="embeddings-y-graph-neural-network.html#cb245-34" tabindex="-1"></a><span class="co"># Entrenamiento breve</span></span>
<span id="cb245-35"><a href="embeddings-y-graph-neural-network.html#cb245-35" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb245-36"><a href="embeddings-y-graph-neural-network.html#cb245-36" tabindex="-1"></a>    <span class="cf">for</span> xb, yb <span class="kw">in</span> loader:</span>
<span id="cb245-37"><a href="embeddings-y-graph-neural-network.html#cb245-37" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb245-38"><a href="embeddings-y-graph-neural-network.html#cb245-38" tabindex="-1"></a>        logits, _ <span class="op">=</span> model(xb)</span>
<span id="cb245-39"><a href="embeddings-y-graph-neural-network.html#cb245-39" tabindex="-1"></a>        loss <span class="op">=</span> criterion(logits, yb)</span>
<span id="cb245-40"><a href="embeddings-y-graph-neural-network.html#cb245-40" tabindex="-1"></a>        loss.backward()</span>
<span id="cb245-41"><a href="embeddings-y-graph-neural-network.html#cb245-41" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb245-42"><a href="embeddings-y-graph-neural-network.html#cb245-42" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb245-43"><a href="embeddings-y-graph-neural-network.html#cb245-43" tabindex="-1"></a>      <span class="bu">print</span>(<span class="ss">f&quot;Epoch </span><span class="sc">{</span>epoch<span class="sc">:&gt;3}</span><span class="ss"> | Loss: </span><span class="sc">{</span>loss<span class="sc">:.5f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>## Epoch   0 | Loss: 0.97061
## Epoch  10 | Loss: 0.51020
## Epoch  20 | Loss: 0.26753
## Epoch  30 | Loss: 0.20009
## Epoch  40 | Loss: 0.11925
## Epoch  50 | Loss: 0.06770
## Epoch  60 | Loss: 0.07769
## Epoch  70 | Loss: 0.13522
## Epoch  80 | Loss: 0.09195
## Epoch  90 | Loss: 0.10649</code></pre>
<div class="sourceCode" id="cb247"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb247-1"><a href="embeddings-y-graph-neural-network.html#cb247-1" tabindex="-1"></a><span class="co"># Obtener embeddings</span></span>
<span id="cb247-2"><a href="embeddings-y-graph-neural-network.html#cb247-2" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb247-3"><a href="embeddings-y-graph-neural-network.html#cb247-3" tabindex="-1"></a>    embeddings <span class="op">=</span> model.encoder(X).numpy()</span>
<span id="cb247-4"><a href="embeddings-y-graph-neural-network.html#cb247-4" tabindex="-1"></a></span>
<span id="cb247-5"><a href="embeddings-y-graph-neural-network.html#cb247-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Embeddings: &quot;</span>)</span></code></pre></div>
<pre><code>## Embeddings:</code></pre>
<div class="sourceCode" id="cb249"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb249-1"><a href="embeddings-y-graph-neural-network.html#cb249-1" tabindex="-1"></a>embeddings[:<span class="dv">2</span>]</span></code></pre></div>
<pre><code>## array([[1.228936 , 0.       , 6.7014956, 0.       , 0.       , 3.3533223,
##         0.       , 2.6186879],
##        [1.3932565, 0.       , 6.039477 , 0.       , 0.       , 2.9469004,
##         0.       , 2.3108451]], dtype=float32)</code></pre>
<p>Ahora usaremos PCA para obtener la representación de dichos puntos:</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb251-1"><a href="embeddings-y-graph-neural-network.html#cb251-1" tabindex="-1"></a><span class="co"># PCA a 2 dimensiones</span></span>
<span id="cb251-2"><a href="embeddings-y-graph-neural-network.html#cb251-2" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb251-3"><a href="embeddings-y-graph-neural-network.html#cb251-3" tabindex="-1"></a>emb_2d <span class="op">=</span> pca.fit_transform(embeddings)</span>
<span id="cb251-4"><a href="embeddings-y-graph-neural-network.html#cb251-4" tabindex="-1"></a></span>
<span id="cb251-5"><a href="embeddings-y-graph-neural-network.html#cb251-5" tabindex="-1"></a><span class="co"># Plot (una sola figura)</span></span>
<span id="cb251-6"><a href="embeddings-y-graph-neural-network.html#cb251-6" tabindex="-1"></a>plt.figure()</span></code></pre></div>
<pre><code>## &lt;Figure size 700x500 with 0 Axes&gt;</code></pre>
<div class="sourceCode" id="cb253"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb253-1"><a href="embeddings-y-graph-neural-network.html#cb253-1" tabindex="-1"></a>scatter <span class="op">=</span> plt.scatter(</span>
<span id="cb253-2"><a href="embeddings-y-graph-neural-network.html#cb253-2" tabindex="-1"></a>    emb_2d[:, <span class="dv">0</span>],</span>
<span id="cb253-3"><a href="embeddings-y-graph-neural-network.html#cb253-3" tabindex="-1"></a>    emb_2d[:, <span class="dv">1</span>],</span>
<span id="cb253-4"><a href="embeddings-y-graph-neural-network.html#cb253-4" tabindex="-1"></a>    c<span class="op">=</span>y</span>
<span id="cb253-5"><a href="embeddings-y-graph-neural-network.html#cb253-5" tabindex="-1"></a>)</span>
<span id="cb253-6"><a href="embeddings-y-graph-neural-network.html#cb253-6" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;PCA 1&quot;</span>)</span></code></pre></div>
<pre><code>## Text(0.5, 0, &#39;PCA 1&#39;)</code></pre>
<div class="sourceCode" id="cb255"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb255-1"><a href="embeddings-y-graph-neural-network.html#cb255-1" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;PCA 2&quot;</span>)</span></code></pre></div>
<pre><code>## Text(0, 0.5, &#39;PCA 2&#39;)</code></pre>
<div class="sourceCode" id="cb257"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb257-1"><a href="embeddings-y-graph-neural-network.html#cb257-1" tabindex="-1"></a>plt.title(<span class="st">&quot;Embedding del dataset Iris (PyTorch + PCA)&quot;</span>)</span></code></pre></div>
<pre><code>## Text(0.5, 1.0, &#39;Embedding del dataset Iris (PyTorch + PCA)&#39;)</code></pre>
<div class="sourceCode" id="cb259"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb259-1"><a href="embeddings-y-graph-neural-network.html#cb259-1" tabindex="-1"></a><span class="co"># Leyenda correcta</span></span>
<span id="cb259-2"><a href="embeddings-y-graph-neural-network.html#cb259-2" tabindex="-1"></a>handles, _ <span class="op">=</span> scatter.legend_elements()</span>
<span id="cb259-3"><a href="embeddings-y-graph-neural-network.html#cb259-3" tabindex="-1"></a>plt.legend(handles, iris.target_names, title<span class="op">=</span><span class="st">&quot;Clase&quot;</span>)</span></code></pre></div>
<pre><code>## &lt;matplotlib.legend.Legend object at 0x7e62564fddd0&gt;</code></pre>
<div class="sourceCode" id="cb261"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb261-1"><a href="embeddings-y-graph-neural-network.html#cb261-1" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="deep_learning_course_python_files/figure-html/unnamed-chunk-171-1.png" alt="" width="672" />
Matematicamente, si tenemos V elementos y queremos vectores de tamaño d,
un embedding es una matriz <span class="math inline">\(E\)</span> que transforma los datos de entrada de la siguiente
forma:</p>
<p><span class="math display">\[\begin{equation}
E \in \mathbb{R}^{V \times d}
\end{equation}\]</span></p>
<p>En donde
* <span class="math inline">\(E\)</span>: Matriz de parámetros aprendibles (Embedding Layer).
* <span class="math inline">\(V\)</span>: Tamaño del vocabulario o número total de entidades.
* <span class="math inline">\(d\)</span>: Dimensionalidad del espacio latente (hiperparámetro).
* <span class="math inline">\(e_1 \cdot e_2\)</span>: Producto punto, que mide la dirección común de los vectores.
* <span class="math inline">\(\|e\|\)</span>: Norma euclidiana que normaliza el vector para ignorar su magnitud y
enfocarse en la orientación.</p>
<p>Para obtener el vector <span class="math inline">\(e_i\)</span> de un elemento con índice <span class="math inline">\(i\)</span>, multiplicamos un
vector $ <span class="math inline">\(x_i\)</span> por la matriz:</p>
<p><span class="math display">\[\begin{equation}
e_i = x_i^\top E
\end{equation}\]</span></p>
<p>Un forma común de saber qué tan parecidos son dos embeddings es mediante el
coseno del ángulo entre ellos:</p>
<p><span class="math display">\[\begin{equation}
\text{sim}(e_1, e_2) = \frac{e_1 \cdot e_2}{\|e_1\| \|e_2\|}
\end{equation}\]</span></p>
<p>Los embedding han tenido mucha adopcion en diversas aplicaciones de Deep Learning:</p>
<ul>
<li>NLP: Word2Vec, GloVe y los embeddings de Transformers (BERT/GPT) para entender el lenguaje.</li>
<li>Sistemas de Recomendación: Representar usuarios y productos en el mismo espacio para medir su afinidad.</li>
<li>Visión por Computador: Face Embeddings para reconocimiento facial (si dos fotos generan vectores cercanos, son la misma persona).</li>
<li>GNNs: Todos los modelos previos (GCN, GAT, SAGE) generan, en última instancia, Node Embeddings.</li>
</ul>
</div>
<div id="word2vec" class="section level2 hasAnchor" number="15.2">
<h2><span class="header-section-number">15.2</span> Word2Vec<a href="embeddings-y-graph-neural-network.html#word2vec" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>WIP</p>
</div>
<div id="node2vec" class="section level2 hasAnchor" number="15.3">
<h2><span class="header-section-number">15.3</span> Node2Vec<a href="embeddings-y-graph-neural-network.html#node2vec" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Antes de que las GNN fueran el estándar, el reto era: ¿Cómo convertimos un nodo
de un grafo en un vector de números (embedding) que una red neuronal tradicional
pueda entender?</p>
<p><img src="img/21-gnn/shallow_node_embeddings.png" alt="" width="120%" style="display: block; margin: auto;" /></p>
<p>Como hemos mencionado previamente, en el área de procesamiento de lenguaje natural
se propuso al modelo <em>word2vec</em> en donde se construye un embedding de palabras
usando la idea que las palabras que aparecen en contextos similares deben
induceir vectores similares. Node2vec se construye con ideas similares: para
obtener nodos en contexto similares se ejecutan caminatas aleatorias sobre
la gráfica (random walks), de forma que podemos tratar un grafo como un lenguaje.</p>
<p>Node2vec permite dicho el algoritmo explore el grafo de forma más flexible,
capturando tanto comunidades locales como roles estructurales. Dicha exploración
se basa en generar “paseos aleatorios” con cierto nivel de sesgo (biased random
walk) desde cada nodo y luego alimentar esos paseos a un modelo Skip-gram (el
mismo de word2vec).</p>
<p>Las caminatas aleatorias en este enfoque tiene dos hiper-parámetros para explorar
la gráfica mediante caminan que obtienen nodos usando trayectorias que usan
información de los nodos visitados:</p>
<ul>
<li><p><strong>Parámetro de Retorno (<span class="math inline">\(p\)</span>):</strong> Controla la probabilidad de regresar inmediatamente al
nodo anterior. Un <span class="math inline">\(p\)</span> bajo favorece la exploración local (BFS - Breadth-First Search),
capturando la similitud estructural (nodos que actúan como “hubs”).</p></li>
<li><p><strong>Parámetro de “In-out” (<span class="math inline">\(q\)</span>):</strong> Controla la probabilidad de alejarse hacia nodos no visitados.
Un <span class="math inline">\(q\)</span> bajo favorece la exploración profunda (DFS - Depth-First Search), capturando comunidades o macro-estructuras.</p></li>
</ul>
<p>Para calcular la representación <span class="math inline">\(z_v\)</span> de los nodos <span class="math inline">\(v\)</span> de la grafíca, se ajusta
la función de perdida con elobjetivo es maximizar la probabilidad de co-ocurrencia
de nodos en una vecindad:</p>
<p><span class="math display">\[\begin{equation}
\mathcal{L} = \sum_{w \in \mathcal{W}} - \log \left(\sigma(\mathbf{z}_v^{\top} \mathbf{z}_w) \right) + \sum_{w \sim \mathcal{V} \setminus \mathcal{W}} - \log \left( 1 - \sigma(\mathbf{z}_v^{\top} \mathbf{z}_w) \right),
\end{equation}\]</span></p>
<p>El factor de la derecha es es el negativa sampling, que induce una penalización
en la representación del embedding, es decir, si dos nodos no están relacionados
en las vecindades exploradas de las caminatas aleatorias, entonces los queremos
alejados.</p>
<p>Cabe destacar que <em>Node2Vec</em> es una representacion de tipo <em>Shallow Embedding</em>,
las cuales son representaciones vectoriales de baja dimensión mediante una tabla
de búsqueda , de modo que se maximiza la probabilidad de preservar las vecindades;</p>
<p>A su vez, tales representaciones son utiles como entrada para una tarea posterior
determinada; por ejemplo, en tareas a nivel de nodo, pueden utilizarse directamente
como entrada para un clasificador final. Para tareas a nivel de aristas, las
representaciones a nivel de borde se pueden obtener mediante el promedio o el
producto de Hadamard.</p>
<p>Sin embargo, cuentan con ciertas limitaciones: i) no incorporan información de las
características asociada a nodos y/o aristas, ii) son representaciones transductivas
(es decir, no pueden aplicarse elementos no vistos en el entrenamiento, ya que
los parámetros aprendibles están fijados a los nodos de un grafo en particular.</p>
<div id="ejemplo-aplicando-el-modelo-de-skipgrams-a-random-walks" class="section level3 hasAnchor" number="15.3.1">
<h3><span class="header-section-number">15.3.1</span> Ejemplo: Aplicando el modelo de SkipGrams a Random Walks<a href="embeddings-y-graph-neural-network.html#ejemplo-aplicando-el-modelo-de-skipgrams-a-random-walks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>En esta sección se presenta código que muestra una version simplificada de los elementos
de node2vec para la gráfica <em>Karate Club</em>, en primer término se calculan una secuencia de nodos obtenidos de
caminatas aleatorias que posteriormente se emplean para crear un encaje.</p>
<p>Dicho encaje se realiza con la clase <em>Word2Vec</em> de la libreria <em>gensim</em>.</p>
<p>Empleando la siguiente clase se simulan caminatas aleatorias a partir
de la estructura de la gráfica.</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb262-1"><a href="embeddings-y-graph-neural-network.html#cb262-1" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb262-2"><a href="embeddings-y-graph-neural-network.html#cb262-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb262-3"><a href="embeddings-y-graph-neural-network.html#cb262-3" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb262-4"><a href="embeddings-y-graph-neural-network.html#cb262-4" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb262-5"><a href="embeddings-y-graph-neural-network.html#cb262-5" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb262-6"><a href="embeddings-y-graph-neural-network.html#cb262-6" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> KeyedVectors, Word2Vec</span>
<span id="cb262-7"><a href="embeddings-y-graph-neural-network.html#cb262-7" tabindex="-1"></a></span>
<span id="cb262-8"><a href="embeddings-y-graph-neural-network.html#cb262-8" tabindex="-1"></a><span class="kw">class</span> Graph():</span>
<span id="cb262-9"><a href="embeddings-y-graph-neural-network.html#cb262-9" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, nx_G, is_directed, p, q):</span>
<span id="cb262-10"><a href="embeddings-y-graph-neural-network.html#cb262-10" tabindex="-1"></a>        <span class="va">self</span>.G <span class="op">=</span> nx_G</span>
<span id="cb262-11"><a href="embeddings-y-graph-neural-network.html#cb262-11" tabindex="-1"></a>        <span class="va">self</span>.is_directed <span class="op">=</span> is_directed</span>
<span id="cb262-12"><a href="embeddings-y-graph-neural-network.html#cb262-12" tabindex="-1"></a>        <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb262-13"><a href="embeddings-y-graph-neural-network.html#cb262-13" tabindex="-1"></a>        <span class="va">self</span>.q <span class="op">=</span> q</span>
<span id="cb262-14"><a href="embeddings-y-graph-neural-network.html#cb262-14" tabindex="-1"></a></span>
<span id="cb262-15"><a href="embeddings-y-graph-neural-network.html#cb262-15" tabindex="-1"></a>    <span class="kw">def</span> node2vec_walk(<span class="va">self</span>, walk_length, start_node):</span>
<span id="cb262-16"><a href="embeddings-y-graph-neural-network.html#cb262-16" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb262-17"><a href="embeddings-y-graph-neural-network.html#cb262-17" tabindex="-1"></a><span class="co">        Simulate a random walk starting from start node.</span></span>
<span id="cb262-18"><a href="embeddings-y-graph-neural-network.html#cb262-18" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb262-19"><a href="embeddings-y-graph-neural-network.html#cb262-19" tabindex="-1"></a>        G <span class="op">=</span> <span class="va">self</span>.G</span>
<span id="cb262-20"><a href="embeddings-y-graph-neural-network.html#cb262-20" tabindex="-1"></a>        alias_nodes <span class="op">=</span> <span class="va">self</span>.alias_nodes</span>
<span id="cb262-21"><a href="embeddings-y-graph-neural-network.html#cb262-21" tabindex="-1"></a>        alias_edges <span class="op">=</span> <span class="va">self</span>.alias_edges</span>
<span id="cb262-22"><a href="embeddings-y-graph-neural-network.html#cb262-22" tabindex="-1"></a></span>
<span id="cb262-23"><a href="embeddings-y-graph-neural-network.html#cb262-23" tabindex="-1"></a>        walk <span class="op">=</span> [start_node]</span>
<span id="cb262-24"><a href="embeddings-y-graph-neural-network.html#cb262-24" tabindex="-1"></a></span>
<span id="cb262-25"><a href="embeddings-y-graph-neural-network.html#cb262-25" tabindex="-1"></a>        <span class="cf">while</span> <span class="bu">len</span>(walk) <span class="op">&lt;</span> walk_length:</span>
<span id="cb262-26"><a href="embeddings-y-graph-neural-network.html#cb262-26" tabindex="-1"></a>            cur <span class="op">=</span> walk[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb262-27"><a href="embeddings-y-graph-neural-network.html#cb262-27" tabindex="-1"></a>            cur_nbrs <span class="op">=</span> <span class="bu">sorted</span>(G.neighbors(cur))</span>
<span id="cb262-28"><a href="embeddings-y-graph-neural-network.html#cb262-28" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(cur_nbrs) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb262-29"><a href="embeddings-y-graph-neural-network.html#cb262-29" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">len</span>(walk) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb262-30"><a href="embeddings-y-graph-neural-network.html#cb262-30" tabindex="-1"></a>                    walk.append(</span>
<span id="cb262-31"><a href="embeddings-y-graph-neural-network.html#cb262-31" tabindex="-1"></a>                        cur_nbrs[</span>
<span id="cb262-32"><a href="embeddings-y-graph-neural-network.html#cb262-32" tabindex="-1"></a>                            alias_draw(</span>
<span id="cb262-33"><a href="embeddings-y-graph-neural-network.html#cb262-33" tabindex="-1"></a>                                alias_nodes[cur][<span class="dv">0</span>],</span>
<span id="cb262-34"><a href="embeddings-y-graph-neural-network.html#cb262-34" tabindex="-1"></a>                                alias_nodes[cur][<span class="dv">1</span>]</span>
<span id="cb262-35"><a href="embeddings-y-graph-neural-network.html#cb262-35" tabindex="-1"></a>                            )</span>
<span id="cb262-36"><a href="embeddings-y-graph-neural-network.html#cb262-36" tabindex="-1"></a>                        ]</span>
<span id="cb262-37"><a href="embeddings-y-graph-neural-network.html#cb262-37" tabindex="-1"></a>                    )</span>
<span id="cb262-38"><a href="embeddings-y-graph-neural-network.html#cb262-38" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb262-39"><a href="embeddings-y-graph-neural-network.html#cb262-39" tabindex="-1"></a>                    prev <span class="op">=</span> walk[<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb262-40"><a href="embeddings-y-graph-neural-network.html#cb262-40" tabindex="-1"></a>                    next_node <span class="op">=</span> cur_nbrs[</span>
<span id="cb262-41"><a href="embeddings-y-graph-neural-network.html#cb262-41" tabindex="-1"></a>                        alias_draw(</span>
<span id="cb262-42"><a href="embeddings-y-graph-neural-network.html#cb262-42" tabindex="-1"></a>                            alias_edges[(prev, cur)][<span class="dv">0</span>],</span>
<span id="cb262-43"><a href="embeddings-y-graph-neural-network.html#cb262-43" tabindex="-1"></a>                            alias_edges[(prev, cur)][<span class="dv">1</span>]</span>
<span id="cb262-44"><a href="embeddings-y-graph-neural-network.html#cb262-44" tabindex="-1"></a>                        )</span>
<span id="cb262-45"><a href="embeddings-y-graph-neural-network.html#cb262-45" tabindex="-1"></a>                    ]</span>
<span id="cb262-46"><a href="embeddings-y-graph-neural-network.html#cb262-46" tabindex="-1"></a>                    walk.append(next_node)</span>
<span id="cb262-47"><a href="embeddings-y-graph-neural-network.html#cb262-47" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb262-48"><a href="embeddings-y-graph-neural-network.html#cb262-48" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb262-49"><a href="embeddings-y-graph-neural-network.html#cb262-49" tabindex="-1"></a></span>
<span id="cb262-50"><a href="embeddings-y-graph-neural-network.html#cb262-50" tabindex="-1"></a>        <span class="cf">return</span> walk</span>
<span id="cb262-51"><a href="embeddings-y-graph-neural-network.html#cb262-51" tabindex="-1"></a></span>
<span id="cb262-52"><a href="embeddings-y-graph-neural-network.html#cb262-52" tabindex="-1"></a>    <span class="kw">def</span> simulate_walks(<span class="va">self</span>, num_walks, walk_length):</span>
<span id="cb262-53"><a href="embeddings-y-graph-neural-network.html#cb262-53" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb262-54"><a href="embeddings-y-graph-neural-network.html#cb262-54" tabindex="-1"></a><span class="co">        Repeatedly simulate random walks from each node.</span></span>
<span id="cb262-55"><a href="embeddings-y-graph-neural-network.html#cb262-55" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb262-56"><a href="embeddings-y-graph-neural-network.html#cb262-56" tabindex="-1"></a>        G <span class="op">=</span> <span class="va">self</span>.G</span>
<span id="cb262-57"><a href="embeddings-y-graph-neural-network.html#cb262-57" tabindex="-1"></a>        walks <span class="op">=</span> []</span>
<span id="cb262-58"><a href="embeddings-y-graph-neural-network.html#cb262-58" tabindex="-1"></a>        nodes <span class="op">=</span> <span class="bu">list</span>(G.nodes())</span>
<span id="cb262-59"><a href="embeddings-y-graph-neural-network.html#cb262-59" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;Walk iteration:&#39;</span>)</span>
<span id="cb262-60"><a href="embeddings-y-graph-neural-network.html#cb262-60" tabindex="-1"></a>        <span class="cf">for</span> walk_iter <span class="kw">in</span> <span class="bu">range</span>(num_walks):</span>
<span id="cb262-61"><a href="embeddings-y-graph-neural-network.html#cb262-61" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&#39;</span><span class="sc">{</span>walk_iter <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> / </span><span class="sc">{</span>num_walks<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb262-62"><a href="embeddings-y-graph-neural-network.html#cb262-62" tabindex="-1"></a>            random.shuffle(nodes)</span>
<span id="cb262-63"><a href="embeddings-y-graph-neural-network.html#cb262-63" tabindex="-1"></a>            <span class="cf">for</span> node <span class="kw">in</span> nodes:</span>
<span id="cb262-64"><a href="embeddings-y-graph-neural-network.html#cb262-64" tabindex="-1"></a>                walks.append(</span>
<span id="cb262-65"><a href="embeddings-y-graph-neural-network.html#cb262-65" tabindex="-1"></a>                    <span class="va">self</span>.node2vec_walk(</span>
<span id="cb262-66"><a href="embeddings-y-graph-neural-network.html#cb262-66" tabindex="-1"></a>                        walk_length<span class="op">=</span>walk_length,</span>
<span id="cb262-67"><a href="embeddings-y-graph-neural-network.html#cb262-67" tabindex="-1"></a>                        start_node<span class="op">=</span>node</span>
<span id="cb262-68"><a href="embeddings-y-graph-neural-network.html#cb262-68" tabindex="-1"></a>                    )</span>
<span id="cb262-69"><a href="embeddings-y-graph-neural-network.html#cb262-69" tabindex="-1"></a>                )</span>
<span id="cb262-70"><a href="embeddings-y-graph-neural-network.html#cb262-70" tabindex="-1"></a>        <span class="cf">return</span> walks</span>
<span id="cb262-71"><a href="embeddings-y-graph-neural-network.html#cb262-71" tabindex="-1"></a></span>
<span id="cb262-72"><a href="embeddings-y-graph-neural-network.html#cb262-72" tabindex="-1"></a>    <span class="kw">def</span> get_alias_edge(<span class="va">self</span>, src, dst):</span>
<span id="cb262-73"><a href="embeddings-y-graph-neural-network.html#cb262-73" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb262-74"><a href="embeddings-y-graph-neural-network.html#cb262-74" tabindex="-1"></a><span class="co">        Get the alias edge setup lists for a given edge.</span></span>
<span id="cb262-75"><a href="embeddings-y-graph-neural-network.html#cb262-75" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb262-76"><a href="embeddings-y-graph-neural-network.html#cb262-76" tabindex="-1"></a>        G <span class="op">=</span> <span class="va">self</span>.G</span>
<span id="cb262-77"><a href="embeddings-y-graph-neural-network.html#cb262-77" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>.p</span>
<span id="cb262-78"><a href="embeddings-y-graph-neural-network.html#cb262-78" tabindex="-1"></a>        q <span class="op">=</span> <span class="va">self</span>.q</span>
<span id="cb262-79"><a href="embeddings-y-graph-neural-network.html#cb262-79" tabindex="-1"></a></span>
<span id="cb262-80"><a href="embeddings-y-graph-neural-network.html#cb262-80" tabindex="-1"></a>        unnormalized_probs <span class="op">=</span> []</span>
<span id="cb262-81"><a href="embeddings-y-graph-neural-network.html#cb262-81" tabindex="-1"></a>        <span class="cf">for</span> dst_nbr <span class="kw">in</span> <span class="bu">sorted</span>(G.neighbors(dst)):</span>
<span id="cb262-82"><a href="embeddings-y-graph-neural-network.html#cb262-82" tabindex="-1"></a>            weight <span class="op">=</span> G[dst][dst_nbr].get(<span class="st">&#39;weight&#39;</span>, <span class="fl">1.0</span>)</span>
<span id="cb262-83"><a href="embeddings-y-graph-neural-network.html#cb262-83" tabindex="-1"></a></span>
<span id="cb262-84"><a href="embeddings-y-graph-neural-network.html#cb262-84" tabindex="-1"></a>            <span class="cf">if</span> dst_nbr <span class="op">==</span> src:</span>
<span id="cb262-85"><a href="embeddings-y-graph-neural-network.html#cb262-85" tabindex="-1"></a>                unnormalized_probs.append(weight <span class="op">/</span> p)</span>
<span id="cb262-86"><a href="embeddings-y-graph-neural-network.html#cb262-86" tabindex="-1"></a>            <span class="cf">elif</span> G.has_edge(dst_nbr, src):</span>
<span id="cb262-87"><a href="embeddings-y-graph-neural-network.html#cb262-87" tabindex="-1"></a>                unnormalized_probs.append(weight)</span>
<span id="cb262-88"><a href="embeddings-y-graph-neural-network.html#cb262-88" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb262-89"><a href="embeddings-y-graph-neural-network.html#cb262-89" tabindex="-1"></a>                unnormalized_probs.append(weight <span class="op">/</span> q)</span>
<span id="cb262-90"><a href="embeddings-y-graph-neural-network.html#cb262-90" tabindex="-1"></a></span>
<span id="cb262-91"><a href="embeddings-y-graph-neural-network.html#cb262-91" tabindex="-1"></a>        norm_const <span class="op">=</span> <span class="bu">sum</span>(unnormalized_probs)</span>
<span id="cb262-92"><a href="embeddings-y-graph-neural-network.html#cb262-92" tabindex="-1"></a>        normalized_probs <span class="op">=</span> [</span>
<span id="cb262-93"><a href="embeddings-y-graph-neural-network.html#cb262-93" tabindex="-1"></a>            <span class="bu">float</span>(u_prob) <span class="op">/</span> norm_const <span class="cf">for</span> u_prob <span class="kw">in</span> unnormalized_probs</span>
<span id="cb262-94"><a href="embeddings-y-graph-neural-network.html#cb262-94" tabindex="-1"></a>        ]</span>
<span id="cb262-95"><a href="embeddings-y-graph-neural-network.html#cb262-95" tabindex="-1"></a></span>
<span id="cb262-96"><a href="embeddings-y-graph-neural-network.html#cb262-96" tabindex="-1"></a>        <span class="cf">return</span> alias_setup(normalized_probs)</span>
<span id="cb262-97"><a href="embeddings-y-graph-neural-network.html#cb262-97" tabindex="-1"></a></span>
<span id="cb262-98"><a href="embeddings-y-graph-neural-network.html#cb262-98" tabindex="-1"></a>    <span class="kw">def</span> preprocess_transition_probs(<span class="va">self</span>):</span>
<span id="cb262-99"><a href="embeddings-y-graph-neural-network.html#cb262-99" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb262-100"><a href="embeddings-y-graph-neural-network.html#cb262-100" tabindex="-1"></a><span class="co">        Preprocessing of transition probabilities for guiding the random walks.</span></span>
<span id="cb262-101"><a href="embeddings-y-graph-neural-network.html#cb262-101" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb262-102"><a href="embeddings-y-graph-neural-network.html#cb262-102" tabindex="-1"></a>        G <span class="op">=</span> <span class="va">self</span>.G</span>
<span id="cb262-103"><a href="embeddings-y-graph-neural-network.html#cb262-103" tabindex="-1"></a>        is_directed <span class="op">=</span> <span class="va">self</span>.is_directed</span>
<span id="cb262-104"><a href="embeddings-y-graph-neural-network.html#cb262-104" tabindex="-1"></a></span>
<span id="cb262-105"><a href="embeddings-y-graph-neural-network.html#cb262-105" tabindex="-1"></a>        alias_nodes <span class="op">=</span> {}</span>
<span id="cb262-106"><a href="embeddings-y-graph-neural-network.html#cb262-106" tabindex="-1"></a>        <span class="cf">for</span> node <span class="kw">in</span> G.nodes():</span>
<span id="cb262-107"><a href="embeddings-y-graph-neural-network.html#cb262-107" tabindex="-1"></a>            unnormalized_probs <span class="op">=</span> [</span>
<span id="cb262-108"><a href="embeddings-y-graph-neural-network.html#cb262-108" tabindex="-1"></a>                G[node][nbr].get(<span class="st">&#39;weight&#39;</span>, <span class="fl">1.0</span>)</span>
<span id="cb262-109"><a href="embeddings-y-graph-neural-network.html#cb262-109" tabindex="-1"></a>                <span class="cf">for</span> nbr <span class="kw">in</span> <span class="bu">sorted</span>(G.neighbors(node))</span>
<span id="cb262-110"><a href="embeddings-y-graph-neural-network.html#cb262-110" tabindex="-1"></a>            ]</span>
<span id="cb262-111"><a href="embeddings-y-graph-neural-network.html#cb262-111" tabindex="-1"></a>            norm_const <span class="op">=</span> <span class="bu">sum</span>(unnormalized_probs)</span>
<span id="cb262-112"><a href="embeddings-y-graph-neural-network.html#cb262-112" tabindex="-1"></a>            normalized_probs <span class="op">=</span> [</span>
<span id="cb262-113"><a href="embeddings-y-graph-neural-network.html#cb262-113" tabindex="-1"></a>                <span class="bu">float</span>(u_prob) <span class="op">/</span> norm_const <span class="cf">for</span> u_prob <span class="kw">in</span> unnormalized_probs</span>
<span id="cb262-114"><a href="embeddings-y-graph-neural-network.html#cb262-114" tabindex="-1"></a>            ]</span>
<span id="cb262-115"><a href="embeddings-y-graph-neural-network.html#cb262-115" tabindex="-1"></a>            alias_nodes[node] <span class="op">=</span> alias_setup(normalized_probs)</span>
<span id="cb262-116"><a href="embeddings-y-graph-neural-network.html#cb262-116" tabindex="-1"></a></span>
<span id="cb262-117"><a href="embeddings-y-graph-neural-network.html#cb262-117" tabindex="-1"></a>        alias_edges <span class="op">=</span> {}</span>
<span id="cb262-118"><a href="embeddings-y-graph-neural-network.html#cb262-118" tabindex="-1"></a></span>
<span id="cb262-119"><a href="embeddings-y-graph-neural-network.html#cb262-119" tabindex="-1"></a>        <span class="cf">if</span> is_directed:</span>
<span id="cb262-120"><a href="embeddings-y-graph-neural-network.html#cb262-120" tabindex="-1"></a>            <span class="cf">for</span> edge <span class="kw">in</span> G.edges():</span>
<span id="cb262-121"><a href="embeddings-y-graph-neural-network.html#cb262-121" tabindex="-1"></a>                alias_edges[edge] <span class="op">=</span> <span class="va">self</span>.get_alias_edge(edge[<span class="dv">0</span>], edge[<span class="dv">1</span>])</span>
<span id="cb262-122"><a href="embeddings-y-graph-neural-network.html#cb262-122" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb262-123"><a href="embeddings-y-graph-neural-network.html#cb262-123" tabindex="-1"></a>            <span class="cf">for</span> edge <span class="kw">in</span> G.edges():</span>
<span id="cb262-124"><a href="embeddings-y-graph-neural-network.html#cb262-124" tabindex="-1"></a>                alias_edges[edge] <span class="op">=</span> <span class="va">self</span>.get_alias_edge(edge[<span class="dv">0</span>], edge[<span class="dv">1</span>])</span>
<span id="cb262-125"><a href="embeddings-y-graph-neural-network.html#cb262-125" tabindex="-1"></a>                alias_edges[(edge[<span class="dv">1</span>], edge[<span class="dv">0</span>])] <span class="op">=</span> <span class="va">self</span>.get_alias_edge(edge[<span class="dv">1</span>], edge[<span class="dv">0</span>])</span>
<span id="cb262-126"><a href="embeddings-y-graph-neural-network.html#cb262-126" tabindex="-1"></a></span>
<span id="cb262-127"><a href="embeddings-y-graph-neural-network.html#cb262-127" tabindex="-1"></a>        <span class="va">self</span>.alias_nodes <span class="op">=</span> alias_nodes</span>
<span id="cb262-128"><a href="embeddings-y-graph-neural-network.html#cb262-128" tabindex="-1"></a>        <span class="va">self</span>.alias_edges <span class="op">=</span> alias_edges</span>
<span id="cb262-129"><a href="embeddings-y-graph-neural-network.html#cb262-129" tabindex="-1"></a></span>
<span id="cb262-130"><a href="embeddings-y-graph-neural-network.html#cb262-130" tabindex="-1"></a></span>
<span id="cb262-131"><a href="embeddings-y-graph-neural-network.html#cb262-131" tabindex="-1"></a><span class="kw">def</span> alias_setup(probs):</span>
<span id="cb262-132"><a href="embeddings-y-graph-neural-network.html#cb262-132" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb262-133"><a href="embeddings-y-graph-neural-network.html#cb262-133" tabindex="-1"></a><span class="co">    Compute utility lists for non-uniform sampling from discrete distributions.</span></span>
<span id="cb262-134"><a href="embeddings-y-graph-neural-network.html#cb262-134" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb262-135"><a href="embeddings-y-graph-neural-network.html#cb262-135" tabindex="-1"></a>    K <span class="op">=</span> <span class="bu">len</span>(probs)</span>
<span id="cb262-136"><a href="embeddings-y-graph-neural-network.html#cb262-136" tabindex="-1"></a>    q <span class="op">=</span> np.zeros(K)</span>
<span id="cb262-137"><a href="embeddings-y-graph-neural-network.html#cb262-137" tabindex="-1"></a>    J <span class="op">=</span> np.zeros(K, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb262-138"><a href="embeddings-y-graph-neural-network.html#cb262-138" tabindex="-1"></a></span>
<span id="cb262-139"><a href="embeddings-y-graph-neural-network.html#cb262-139" tabindex="-1"></a>    smaller <span class="op">=</span> []</span>
<span id="cb262-140"><a href="embeddings-y-graph-neural-network.html#cb262-140" tabindex="-1"></a>    larger <span class="op">=</span> []</span>
<span id="cb262-141"><a href="embeddings-y-graph-neural-network.html#cb262-141" tabindex="-1"></a></span>
<span id="cb262-142"><a href="embeddings-y-graph-neural-network.html#cb262-142" tabindex="-1"></a>    <span class="cf">for</span> kk, prob <span class="kw">in</span> <span class="bu">enumerate</span>(probs):</span>
<span id="cb262-143"><a href="embeddings-y-graph-neural-network.html#cb262-143" tabindex="-1"></a>        q[kk] <span class="op">=</span> K <span class="op">*</span> prob</span>
<span id="cb262-144"><a href="embeddings-y-graph-neural-network.html#cb262-144" tabindex="-1"></a>        <span class="cf">if</span> q[kk] <span class="op">&lt;</span> <span class="fl">1.0</span>:</span>
<span id="cb262-145"><a href="embeddings-y-graph-neural-network.html#cb262-145" tabindex="-1"></a>            smaller.append(kk)</span>
<span id="cb262-146"><a href="embeddings-y-graph-neural-network.html#cb262-146" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb262-147"><a href="embeddings-y-graph-neural-network.html#cb262-147" tabindex="-1"></a>            larger.append(kk)</span>
<span id="cb262-148"><a href="embeddings-y-graph-neural-network.html#cb262-148" tabindex="-1"></a></span>
<span id="cb262-149"><a href="embeddings-y-graph-neural-network.html#cb262-149" tabindex="-1"></a>    <span class="cf">while</span> smaller <span class="kw">and</span> larger:</span>
<span id="cb262-150"><a href="embeddings-y-graph-neural-network.html#cb262-150" tabindex="-1"></a>        small <span class="op">=</span> smaller.pop()</span>
<span id="cb262-151"><a href="embeddings-y-graph-neural-network.html#cb262-151" tabindex="-1"></a>        large <span class="op">=</span> larger.pop()</span>
<span id="cb262-152"><a href="embeddings-y-graph-neural-network.html#cb262-152" tabindex="-1"></a></span>
<span id="cb262-153"><a href="embeddings-y-graph-neural-network.html#cb262-153" tabindex="-1"></a>        J[small] <span class="op">=</span> large</span>
<span id="cb262-154"><a href="embeddings-y-graph-neural-network.html#cb262-154" tabindex="-1"></a>        q[large] <span class="op">=</span> q[large] <span class="op">+</span> q[small] <span class="op">-</span> <span class="fl">1.0</span></span>
<span id="cb262-155"><a href="embeddings-y-graph-neural-network.html#cb262-155" tabindex="-1"></a></span>
<span id="cb262-156"><a href="embeddings-y-graph-neural-network.html#cb262-156" tabindex="-1"></a>        <span class="cf">if</span> q[large] <span class="op">&lt;</span> <span class="fl">1.0</span>:</span>
<span id="cb262-157"><a href="embeddings-y-graph-neural-network.html#cb262-157" tabindex="-1"></a>            smaller.append(large)</span>
<span id="cb262-158"><a href="embeddings-y-graph-neural-network.html#cb262-158" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb262-159"><a href="embeddings-y-graph-neural-network.html#cb262-159" tabindex="-1"></a>            larger.append(large)</span>
<span id="cb262-160"><a href="embeddings-y-graph-neural-network.html#cb262-160" tabindex="-1"></a></span>
<span id="cb262-161"><a href="embeddings-y-graph-neural-network.html#cb262-161" tabindex="-1"></a>    <span class="cf">return</span> J, q</span>
<span id="cb262-162"><a href="embeddings-y-graph-neural-network.html#cb262-162" tabindex="-1"></a></span>
<span id="cb262-163"><a href="embeddings-y-graph-neural-network.html#cb262-163" tabindex="-1"></a></span>
<span id="cb262-164"><a href="embeddings-y-graph-neural-network.html#cb262-164" tabindex="-1"></a><span class="kw">def</span> alias_draw(J, q):</span>
<span id="cb262-165"><a href="embeddings-y-graph-neural-network.html#cb262-165" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb262-166"><a href="embeddings-y-graph-neural-network.html#cb262-166" tabindex="-1"></a><span class="co">    Draw sample from a non-uniform discrete distribution using alias sampling.</span></span>
<span id="cb262-167"><a href="embeddings-y-graph-neural-network.html#cb262-167" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb262-168"><a href="embeddings-y-graph-neural-network.html#cb262-168" tabindex="-1"></a>    K <span class="op">=</span> <span class="bu">len</span>(J)</span>
<span id="cb262-169"><a href="embeddings-y-graph-neural-network.html#cb262-169" tabindex="-1"></a>    kk <span class="op">=</span> <span class="bu">int</span>(np.floor(np.random.rand() <span class="op">*</span> K))</span>
<span id="cb262-170"><a href="embeddings-y-graph-neural-network.html#cb262-170" tabindex="-1"></a></span>
<span id="cb262-171"><a href="embeddings-y-graph-neural-network.html#cb262-171" tabindex="-1"></a>    <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> q[kk]:</span>
<span id="cb262-172"><a href="embeddings-y-graph-neural-network.html#cb262-172" tabindex="-1"></a>        <span class="cf">return</span> kk</span>
<span id="cb262-173"><a href="embeddings-y-graph-neural-network.html#cb262-173" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb262-174"><a href="embeddings-y-graph-neural-network.html#cb262-174" tabindex="-1"></a>        <span class="cf">return</span> J[kk]</span></code></pre></div>
<p>La siguiente función se en</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb263-1"><a href="embeddings-y-graph-neural-network.html#cb263-1" tabindex="-1"></a><span class="kw">def</span> learn_embeddings(</span>
<span id="cb263-2"><a href="embeddings-y-graph-neural-network.html#cb263-2" tabindex="-1"></a>    walks,</span>
<span id="cb263-3"><a href="embeddings-y-graph-neural-network.html#cb263-3" tabindex="-1"></a>    vector_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">128</span>,</span>
<span id="cb263-4"><a href="embeddings-y-graph-neural-network.html#cb263-4" tabindex="-1"></a>    window: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb263-5"><a href="embeddings-y-graph-neural-network.html#cb263-5" tabindex="-1"></a>    workers: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span>,</span>
<span id="cb263-6"><a href="embeddings-y-graph-neural-network.html#cb263-6" tabindex="-1"></a>    epochs: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb263-7"><a href="embeddings-y-graph-neural-network.html#cb263-7" tabindex="-1"></a>):</span>
<span id="cb263-8"><a href="embeddings-y-graph-neural-network.html#cb263-8" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb263-9"><a href="embeddings-y-graph-neural-network.html#cb263-9" tabindex="-1"></a><span class="co">    Learn embeddings by optimizing the Skip-gram objective using SGD.</span></span>
<span id="cb263-10"><a href="embeddings-y-graph-neural-network.html#cb263-10" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb263-11"><a href="embeddings-y-graph-neural-network.html#cb263-11" tabindex="-1"></a></span>
<span id="cb263-12"><a href="embeddings-y-graph-neural-network.html#cb263-12" tabindex="-1"></a>    <span class="co"># gensim requires lists, not iterators</span></span>
<span id="cb263-13"><a href="embeddings-y-graph-neural-network.html#cb263-13" tabindex="-1"></a>    walks <span class="op">=</span> [<span class="bu">list</span>(<span class="bu">map</span>(<span class="bu">str</span>, walk)) <span class="cf">for</span> walk <span class="kw">in</span> walks]</span>
<span id="cb263-14"><a href="embeddings-y-graph-neural-network.html#cb263-14" tabindex="-1"></a></span>
<span id="cb263-15"><a href="embeddings-y-graph-neural-network.html#cb263-15" tabindex="-1"></a>    model <span class="op">=</span> Word2Vec(</span>
<span id="cb263-16"><a href="embeddings-y-graph-neural-network.html#cb263-16" tabindex="-1"></a>        sentences<span class="op">=</span>walks,</span>
<span id="cb263-17"><a href="embeddings-y-graph-neural-network.html#cb263-17" tabindex="-1"></a>        vector_size<span class="op">=</span>vector_size,</span>
<span id="cb263-18"><a href="embeddings-y-graph-neural-network.html#cb263-18" tabindex="-1"></a>        window<span class="op">=</span>window,</span>
<span id="cb263-19"><a href="embeddings-y-graph-neural-network.html#cb263-19" tabindex="-1"></a>        min_count<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb263-20"><a href="embeddings-y-graph-neural-network.html#cb263-20" tabindex="-1"></a>        sg<span class="op">=</span><span class="dv">1</span>,              <span class="co"># skip-gram</span></span>
<span id="cb263-21"><a href="embeddings-y-graph-neural-network.html#cb263-21" tabindex="-1"></a>        workers<span class="op">=</span>workers,</span>
<span id="cb263-22"><a href="embeddings-y-graph-neural-network.html#cb263-22" tabindex="-1"></a>        epochs<span class="op">=</span>epochs</span>
<span id="cb263-23"><a href="embeddings-y-graph-neural-network.html#cb263-23" tabindex="-1"></a>    )</span>
<span id="cb263-24"><a href="embeddings-y-graph-neural-network.html#cb263-24" tabindex="-1"></a></span>
<span id="cb263-25"><a href="embeddings-y-graph-neural-network.html#cb263-25" tabindex="-1"></a>    <span class="co"># save in word2vec format (node2vec-compatible)</span></span>
<span id="cb263-26"><a href="embeddings-y-graph-neural-network.html#cb263-26" tabindex="-1"></a>    model.wv.save_word2vec_format(<span class="st">&quot;embeddings.emb&quot;</span>)</span>
<span id="cb263-27"><a href="embeddings-y-graph-neural-network.html#cb263-27" tabindex="-1"></a></span>
<span id="cb263-28"><a href="embeddings-y-graph-neural-network.html#cb263-28" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
<p>Establecidao lo anterior cargamos los datos que representan
a la gráfica del <em>Karate Club</em>:</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb264-1"><a href="embeddings-y-graph-neural-network.html#cb264-1" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb264-2"><a href="embeddings-y-graph-neural-network.html#cb264-2" tabindex="-1"></a></span>
<span id="cb264-3"><a href="embeddings-y-graph-neural-network.html#cb264-3" tabindex="-1"></a>nx_G <span class="op">=</span> nx.karate_club_graph()</span>
<span id="cb264-4"><a href="embeddings-y-graph-neural-network.html#cb264-4" tabindex="-1"></a>G <span class="op">=</span> Graph(nx_G, is_directed<span class="op">=</span><span class="va">False</span>, p<span class="op">=</span><span class="fl">0.8</span>, q<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb264-5"><a href="embeddings-y-graph-neural-network.html#cb264-5" tabindex="-1"></a></span>
<span id="cb264-6"><a href="embeddings-y-graph-neural-network.html#cb264-6" tabindex="-1"></a>G.preprocess_transition_probs()</span>
<span id="cb264-7"><a href="embeddings-y-graph-neural-network.html#cb264-7" tabindex="-1"></a>walks <span class="op">=</span> G.simulate_walks(num_walks<span class="op">=</span><span class="dv">20</span>, walk_length<span class="op">=</span><span class="dv">10</span>)</span></code></pre></div>
<pre><code>## Walk iteration:
## 1 / 20
## 2 / 20
## 3 / 20
## 4 / 20
## 5 / 20
## 6 / 20
## 7 / 20
## 8 / 20
## 9 / 20
## 10 / 20
## 11 / 20
## 12 / 20
## 13 / 20
## 14 / 20
## 15 / 20
## 16 / 20
## 17 / 20
## 18 / 20
## 19 / 20
## 20 / 20</code></pre>
<p>Ahora podemos revisar como se ven las caminatas:</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb266-1"><a href="embeddings-y-graph-neural-network.html#cb266-1" tabindex="-1"></a>walks[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [8, 32, 31, 28, 2, 13, 33, 8, 0, 5]</code></pre>
<p>A partir de este punto calculamos los encajes de los nodos</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb268-1"><a href="embeddings-y-graph-neural-network.html#cb268-1" tabindex="-1"></a>model <span class="op">=</span> learn_embeddings(walks,epochs<span class="op">=</span><span class="dv">100</span>)</span></code></pre></div>
<p><strong>Nota:</strong> Este encaje solo usa las caminatas, no involucra features
numéricas</p>
<p>Ahora mostraremos como se ve el encaje recién generado usado PCA:</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb269-1"><a href="embeddings-y-graph-neural-network.html#cb269-1" tabindex="-1"></a><span class="kw">def</span> load_embeddings(path<span class="op">=</span><span class="st">&quot;embeddings.emb&quot;</span>):</span>
<span id="cb269-2"><a href="embeddings-y-graph-neural-network.html#cb269-2" tabindex="-1"></a>    kv <span class="op">=</span> KeyedVectors.load_word2vec_format(path)</span>
<span id="cb269-3"><a href="embeddings-y-graph-neural-network.html#cb269-3" tabindex="-1"></a>    nodes <span class="op">=</span> kv.index_to_key</span>
<span id="cb269-4"><a href="embeddings-y-graph-neural-network.html#cb269-4" tabindex="-1"></a>    X <span class="op">=</span> np.array([kv[node] <span class="cf">for</span> node <span class="kw">in</span> nodes])</span>
<span id="cb269-5"><a href="embeddings-y-graph-neural-network.html#cb269-5" tabindex="-1"></a>    <span class="cf">return</span> nodes, X</span>
<span id="cb269-6"><a href="embeddings-y-graph-neural-network.html#cb269-6" tabindex="-1"></a></span>
<span id="cb269-7"><a href="embeddings-y-graph-neural-network.html#cb269-7" tabindex="-1"></a><span class="kw">def</span> plot_embeddings_pca(nodes, X, graph):</span>
<span id="cb269-8"><a href="embeddings-y-graph-neural-network.html#cb269-8" tabindex="-1"></a>    pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb269-9"><a href="embeddings-y-graph-neural-network.html#cb269-9" tabindex="-1"></a>    X_2d <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb269-10"><a href="embeddings-y-graph-neural-network.html#cb269-10" tabindex="-1"></a></span>
<span id="cb269-11"><a href="embeddings-y-graph-neural-network.html#cb269-11" tabindex="-1"></a>    club_to_color <span class="op">=</span> {</span>
<span id="cb269-12"><a href="embeddings-y-graph-neural-network.html#cb269-12" tabindex="-1"></a>        <span class="st">&quot;Mr. Hi&quot;</span>: <span class="st">&quot;tab:blue&quot;</span>,</span>
<span id="cb269-13"><a href="embeddings-y-graph-neural-network.html#cb269-13" tabindex="-1"></a>        <span class="st">&quot;Officer&quot;</span>: <span class="st">&quot;tab:orange&quot;</span></span>
<span id="cb269-14"><a href="embeddings-y-graph-neural-network.html#cb269-14" tabindex="-1"></a>    }</span>
<span id="cb269-15"><a href="embeddings-y-graph-neural-network.html#cb269-15" tabindex="-1"></a></span>
<span id="cb269-16"><a href="embeddings-y-graph-neural-network.html#cb269-16" tabindex="-1"></a>    colors <span class="op">=</span> [</span>
<span id="cb269-17"><a href="embeddings-y-graph-neural-network.html#cb269-17" tabindex="-1"></a>        club_to_color[graph.nodes[<span class="bu">int</span>(node)][<span class="st">&quot;club&quot;</span>]]</span>
<span id="cb269-18"><a href="embeddings-y-graph-neural-network.html#cb269-18" tabindex="-1"></a>        <span class="cf">for</span> node <span class="kw">in</span> nodes</span>
<span id="cb269-19"><a href="embeddings-y-graph-neural-network.html#cb269-19" tabindex="-1"></a>    ]</span>
<span id="cb269-20"><a href="embeddings-y-graph-neural-network.html#cb269-20" tabindex="-1"></a></span>
<span id="cb269-21"><a href="embeddings-y-graph-neural-network.html#cb269-21" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">10</span>))</span>
<span id="cb269-22"><a href="embeddings-y-graph-neural-network.html#cb269-22" tabindex="-1"></a>    plt.scatter(X_2d[:, <span class="dv">0</span>], X_2d[:, <span class="dv">1</span>], c<span class="op">=</span>colors, s<span class="op">=</span><span class="dv">80</span>, alpha<span class="op">=</span><span class="fl">0.85</span>)</span>
<span id="cb269-23"><a href="embeddings-y-graph-neural-network.html#cb269-23" tabindex="-1"></a></span>
<span id="cb269-24"><a href="embeddings-y-graph-neural-network.html#cb269-24" tabindex="-1"></a>    <span class="cf">for</span> i, node <span class="kw">in</span> <span class="bu">enumerate</span>(nodes):</span>
<span id="cb269-25"><a href="embeddings-y-graph-neural-network.html#cb269-25" tabindex="-1"></a>        plt.text(</span>
<span id="cb269-26"><a href="embeddings-y-graph-neural-network.html#cb269-26" tabindex="-1"></a>            X_2d[i, <span class="dv">0</span>] <span class="op">+</span> <span class="fl">0.01</span>,</span>
<span id="cb269-27"><a href="embeddings-y-graph-neural-network.html#cb269-27" tabindex="-1"></a>            X_2d[i, <span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.01</span>,</span>
<span id="cb269-28"><a href="embeddings-y-graph-neural-network.html#cb269-28" tabindex="-1"></a>            node,</span>
<span id="cb269-29"><a href="embeddings-y-graph-neural-network.html#cb269-29" tabindex="-1"></a>            fontsize<span class="op">=</span><span class="dv">9</span></span>
<span id="cb269-30"><a href="embeddings-y-graph-neural-network.html#cb269-30" tabindex="-1"></a>        )</span>
<span id="cb269-31"><a href="embeddings-y-graph-neural-network.html#cb269-31" tabindex="-1"></a></span>
<span id="cb269-32"><a href="embeddings-y-graph-neural-network.html#cb269-32" tabindex="-1"></a>    <span class="cf">for</span> club, color <span class="kw">in</span> club_to_color.items():</span>
<span id="cb269-33"><a href="embeddings-y-graph-neural-network.html#cb269-33" tabindex="-1"></a>        plt.scatter([], [], c<span class="op">=</span>color, label<span class="op">=</span>club)</span>
<span id="cb269-34"><a href="embeddings-y-graph-neural-network.html#cb269-34" tabindex="-1"></a></span>
<span id="cb269-35"><a href="embeddings-y-graph-neural-network.html#cb269-35" tabindex="-1"></a>    plt.legend(title<span class="op">=</span><span class="st">&quot;Karate Club Group&quot;</span>)</span>
<span id="cb269-36"><a href="embeddings-y-graph-neural-network.html#cb269-36" tabindex="-1"></a>    plt.title(<span class="st">&quot;Node2Vec Embeddings (PCA)&quot;</span>)</span>
<span id="cb269-37"><a href="embeddings-y-graph-neural-network.html#cb269-37" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;PC1&quot;</span>)</span>
<span id="cb269-38"><a href="embeddings-y-graph-neural-network.html#cb269-38" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;PC2&quot;</span>)</span>
<span id="cb269-39"><a href="embeddings-y-graph-neural-network.html#cb269-39" tabindex="-1"></a>    plt.grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb269-40"><a href="embeddings-y-graph-neural-network.html#cb269-40" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb269-41"><a href="embeddings-y-graph-neural-network.html#cb269-41" tabindex="-1"></a>    plt.show()</span>
<span id="cb269-42"><a href="embeddings-y-graph-neural-network.html#cb269-42" tabindex="-1"></a></span>
<span id="cb269-43"><a href="embeddings-y-graph-neural-network.html#cb269-43" tabindex="-1"></a></span>
<span id="cb269-44"><a href="embeddings-y-graph-neural-network.html#cb269-44" tabindex="-1"></a><span class="co"># usage</span></span>
<span id="cb269-45"><a href="embeddings-y-graph-neural-network.html#cb269-45" tabindex="-1"></a>G <span class="op">=</span> nx.karate_club_graph()</span>
<span id="cb269-46"><a href="embeddings-y-graph-neural-network.html#cb269-46" tabindex="-1"></a>nodes, X <span class="op">=</span> load_embeddings(<span class="st">&quot;embeddings.emb&quot;</span>)</span>
<span id="cb269-47"><a href="embeddings-y-graph-neural-network.html#cb269-47" tabindex="-1"></a>plot_embeddings_pca(nodes, X, G)</span></code></pre></div>
<p><img src="deep_learning_course_python_files/figure-html/unnamed-chunk-178-1.png" alt="" width="1152" /></p>
<p>A modo de comparativa, presentamos el gráfico del <em>Karate Club</em>.
Se aprecia la cercania de los nodos y el encaje de arriba.</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb270-1"><a href="embeddings-y-graph-neural-network.html#cb270-1" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb270-2"><a href="embeddings-y-graph-neural-network.html#cb270-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb270-3"><a href="embeddings-y-graph-neural-network.html#cb270-3" tabindex="-1"></a></span>
<span id="cb270-4"><a href="embeddings-y-graph-neural-network.html#cb270-4" tabindex="-1"></a><span class="co"># graph</span></span>
<span id="cb270-5"><a href="embeddings-y-graph-neural-network.html#cb270-5" tabindex="-1"></a>G <span class="op">=</span> nx.karate_club_graph()</span>
<span id="cb270-6"><a href="embeddings-y-graph-neural-network.html#cb270-6" tabindex="-1"></a></span>
<span id="cb270-7"><a href="embeddings-y-graph-neural-network.html#cb270-7" tabindex="-1"></a><span class="co"># color by club</span></span>
<span id="cb270-8"><a href="embeddings-y-graph-neural-network.html#cb270-8" tabindex="-1"></a>club_to_color <span class="op">=</span> {</span>
<span id="cb270-9"><a href="embeddings-y-graph-neural-network.html#cb270-9" tabindex="-1"></a>    <span class="st">&quot;Mr. Hi&quot;</span>: <span class="st">&quot;tab:blue&quot;</span>,</span>
<span id="cb270-10"><a href="embeddings-y-graph-neural-network.html#cb270-10" tabindex="-1"></a>    <span class="st">&quot;Officer&quot;</span>: <span class="st">&quot;tab:orange&quot;</span></span>
<span id="cb270-11"><a href="embeddings-y-graph-neural-network.html#cb270-11" tabindex="-1"></a>}</span>
<span id="cb270-12"><a href="embeddings-y-graph-neural-network.html#cb270-12" tabindex="-1"></a></span>
<span id="cb270-13"><a href="embeddings-y-graph-neural-network.html#cb270-13" tabindex="-1"></a>node_colors <span class="op">=</span> [</span>
<span id="cb270-14"><a href="embeddings-y-graph-neural-network.html#cb270-14" tabindex="-1"></a>    club_to_color[G.nodes[n][<span class="st">&quot;club&quot;</span>]]</span>
<span id="cb270-15"><a href="embeddings-y-graph-neural-network.html#cb270-15" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> G.nodes()</span>
<span id="cb270-16"><a href="embeddings-y-graph-neural-network.html#cb270-16" tabindex="-1"></a>]</span>
<span id="cb270-17"><a href="embeddings-y-graph-neural-network.html#cb270-17" tabindex="-1"></a></span>
<span id="cb270-18"><a href="embeddings-y-graph-neural-network.html#cb270-18" tabindex="-1"></a><span class="co"># layout</span></span>
<span id="cb270-19"><a href="embeddings-y-graph-neural-network.html#cb270-19" tabindex="-1"></a>pos <span class="op">=</span> nx.spring_layout(G, seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb270-20"><a href="embeddings-y-graph-neural-network.html#cb270-20" tabindex="-1"></a></span>
<span id="cb270-21"><a href="embeddings-y-graph-neural-network.html#cb270-21" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb270-22"><a href="embeddings-y-graph-neural-network.html#cb270-22" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">10</span>))</span></code></pre></div>
<pre><code>## &lt;Figure size 1200x1000 with 0 Axes&gt;</code></pre>
<div class="sourceCode" id="cb272"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb272-1"><a href="embeddings-y-graph-neural-network.html#cb272-1" tabindex="-1"></a>nx.draw_networkx_nodes(</span>
<span id="cb272-2"><a href="embeddings-y-graph-neural-network.html#cb272-2" tabindex="-1"></a>    G,</span>
<span id="cb272-3"><a href="embeddings-y-graph-neural-network.html#cb272-3" tabindex="-1"></a>    pos,</span>
<span id="cb272-4"><a href="embeddings-y-graph-neural-network.html#cb272-4" tabindex="-1"></a>    node_color<span class="op">=</span>node_colors,</span>
<span id="cb272-5"><a href="embeddings-y-graph-neural-network.html#cb272-5" tabindex="-1"></a>    node_size<span class="op">=</span><span class="dv">600</span>,</span>
<span id="cb272-6"><a href="embeddings-y-graph-neural-network.html#cb272-6" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.9</span></span>
<span id="cb272-7"><a href="embeddings-y-graph-neural-network.html#cb272-7" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## &lt;matplotlib.collections.PathCollection object at 0x7e625695db10&gt;</code></pre>
<div class="sourceCode" id="cb274"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb274-1"><a href="embeddings-y-graph-neural-network.html#cb274-1" tabindex="-1"></a>nx.draw_networkx_edges(G, pos, alpha<span class="op">=</span><span class="fl">0.4</span>)</span></code></pre></div>
<pre><code>## &lt;matplotlib.collections.LineCollection object at 0x7e6265d6fc10&gt;</code></pre>
<div class="sourceCode" id="cb276"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb276-1"><a href="embeddings-y-graph-neural-network.html#cb276-1" tabindex="-1"></a>nx.draw_networkx_labels(G, pos, font_size<span class="op">=</span><span class="dv">9</span>)</span></code></pre></div>
<pre><code>## {0: Text(-0.1637610156025021, 0.32964542107317624, &#39;0&#39;), 1: Text(-0.22110656624798736, 0.15465314513942682, &#39;1&#39;), 2: Text(-0.09674034041415447, -0.0037573560658175995, &#39;2&#39;), 3: Text(-0.3591582159625228, 0.1728032018032186, &#39;3&#39;), 4: Text(-0.07229970678169054, 0.6118299837698934, &#39;4&#39;), 5: Text(-0.09717721706357874, 0.751932396613453, &#39;5&#39;), 6: Text(-0.1947784244080323, 0.7423079030021026, &#39;6&#39;), 7: Text(-0.3251725240524693, 0.0742949529007427, &#39;7&#39;), 8: Text(-0.0011697920357091234, -0.10817287772994941, &#39;8&#39;), 9: Text(0.004290475810724824, -0.5530221298693542, &#39;9&#39;), 10: Text(0.042837332459480335, 0.6887719750394118, &#39;10&#39;), 11: Text(-0.47044853515423185, 0.5798729526476855, &#39;11&#39;), 12: Text(-0.6315773706598862, 0.24946302282839056, &#39;12&#39;), 13: Text(-0.15233910119383867, 0.041140927956493364, &#39;13&#39;), 14: Text(0.30552515064095687, -0.5971952387445756, &#39;14&#39;), 15: Text(-0.028298909250223627, -0.4732965857414292, &#39;15&#39;), 16: Text(-0.14049188351342506, 1.0, &#39;16&#39;), 17: Text(-0.34898558904265237, 0.5328400662671233, &#39;17&#39;), 18: Text(0.2857217216627078, -0.729561070874141, &#39;18&#39;), 19: Text(0.03928628689562421, 0.20684603040975344, &#39;19&#39;), 20: Text(0.11506086536418461, -0.7241738778220087, &#39;20&#39;), 21: Text(-0.46423812660329555, 0.35865653156837984, &#39;21&#39;), 22: Text(-0.07550970574233766, -0.6032909516752911, &#39;22&#39;), 23: Text(0.34042142719841684, -0.3121094950798013, &#39;23&#39;), 24: Text(0.581428589750548, -0.10159916409318481, &#39;24&#39;), 25: Text(0.44187123403994266, -0.12852017387518871, &#39;25&#39;), 26: Text(0.5410026608885361, -0.31543446975966133, &#39;26&#39;), 27: Text(0.31286553761978536, -0.1580322192793705, &#39;27&#39;), 28: Text(0.025224740721732208, -0.21020612426204355, &#39;28&#39;), 29: Text(0.40722416495399316, -0.4430701164729471, &#39;29&#39;), 30: Text(-0.10091807492123406, -0.23595984038361023, &#39;30&#39;), 31: Text(0.22244687101605135, -0.09337813605721001, &#39;31&#39;), 32: Text(0.14500092919216684, -0.3868283900546997, &#39;32&#39;), 33: Text(0.1339631104349227, -0.3174502931789706, &#39;33&#39;)}</code></pre>
<p><img src="deep_learning_course_python_files/figure-html/unnamed-chunk-179-3.png" alt="" width="1152" /></p>
</div>
</div>
<div id="sageconv" class="section level2 hasAnchor" number="15.4">
<h2><span class="header-section-number">15.4</span> SageConv<a href="embeddings-y-graph-neural-network.html#sageconv" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La mayoría de los modelos anteriores (como GCN o node2vec) asumen que el gráfica
es estático y que todos los nodos están presentes durante el entrenamiento. Esto
se conoce como aprendizaje transductivo.</p>
<p>Sin embargo, en aplicaciones prácticas, como en redes de sociales, la gráfica
cambia dinámicamente en periodos muy cortos. Por ejemplo, al añadir un usuario
nuevo, los modelos transductivos no podrían llevar a cabo predicciones; tendríamos
que reentrenar todo el modelo desde cero para generar un embedding para ese nuevo nodo.</p>
<p>Este es un problema que es atacado por el modelo <strong>GraphSAGE (SAGE viene de SAmple and aggreGatE)</strong>
el cual tiene un diseñado inductivo, en cual tiene un mecanismo de agregación
de información que le permite generar representaciones para nodos que nunca
vio durante el entrenamiento.</p>
<p>Este modelo fue introducido en 2018 por un (equipo de Standford)[<a href="https://arxiv.org/pdf/1706.02216" class="uri">https://arxiv.org/pdf/1706.02216</a>].
A diferencia de las GCN que utilizan la matriz de adyacencia completa, tal
opera mediante un proceso de muestreo de vecindad.</p>
<p><img src="img/21-gnn/sage_diagram.png" alt="" width="660" style="display: block; margin: auto;" /></p>
<p>Los pasos del modelo se resumen a continuación:</p>
<ul>
<li><p><strong>A) Sampling:</strong> En un primer paso, para cada nodo se genera una caminata aleatoria de nodos, con una longitud pre-fijada.</p></li>
<li><p><strong>B) Aggregation:</strong> Dichas caminatas, sirven para “resumir” la información de nodos que co-ocurren en la estructura de vecindades que exploraron en la gráfica. Dicha información se resumen mediante funciones de agregregacion <span class="math inline">\(AGGREGATE_k\)</span>, como se presenta a continuación:</p></li>
</ul>
<p><span class="math display">\[\begin{equation}
h_{\mathcal{N}(i)}^{(k)} = \text{AGGREGATE}_k \left( \{ h_j^{(k-1)}, \forall j \in \mathcal{N}(i) \} \right)
\end{equation}\]</span></p>
<p>En tal expresión, los términos involucrados se refieren a:</p>
<ul>
<li><span class="math inline">\(h_{\mathcal{N}(i)}^{(k)}\)</span>: Representación agregada de los vecinos del nodo <span class="math inline">\(i\)</span> en la capa <span class="math inline">\(k\)</span>,
*<span class="math inline">\(\text{AGGREGATE}_k\)</span>: Función de agregación de información de los nodos,</li>
<li><span class="math inline">\(\text{concat}(\cdot, \cdot)\)</span>: Operación de concatenación que preserva la identidad del nodo central diferenciándola de su contexto,</li>
<li><span class="math inline">\(\mathbf{W}^{(k)}\)</span>: Matriz de pesos aprendible de la capa <span class="math inline">\(k\)</span>,</li>
<li><span class="math inline">\(\sigma\)</span>: Función de activación no lineal.</li>
</ul>
<p>Cabe destacar que en el artículo original se exploran las siguientes opciones de agregación:</p>
<ul>
<li><p><em>Mean aggregator</em>: Promedio de los vectores,</p></li>
<li><p><em>LSTM aggregator</em>: Estimación con una red LSTM,</p></li>
<li><p><em>Pooling aggregator</em>: Aplica una red densa seguida de un operador de máximo (<span class="math inline">\(Max( \cdot, 0)\)</span>).</p></li>
<li><p><strong>C) Combinación y actualización:</strong> Posteriormente, se concatena la información agregada con la representación actual del nodo y se proyecta:</p></li>
</ul>
<p><span class="math display">\[\begin{equation}
h_i^{(k)} = \sigma \left( \mathbf{W}^{(k)} \cdot \text{concat}(h_i^{(k-1)}, h_{\mathcal{N}(i)}^{(k)}) \right)
\end{equation}\]</span></p>
<p>Es dable mencionar que, este modelo al incorporarse un nuevo nodo, las nuevas estimaciones se hacen
considerendo la agregación de información de los nodos vecinos. Es decir, elimina la dependencia de las predicciones usando la gráfica completa, por estrategia de agregación de los nodos en una vecindad, donde el proceso de cómo viaja la información queda representado por las funciones de agregación y el concatenado de la misma.</p>
<p>Para mejor referencia, se presenta la descripción original del algoritmo en el paper original</p>
<p><img src="img/21-gnn/sage_algorithm.png" alt="" width="685" style="display: block; margin: auto;" /></p>
<div id="ejemplo-clasificación-de-artículos-de-investigación-por-categoría-parte-ii-1" class="section level3 hasAnchor" number="15.4.1">
<h3><span class="header-section-number">15.4.1</span> Ejemplo: Clasificación de artículos de investigación por categoría (Parte II)<a href="embeddings-y-graph-neural-network.html#ejemplo-clasificaci%C3%B3n-de-art%C3%ADculos-de-investigaci%C3%B3n-por-categor%C3%ADa-parte-ii-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La arquitecura del ejemplo es la siguiente, donde usaremos un encaje denso
usando una Graph Convolution Network:</p>
<p><img src="img/21-gnn/link_prediction_example.png" alt="" width="120%" style="display: block; margin: auto;" /></p>
<p>Leemos la data de</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb278-1"><a href="embeddings-y-graph-neural-network.html#cb278-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score</span>
<span id="cb278-2"><a href="embeddings-y-graph-neural-network.html#cb278-2" tabindex="-1"></a></span>
<span id="cb278-3"><a href="embeddings-y-graph-neural-network.html#cb278-3" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb278-4"><a href="embeddings-y-graph-neural-network.html#cb278-4" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb278-5"><a href="embeddings-y-graph-neural-network.html#cb278-5" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb278-6"><a href="embeddings-y-graph-neural-network.html#cb278-6" tabindex="-1"></a><span class="im">import</span> torch_geometric.transforms <span class="im">as</span> T</span>
<span id="cb278-7"><a href="embeddings-y-graph-neural-network.html#cb278-7" tabindex="-1"></a><span class="im">from</span> torch_geometric.datasets <span class="im">import</span> Planetoid</span>
<span id="cb278-8"><a href="embeddings-y-graph-neural-network.html#cb278-8" tabindex="-1"></a><span class="im">from</span> torch_geometric.nn <span class="im">import</span> GCNConv</span>
<span id="cb278-9"><a href="embeddings-y-graph-neural-network.html#cb278-9" tabindex="-1"></a><span class="im">from</span> torch_geometric.utils <span class="im">import</span> negative_sampling</span>
<span id="cb278-10"><a href="embeddings-y-graph-neural-network.html#cb278-10" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score</span>
<span id="cb278-11"><a href="embeddings-y-graph-neural-network.html#cb278-11" tabindex="-1"></a></span>
<span id="cb278-12"><a href="embeddings-y-graph-neural-network.html#cb278-12" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb278-13"><a href="embeddings-y-graph-neural-network.html#cb278-13" tabindex="-1"></a></span>
<span id="cb278-14"><a href="embeddings-y-graph-neural-network.html#cb278-14" tabindex="-1"></a>transform <span class="op">=</span> T.Compose([</span>
<span id="cb278-15"><a href="embeddings-y-graph-neural-network.html#cb278-15" tabindex="-1"></a>    T.NormalizeFeatures(),</span>
<span id="cb278-16"><a href="embeddings-y-graph-neural-network.html#cb278-16" tabindex="-1"></a>    T.ToDevice(device),</span>
<span id="cb278-17"><a href="embeddings-y-graph-neural-network.html#cb278-17" tabindex="-1"></a>    T.RandomLinkSplit(</span>
<span id="cb278-18"><a href="embeddings-y-graph-neural-network.html#cb278-18" tabindex="-1"></a>      num_val<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb278-19"><a href="embeddings-y-graph-neural-network.html#cb278-19" tabindex="-1"></a>      num_test<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb278-20"><a href="embeddings-y-graph-neural-network.html#cb278-20" tabindex="-1"></a>      is_undirected<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb278-21"><a href="embeddings-y-graph-neural-network.html#cb278-21" tabindex="-1"></a>      add_negative_train_samples<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb278-22"><a href="embeddings-y-graph-neural-network.html#cb278-22" tabindex="-1"></a>])</span>
<span id="cb278-23"><a href="embeddings-y-graph-neural-network.html#cb278-23" tabindex="-1"></a></span>
<span id="cb278-24"><a href="embeddings-y-graph-neural-network.html#cb278-24" tabindex="-1"></a>dataset <span class="op">=</span> Planetoid(<span class="st">&#39;./data/&#39;</span>, name<span class="op">=</span><span class="st">&#39;Cora&#39;</span>, transform<span class="op">=</span>transform)</span>
<span id="cb278-25"><a href="embeddings-y-graph-neural-network.html#cb278-25" tabindex="-1"></a></span>
<span id="cb278-26"><a href="embeddings-y-graph-neural-network.html#cb278-26" tabindex="-1"></a><span class="co"># After applying the `RandomLinkSplit` transform, the data is transformed from</span></span>
<span id="cb278-27"><a href="embeddings-y-graph-neural-network.html#cb278-27" tabindex="-1"></a><span class="co"># a data object to a list of tuples (train_data, val_data, test_data), with</span></span>
<span id="cb278-28"><a href="embeddings-y-graph-neural-network.html#cb278-28" tabindex="-1"></a><span class="co"># each element representing the corresponding split.</span></span>
<span id="cb278-29"><a href="embeddings-y-graph-neural-network.html#cb278-29" tabindex="-1"></a>train_data, val_data, test_data <span class="op">=</span> dataset[<span class="dv">0</span>]</span></code></pre></div>
<p>Este es el modelo que emplearemos:</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb279-1"><a href="embeddings-y-graph-neural-network.html#cb279-1" tabindex="-1"></a><span class="kw">class</span> GCNLinkPredictor(nn.Module):</span>
<span id="cb279-2"><a href="embeddings-y-graph-neural-network.html#cb279-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim_in, dim_h, dim_z):</span>
<span id="cb279-3"><a href="embeddings-y-graph-neural-network.html#cb279-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb279-4"><a href="embeddings-y-graph-neural-network.html#cb279-4" tabindex="-1"></a>        torch.manual_seed(<span class="dv">1234567</span>)</span>
<span id="cb279-5"><a href="embeddings-y-graph-neural-network.html#cb279-5" tabindex="-1"></a></span>
<span id="cb279-6"><a href="embeddings-y-graph-neural-network.html#cb279-6" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> GCNConv(dim_in, dim_h)</span>
<span id="cb279-7"><a href="embeddings-y-graph-neural-network.html#cb279-7" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> GCNConv(dim_h, dim_z)</span>
<span id="cb279-8"><a href="embeddings-y-graph-neural-network.html#cb279-8" tabindex="-1"></a></span>
<span id="cb279-9"><a href="embeddings-y-graph-neural-network.html#cb279-9" tabindex="-1"></a>        <span class="va">self</span>.criterion <span class="op">=</span> nn.BCEWithLogitsLoss()</span>
<span id="cb279-10"><a href="embeddings-y-graph-neural-network.html#cb279-10" tabindex="-1"></a></span>
<span id="cb279-11"><a href="embeddings-y-graph-neural-network.html#cb279-11" tabindex="-1"></a>    <span class="co"># -------------------------------------------------</span></span>
<span id="cb279-12"><a href="embeddings-y-graph-neural-network.html#cb279-12" tabindex="-1"></a>    <span class="co"># Encoder</span></span>
<span id="cb279-13"><a href="embeddings-y-graph-neural-network.html#cb279-13" tabindex="-1"></a>    <span class="co"># -------------------------------------------------</span></span>
<span id="cb279-14"><a href="embeddings-y-graph-neural-network.html#cb279-14" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, x, edge_index):</span>
<span id="cb279-15"><a href="embeddings-y-graph-neural-network.html#cb279-15" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.conv1(x, edge_index)</span>
<span id="cb279-16"><a href="embeddings-y-graph-neural-network.html#cb279-16" tabindex="-1"></a>        h <span class="op">=</span> F.relu(h)</span>
<span id="cb279-17"><a href="embeddings-y-graph-neural-network.html#cb279-17" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.conv2(h, edge_index)</span>
<span id="cb279-18"><a href="embeddings-y-graph-neural-network.html#cb279-18" tabindex="-1"></a>        <span class="cf">return</span> h</span>
<span id="cb279-19"><a href="embeddings-y-graph-neural-network.html#cb279-19" tabindex="-1"></a></span>
<span id="cb279-20"><a href="embeddings-y-graph-neural-network.html#cb279-20" tabindex="-1"></a>    <span class="co"># -------------------------------------------------</span></span>
<span id="cb279-21"><a href="embeddings-y-graph-neural-network.html#cb279-21" tabindex="-1"></a>    <span class="co"># Decoder (dot product)</span></span>
<span id="cb279-22"><a href="embeddings-y-graph-neural-network.html#cb279-22" tabindex="-1"></a>    <span class="co"># -------------------------------------------------</span></span>
<span id="cb279-23"><a href="embeddings-y-graph-neural-network.html#cb279-23" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, z, edge_index):</span>
<span id="cb279-24"><a href="embeddings-y-graph-neural-network.html#cb279-24" tabindex="-1"></a>        <span class="cf">return</span> (z[edge_index[<span class="dv">0</span>]] <span class="op">*</span> z[edge_index[<span class="dv">1</span>]]).<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb279-25"><a href="embeddings-y-graph-neural-network.html#cb279-25" tabindex="-1"></a></span>
<span id="cb279-26"><a href="embeddings-y-graph-neural-network.html#cb279-26" tabindex="-1"></a>    <span class="kw">def</span> decode_all(<span class="va">self</span>, z):</span>
<span id="cb279-27"><a href="embeddings-y-graph-neural-network.html#cb279-27" tabindex="-1"></a>        adj <span class="op">=</span> z <span class="op">@</span> z.t()</span>
<span id="cb279-28"><a href="embeddings-y-graph-neural-network.html#cb279-28" tabindex="-1"></a>        <span class="cf">return</span> (adj <span class="op">&gt;</span> <span class="dv">0</span>).nonzero(as_tuple<span class="op">=</span><span class="va">False</span>).t()</span>
<span id="cb279-29"><a href="embeddings-y-graph-neural-network.html#cb279-29" tabindex="-1"></a></span>
<span id="cb279-30"><a href="embeddings-y-graph-neural-network.html#cb279-30" tabindex="-1"></a>    <span class="co"># -------------------------------------------------</span></span>
<span id="cb279-31"><a href="embeddings-y-graph-neural-network.html#cb279-31" tabindex="-1"></a>    <span class="co"># Training</span></span>
<span id="cb279-32"><a href="embeddings-y-graph-neural-network.html#cb279-32" tabindex="-1"></a>    <span class="co"># -------------------------------------------------</span></span>
<span id="cb279-33"><a href="embeddings-y-graph-neural-network.html#cb279-33" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, train_data, val_data, test_data, epochs, lr<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb279-34"><a href="embeddings-y-graph-neural-network.html#cb279-34" tabindex="-1"></a>        optimizer <span class="op">=</span> torch.optim.Adam(<span class="va">self</span>.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb279-35"><a href="embeddings-y-graph-neural-network.html#cb279-35" tabindex="-1"></a></span>
<span id="cb279-36"><a href="embeddings-y-graph-neural-network.html#cb279-36" tabindex="-1"></a>        best_val_auc <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb279-37"><a href="embeddings-y-graph-neural-network.html#cb279-37" tabindex="-1"></a>        best_test_auc <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb279-38"><a href="embeddings-y-graph-neural-network.html#cb279-38" tabindex="-1"></a></span>
<span id="cb279-39"><a href="embeddings-y-graph-neural-network.html#cb279-39" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb279-40"><a href="embeddings-y-graph-neural-network.html#cb279-40" tabindex="-1"></a>            loss <span class="op">=</span> <span class="va">self</span>._train_epoch(train_data, optimizer)</span>
<span id="cb279-41"><a href="embeddings-y-graph-neural-network.html#cb279-41" tabindex="-1"></a>            val_auc <span class="op">=</span> <span class="va">self</span>.validate(val_data)</span>
<span id="cb279-42"><a href="embeddings-y-graph-neural-network.html#cb279-42" tabindex="-1"></a>            test_auc <span class="op">=</span> <span class="va">self</span>.test(test_data)</span>
<span id="cb279-43"><a href="embeddings-y-graph-neural-network.html#cb279-43" tabindex="-1"></a></span>
<span id="cb279-44"><a href="embeddings-y-graph-neural-network.html#cb279-44" tabindex="-1"></a>            <span class="cf">if</span> val_auc <span class="op">&gt;</span> best_val_auc:</span>
<span id="cb279-45"><a href="embeddings-y-graph-neural-network.html#cb279-45" tabindex="-1"></a>                best_val_auc <span class="op">=</span> val_auc</span>
<span id="cb279-46"><a href="embeddings-y-graph-neural-network.html#cb279-46" tabindex="-1"></a>                best_test_auc <span class="op">=</span> test_auc</span>
<span id="cb279-47"><a href="embeddings-y-graph-neural-network.html#cb279-47" tabindex="-1"></a>                </span>
<span id="cb279-48"><a href="embeddings-y-graph-neural-network.html#cb279-48" tabindex="-1"></a>                </span>
<span id="cb279-49"><a href="embeddings-y-graph-neural-network.html#cb279-49" tabindex="-1"></a>            <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">20</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb279-50"><a href="embeddings-y-graph-neural-network.html#cb279-50" tabindex="-1"></a>              <span class="bu">print</span>(</span>
<span id="cb279-51"><a href="embeddings-y-graph-neural-network.html#cb279-51" tabindex="-1"></a>                  <span class="ss">f&quot;Epoch </span><span class="sc">{</span>epoch<span class="sc">:03d}</span><span class="ss"> | &quot;</span></span>
<span id="cb279-52"><a href="embeddings-y-graph-neural-network.html#cb279-52" tabindex="-1"></a>                  <span class="ss">f&quot;Loss: </span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss"> | &quot;</span></span>
<span id="cb279-53"><a href="embeddings-y-graph-neural-network.html#cb279-53" tabindex="-1"></a>                  <span class="ss">f&quot;Val AUC: </span><span class="sc">{</span>val_auc<span class="sc">:.4f}</span><span class="ss"> | &quot;</span></span>
<span id="cb279-54"><a href="embeddings-y-graph-neural-network.html#cb279-54" tabindex="-1"></a>                  <span class="ss">f&quot;Test AUC: </span><span class="sc">{</span>test_auc<span class="sc">:.4f}</span><span class="ss">&quot;</span></span>
<span id="cb279-55"><a href="embeddings-y-graph-neural-network.html#cb279-55" tabindex="-1"></a>              )</span>
<span id="cb279-56"><a href="embeddings-y-graph-neural-network.html#cb279-56" tabindex="-1"></a></span>
<span id="cb279-57"><a href="embeddings-y-graph-neural-network.html#cb279-57" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Final Test AUC: </span><span class="sc">{</span>best_test_auc<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb279-58"><a href="embeddings-y-graph-neural-network.html#cb279-58" tabindex="-1"></a></span>
<span id="cb279-59"><a href="embeddings-y-graph-neural-network.html#cb279-59" tabindex="-1"></a>    <span class="kw">def</span> _train_epoch(<span class="va">self</span>, data, optimizer):</span>
<span id="cb279-60"><a href="embeddings-y-graph-neural-network.html#cb279-60" tabindex="-1"></a>        <span class="va">self</span>.train()</span>
<span id="cb279-61"><a href="embeddings-y-graph-neural-network.html#cb279-61" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb279-62"><a href="embeddings-y-graph-neural-network.html#cb279-62" tabindex="-1"></a></span>
<span id="cb279-63"><a href="embeddings-y-graph-neural-network.html#cb279-63" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.encode(data.x, data.edge_index)</span>
<span id="cb279-64"><a href="embeddings-y-graph-neural-network.html#cb279-64" tabindex="-1"></a></span>
<span id="cb279-65"><a href="embeddings-y-graph-neural-network.html#cb279-65" tabindex="-1"></a>        neg_edge_index <span class="op">=</span> negative_sampling(</span>
<span id="cb279-66"><a href="embeddings-y-graph-neural-network.html#cb279-66" tabindex="-1"></a>            edge_index<span class="op">=</span>data.edge_index,</span>
<span id="cb279-67"><a href="embeddings-y-graph-neural-network.html#cb279-67" tabindex="-1"></a>            num_nodes<span class="op">=</span>data.num_nodes,</span>
<span id="cb279-68"><a href="embeddings-y-graph-neural-network.html#cb279-68" tabindex="-1"></a>            num_neg_samples<span class="op">=</span>data.edge_label_index.size(<span class="dv">1</span>),</span>
<span id="cb279-69"><a href="embeddings-y-graph-neural-network.html#cb279-69" tabindex="-1"></a>            method<span class="op">=</span><span class="st">&quot;sparse&quot;</span></span>
<span id="cb279-70"><a href="embeddings-y-graph-neural-network.html#cb279-70" tabindex="-1"></a>        )</span>
<span id="cb279-71"><a href="embeddings-y-graph-neural-network.html#cb279-71" tabindex="-1"></a></span>
<span id="cb279-72"><a href="embeddings-y-graph-neural-network.html#cb279-72" tabindex="-1"></a>        edge_index <span class="op">=</span> torch.cat(</span>
<span id="cb279-73"><a href="embeddings-y-graph-neural-network.html#cb279-73" tabindex="-1"></a>            [data.edge_label_index, neg_edge_index],</span>
<span id="cb279-74"><a href="embeddings-y-graph-neural-network.html#cb279-74" tabindex="-1"></a>            dim<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb279-75"><a href="embeddings-y-graph-neural-network.html#cb279-75" tabindex="-1"></a>        )</span>
<span id="cb279-76"><a href="embeddings-y-graph-neural-network.html#cb279-76" tabindex="-1"></a></span>
<span id="cb279-77"><a href="embeddings-y-graph-neural-network.html#cb279-77" tabindex="-1"></a>        edge_label <span class="op">=</span> torch.cat(</span>
<span id="cb279-78"><a href="embeddings-y-graph-neural-network.html#cb279-78" tabindex="-1"></a>            [</span>
<span id="cb279-79"><a href="embeddings-y-graph-neural-network.html#cb279-79" tabindex="-1"></a>                data.edge_label,</span>
<span id="cb279-80"><a href="embeddings-y-graph-neural-network.html#cb279-80" tabindex="-1"></a>                data.edge_label.new_zeros(neg_edge_index.size(<span class="dv">1</span>))</span>
<span id="cb279-81"><a href="embeddings-y-graph-neural-network.html#cb279-81" tabindex="-1"></a>            ],</span>
<span id="cb279-82"><a href="embeddings-y-graph-neural-network.html#cb279-82" tabindex="-1"></a>            dim<span class="op">=</span><span class="dv">0</span></span>
<span id="cb279-83"><a href="embeddings-y-graph-neural-network.html#cb279-83" tabindex="-1"></a>        )</span>
<span id="cb279-84"><a href="embeddings-y-graph-neural-network.html#cb279-84" tabindex="-1"></a></span>
<span id="cb279-85"><a href="embeddings-y-graph-neural-network.html#cb279-85" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.decode(z, edge_index).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb279-86"><a href="embeddings-y-graph-neural-network.html#cb279-86" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">self</span>.criterion(logits, edge_label)</span>
<span id="cb279-87"><a href="embeddings-y-graph-neural-network.html#cb279-87" tabindex="-1"></a></span>
<span id="cb279-88"><a href="embeddings-y-graph-neural-network.html#cb279-88" tabindex="-1"></a>        loss.backward()</span>
<span id="cb279-89"><a href="embeddings-y-graph-neural-network.html#cb279-89" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb279-90"><a href="embeddings-y-graph-neural-network.html#cb279-90" tabindex="-1"></a></span>
<span id="cb279-91"><a href="embeddings-y-graph-neural-network.html#cb279-91" tabindex="-1"></a>        <span class="cf">return</span> loss.item()</span>
<span id="cb279-92"><a href="embeddings-y-graph-neural-network.html#cb279-92" tabindex="-1"></a></span>
<span id="cb279-93"><a href="embeddings-y-graph-neural-network.html#cb279-93" tabindex="-1"></a>    <span class="co"># -------------------------------------------------</span></span>
<span id="cb279-94"><a href="embeddings-y-graph-neural-network.html#cb279-94" tabindex="-1"></a>    <span class="co"># Evaluation</span></span>
<span id="cb279-95"><a href="embeddings-y-graph-neural-network.html#cb279-95" tabindex="-1"></a>    <span class="co"># -------------------------------------------------</span></span>
<span id="cb279-96"><a href="embeddings-y-graph-neural-network.html#cb279-96" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb279-97"><a href="embeddings-y-graph-neural-network.html#cb279-97" tabindex="-1"></a>    <span class="kw">def</span> validate(<span class="va">self</span>, data):</span>
<span id="cb279-98"><a href="embeddings-y-graph-neural-network.html#cb279-98" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._eval_auc(data)</span>
<span id="cb279-99"><a href="embeddings-y-graph-neural-network.html#cb279-99" tabindex="-1"></a></span>
<span id="cb279-100"><a href="embeddings-y-graph-neural-network.html#cb279-100" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb279-101"><a href="embeddings-y-graph-neural-network.html#cb279-101" tabindex="-1"></a>    <span class="kw">def</span> test(<span class="va">self</span>, data):</span>
<span id="cb279-102"><a href="embeddings-y-graph-neural-network.html#cb279-102" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._eval_auc(data)</span>
<span id="cb279-103"><a href="embeddings-y-graph-neural-network.html#cb279-103" tabindex="-1"></a></span>
<span id="cb279-104"><a href="embeddings-y-graph-neural-network.html#cb279-104" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb279-105"><a href="embeddings-y-graph-neural-network.html#cb279-105" tabindex="-1"></a>    <span class="kw">def</span> _eval_auc(<span class="va">self</span>, data):</span>
<span id="cb279-106"><a href="embeddings-y-graph-neural-network.html#cb279-106" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">eval</span>()</span>
<span id="cb279-107"><a href="embeddings-y-graph-neural-network.html#cb279-107" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.encode(data.x, data.edge_index)</span>
<span id="cb279-108"><a href="embeddings-y-graph-neural-network.html#cb279-108" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.decode(z, data.edge_label_index).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb279-109"><a href="embeddings-y-graph-neural-network.html#cb279-109" tabindex="-1"></a>        probs <span class="op">=</span> logits.sigmoid()</span>
<span id="cb279-110"><a href="embeddings-y-graph-neural-network.html#cb279-110" tabindex="-1"></a></span>
<span id="cb279-111"><a href="embeddings-y-graph-neural-network.html#cb279-111" tabindex="-1"></a>        <span class="cf">return</span> roc_auc_score(</span>
<span id="cb279-112"><a href="embeddings-y-graph-neural-network.html#cb279-112" tabindex="-1"></a>            data.edge_label.cpu().numpy(),</span>
<span id="cb279-113"><a href="embeddings-y-graph-neural-network.html#cb279-113" tabindex="-1"></a>            probs.cpu().numpy()</span>
<span id="cb279-114"><a href="embeddings-y-graph-neural-network.html#cb279-114" tabindex="-1"></a>        )</span></code></pre></div>
<p>Entrenaremos el modelo:</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb280-1"><a href="embeddings-y-graph-neural-network.html#cb280-1" tabindex="-1"></a>model <span class="op">=</span> GCNLinkPredictor(</span>
<span id="cb280-2"><a href="embeddings-y-graph-neural-network.html#cb280-2" tabindex="-1"></a>    dim_in<span class="op">=</span>dataset.num_features,</span>
<span id="cb280-3"><a href="embeddings-y-graph-neural-network.html#cb280-3" tabindex="-1"></a>    dim_h<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb280-4"><a href="embeddings-y-graph-neural-network.html#cb280-4" tabindex="-1"></a>    dim_z<span class="op">=</span><span class="dv">64</span></span>
<span id="cb280-5"><a href="embeddings-y-graph-neural-network.html#cb280-5" tabindex="-1"></a>).to(device)</span>
<span id="cb280-6"><a href="embeddings-y-graph-neural-network.html#cb280-6" tabindex="-1"></a></span>
<span id="cb280-7"><a href="embeddings-y-graph-neural-network.html#cb280-7" tabindex="-1"></a>model.fit(</span>
<span id="cb280-8"><a href="embeddings-y-graph-neural-network.html#cb280-8" tabindex="-1"></a>    train_data<span class="op">=</span>train_data,</span>
<span id="cb280-9"><a href="embeddings-y-graph-neural-network.html#cb280-9" tabindex="-1"></a>    val_data<span class="op">=</span>val_data,</span>
<span id="cb280-10"><a href="embeddings-y-graph-neural-network.html#cb280-10" tabindex="-1"></a>    test_data<span class="op">=</span>test_data,</span>
<span id="cb280-11"><a href="embeddings-y-graph-neural-network.html#cb280-11" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb280-12"><a href="embeddings-y-graph-neural-network.html#cb280-12" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">0.01</span></span>
<span id="cb280-13"><a href="embeddings-y-graph-neural-network.html#cb280-13" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Epoch 020 | Loss: 0.6507 | Val AUC: 0.6223 | Test AUC: 0.6759
## Epoch 040 | Loss: 0.5127 | Val AUC: 0.8213 | Test AUC: 0.8200
## Epoch 060 | Loss: 0.4715 | Val AUC: 0.8632 | Test AUC: 0.8667
## Epoch 080 | Loss: 0.4565 | Val AUC: 0.8731 | Test AUC: 0.8870
## Epoch 100 | Loss: 0.4440 | Val AUC: 0.8817 | Test AUC: 0.8963
## Final Test AUC: 0.8963</code></pre>
<div class="sourceCode" id="cb282"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb282-1"><a href="embeddings-y-graph-neural-network.html#cb282-1" tabindex="-1"></a>z <span class="op">=</span> model.encode(test_data.x, test_data.edge_index)</span>
<span id="cb282-2"><a href="embeddings-y-graph-neural-network.html#cb282-2" tabindex="-1"></a>final_edge_index <span class="op">=</span> model.decode_all(z)</span>
<span id="cb282-3"><a href="embeddings-y-graph-neural-network.html#cb282-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;z&quot;</span>)</span></code></pre></div>
<pre><code>## z</code></pre>
<div class="sourceCode" id="cb284"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb284-1"><a href="embeddings-y-graph-neural-network.html#cb284-1" tabindex="-1"></a><span class="bu">print</span>(z)</span></code></pre></div>
<pre><code>## tensor([[-0.2263, -0.3726, -0.1493,  ..., -0.2022, -0.0197, -0.1625],
##         [ 0.0563,  0.2069, -0.0334,  ...,  0.0959,  0.1410,  0.2703],
##         [-0.0349,  0.1045, -0.0776,  ...,  0.0055,  0.2627,  0.0729],
##         ...,
##         [ 0.0200, -0.0608, -0.0205,  ..., -0.0028,  0.0893, -0.1904],
##         [-0.0408, -0.0046,  0.0254,  ..., -0.0756,  0.3298, -0.1488],
##         [-0.0293, -0.0088,  0.0251,  ..., -0.0520,  0.2167, -0.1242]],
##        grad_fn=&lt;AddBackward0&gt;)</code></pre>
<div class="sourceCode" id="cb286"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb286-1"><a href="embeddings-y-graph-neural-network.html#cb286-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;final_edge_index&quot;</span>)</span></code></pre></div>
<pre><code>## final_edge_index</code></pre>
<div class="sourceCode" id="cb288"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb288-1"><a href="embeddings-y-graph-neural-network.html#cb288-1" tabindex="-1"></a><span class="bu">print</span>(final_edge_index)</span></code></pre></div>
<pre><code>## tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],
##         [   0,    2,    4,  ..., 2705, 2706, 2707]])</code></pre>
</div>
<div id="ejemplo-clasificación-de-artículos-de-investigación-por-categoría-parte-ii-2" class="section level3 hasAnchor" number="15.4.2">
<h3><span class="header-section-number">15.4.2</span> Ejemplo: Clasificación de artículos de investigación por categoría (Parte II)<a href="embeddings-y-graph-neural-network.html#ejemplo-clasificaci%C3%B3n-de-art%C3%ADculos-de-investigaci%C3%B3n-por-categor%C3%ADa-parte-ii-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A continuación mostramos una arquitectura que permite hace la predicción de aristas
usando GraphSage:</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb290-1"><a href="embeddings-y-graph-neural-network.html#cb290-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb290-2"><a href="embeddings-y-graph-neural-network.html#cb290-2" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb290-3"><a href="embeddings-y-graph-neural-network.html#cb290-3" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb290-4"><a href="embeddings-y-graph-neural-network.html#cb290-4" tabindex="-1"></a><span class="im">from</span> torch_cluster <span class="im">import</span> random_walk</span>
<span id="cb290-5"><a href="embeddings-y-graph-neural-network.html#cb290-5" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb290-6"><a href="embeddings-y-graph-neural-network.html#cb290-6" tabindex="-1"></a></span>
<span id="cb290-7"><a href="embeddings-y-graph-neural-network.html#cb290-7" tabindex="-1"></a><span class="im">import</span> torch_geometric.transforms <span class="im">as</span> T</span>
<span id="cb290-8"><a href="embeddings-y-graph-neural-network.html#cb290-8" tabindex="-1"></a><span class="im">from</span> torch_geometric.nn <span class="im">import</span> SAGEConv</span>
<span id="cb290-9"><a href="embeddings-y-graph-neural-network.html#cb290-9" tabindex="-1"></a><span class="im">from</span> torch_geometric.datasets <span class="im">import</span> Planetoid</span>
<span id="cb290-10"><a href="embeddings-y-graph-neural-network.html#cb290-10" tabindex="-1"></a><span class="im">from</span> torch_geometric.data <span class="im">import</span> NeighborSampler <span class="im">as</span> RawNeighborSampler</span>
<span id="cb290-11"><a href="embeddings-y-graph-neural-network.html#cb290-11" tabindex="-1"></a></span>
<span id="cb290-12"><a href="embeddings-y-graph-neural-network.html#cb290-12" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb290-13"><a href="embeddings-y-graph-neural-network.html#cb290-13" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span></code></pre></div>
<div class="sourceCode" id="cb291"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb291-1"><a href="embeddings-y-graph-neural-network.html#cb291-1" tabindex="-1"></a>dataset <span class="op">=</span> <span class="st">&#39;Cora&#39;</span></span>
<span id="cb291-2"><a href="embeddings-y-graph-neural-network.html#cb291-2" tabindex="-1"></a>path <span class="op">=</span> <span class="st">&#39;./data&#39;</span></span>
<span id="cb291-3"><a href="embeddings-y-graph-neural-network.html#cb291-3" tabindex="-1"></a>dataset <span class="op">=</span> Planetoid(path, dataset, transform<span class="op">=</span>T.NormalizeFeatures())</span>
<span id="cb291-4"><a href="embeddings-y-graph-neural-network.html#cb291-4" tabindex="-1"></a>data <span class="op">=</span> dataset[<span class="dv">0</span>]</span></code></pre></div>
<div class="sourceCode" id="cb292"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb292-1"><a href="embeddings-y-graph-neural-network.html#cb292-1" tabindex="-1"></a><span class="kw">class</span> NeighborSampler(RawNeighborSampler):</span>
<span id="cb292-2"><a href="embeddings-y-graph-neural-network.html#cb292-2" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, batch):</span>
<span id="cb292-3"><a href="embeddings-y-graph-neural-network.html#cb292-3" tabindex="-1"></a>        batch <span class="op">=</span> torch.tensor(batch)</span>
<span id="cb292-4"><a href="embeddings-y-graph-neural-network.html#cb292-4" tabindex="-1"></a>        row, col, _ <span class="op">=</span> <span class="va">self</span>.adj_t.coo()</span>
<span id="cb292-5"><a href="embeddings-y-graph-neural-network.html#cb292-5" tabindex="-1"></a></span>
<span id="cb292-6"><a href="embeddings-y-graph-neural-network.html#cb292-6" tabindex="-1"></a>        <span class="co"># For each node in `batch`, we sample a direct neighbor (as positive</span></span>
<span id="cb292-7"><a href="embeddings-y-graph-neural-network.html#cb292-7" tabindex="-1"></a>        <span class="co"># example) and a random node (as negative example):</span></span>
<span id="cb292-8"><a href="embeddings-y-graph-neural-network.html#cb292-8" tabindex="-1"></a>        pos_batch <span class="op">=</span> random_walk(row, col, batch, walk_length<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb292-9"><a href="embeddings-y-graph-neural-network.html#cb292-9" tabindex="-1"></a>                                coalesced<span class="op">=</span><span class="va">False</span>)[:, <span class="dv">1</span>]</span>
<span id="cb292-10"><a href="embeddings-y-graph-neural-network.html#cb292-10" tabindex="-1"></a></span>
<span id="cb292-11"><a href="embeddings-y-graph-neural-network.html#cb292-11" tabindex="-1"></a>        neg_batch <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="va">self</span>.adj_t.size(<span class="dv">1</span>), (batch.numel(), ),</span>
<span id="cb292-12"><a href="embeddings-y-graph-neural-network.html#cb292-12" tabindex="-1"></a>                                  dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb292-13"><a href="embeddings-y-graph-neural-network.html#cb292-13" tabindex="-1"></a></span>
<span id="cb292-14"><a href="embeddings-y-graph-neural-network.html#cb292-14" tabindex="-1"></a>        batch <span class="op">=</span> torch.cat([batch, pos_batch, neg_batch], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb292-15"><a href="embeddings-y-graph-neural-network.html#cb292-15" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>(NeighborSampler, <span class="va">self</span>).sample(batch)</span>
<span id="cb292-16"><a href="embeddings-y-graph-neural-network.html#cb292-16" tabindex="-1"></a></span>
<span id="cb292-17"><a href="embeddings-y-graph-neural-network.html#cb292-17" tabindex="-1"></a></span>
<span id="cb292-18"><a href="embeddings-y-graph-neural-network.html#cb292-18" tabindex="-1"></a>train_loader <span class="op">=</span> NeighborSampler(data.edge_index, sizes<span class="op">=</span>[<span class="dv">10</span>, <span class="dv">10</span>], batch_size<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb292-19"><a href="embeddings-y-graph-neural-network.html#cb292-19" tabindex="-1"></a>                               shuffle<span class="op">=</span><span class="va">True</span>, num_nodes<span class="op">=</span>data.num_nodes)</span>
<span id="cb292-20"><a href="embeddings-y-graph-neural-network.html#cb292-20" tabindex="-1"></a></span>
<span id="cb292-21"><a href="embeddings-y-graph-neural-network.html#cb292-21" tabindex="-1"></a></span>
<span id="cb292-22"><a href="embeddings-y-graph-neural-network.html#cb292-22" tabindex="-1"></a><span class="kw">class</span> SAGE(nn.Module):</span>
<span id="cb292-23"><a href="embeddings-y-graph-neural-network.html#cb292-23" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, hidden_channels, num_layers):</span>
<span id="cb292-24"><a href="embeddings-y-graph-neural-network.html#cb292-24" tabindex="-1"></a>        <span class="bu">super</span>(SAGE, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb292-25"><a href="embeddings-y-graph-neural-network.html#cb292-25" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb292-26"><a href="embeddings-y-graph-neural-network.html#cb292-26" tabindex="-1"></a>        <span class="va">self</span>.convs <span class="op">=</span> nn.ModuleList()</span>
<span id="cb292-27"><a href="embeddings-y-graph-neural-network.html#cb292-27" tabindex="-1"></a>        </span>
<span id="cb292-28"><a href="embeddings-y-graph-neural-network.html#cb292-28" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers):</span>
<span id="cb292-29"><a href="embeddings-y-graph-neural-network.html#cb292-29" tabindex="-1"></a>            in_channels <span class="op">=</span> in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> hidden_channels</span>
<span id="cb292-30"><a href="embeddings-y-graph-neural-network.html#cb292-30" tabindex="-1"></a>            <span class="va">self</span>.convs.append(SAGEConv(in_channels, hidden_channels))</span>
<span id="cb292-31"><a href="embeddings-y-graph-neural-network.html#cb292-31" tabindex="-1"></a></span>
<span id="cb292-32"><a href="embeddings-y-graph-neural-network.html#cb292-32" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, adjs):</span>
<span id="cb292-33"><a href="embeddings-y-graph-neural-network.html#cb292-33" tabindex="-1"></a>        <span class="cf">for</span> i, (edge_index, _, size) <span class="kw">in</span> <span class="bu">enumerate</span>(adjs):</span>
<span id="cb292-34"><a href="embeddings-y-graph-neural-network.html#cb292-34" tabindex="-1"></a>            x_target <span class="op">=</span> x[:size[<span class="dv">1</span>]]  <span class="co"># Target nodes are always placed first.</span></span>
<span id="cb292-35"><a href="embeddings-y-graph-neural-network.html#cb292-35" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.convs[i]((x, x_target), edge_index)</span>
<span id="cb292-36"><a href="embeddings-y-graph-neural-network.html#cb292-36" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">!=</span> <span class="va">self</span>.num_layers <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb292-37"><a href="embeddings-y-graph-neural-network.html#cb292-37" tabindex="-1"></a>                x <span class="op">=</span> x.relu()</span>
<span id="cb292-38"><a href="embeddings-y-graph-neural-network.html#cb292-38" tabindex="-1"></a>                x <span class="op">=</span> F.dropout(x, p<span class="op">=</span><span class="fl">0.5</span>, training<span class="op">=</span><span class="va">self</span>.training)</span>
<span id="cb292-39"><a href="embeddings-y-graph-neural-network.html#cb292-39" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb292-40"><a href="embeddings-y-graph-neural-network.html#cb292-40" tabindex="-1"></a></span>
<span id="cb292-41"><a href="embeddings-y-graph-neural-network.html#cb292-41" tabindex="-1"></a>    <span class="kw">def</span> full_forward(<span class="va">self</span>, x, edge_index):</span>
<span id="cb292-42"><a href="embeddings-y-graph-neural-network.html#cb292-42" tabindex="-1"></a>        <span class="cf">for</span> i, conv <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.convs):</span>
<span id="cb292-43"><a href="embeddings-y-graph-neural-network.html#cb292-43" tabindex="-1"></a>            x <span class="op">=</span> conv(x, edge_index)</span>
<span id="cb292-44"><a href="embeddings-y-graph-neural-network.html#cb292-44" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">!=</span> <span class="va">self</span>.num_layers <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb292-45"><a href="embeddings-y-graph-neural-network.html#cb292-45" tabindex="-1"></a>                x <span class="op">=</span> x.relu()</span>
<span id="cb292-46"><a href="embeddings-y-graph-neural-network.html#cb292-46" tabindex="-1"></a>                x <span class="op">=</span> F.dropout(x, p<span class="op">=</span><span class="fl">0.5</span>, training<span class="op">=</span><span class="va">self</span>.training)</span>
<span id="cb292-47"><a href="embeddings-y-graph-neural-network.html#cb292-47" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb292-48"><a href="embeddings-y-graph-neural-network.html#cb292-48" tabindex="-1"></a></span>
<span id="cb292-49"><a href="embeddings-y-graph-neural-network.html#cb292-49" tabindex="-1"></a></span>
<span id="cb292-50"><a href="embeddings-y-graph-neural-network.html#cb292-50" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&#39;cuda&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb292-51"><a href="embeddings-y-graph-neural-network.html#cb292-51" tabindex="-1"></a>model <span class="op">=</span> SAGE(data.num_node_features, hidden_channels<span class="op">=</span><span class="dv">64</span>, num_layers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb292-52"><a href="embeddings-y-graph-neural-network.html#cb292-52" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb292-53"><a href="embeddings-y-graph-neural-network.html#cb292-53" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb292-54"><a href="embeddings-y-graph-neural-network.html#cb292-54" tabindex="-1"></a>x, edge_index <span class="op">=</span> data.x.to(device), data.edge_index.to(device)</span>
<span id="cb292-55"><a href="embeddings-y-graph-neural-network.html#cb292-55" tabindex="-1"></a></span>
<span id="cb292-56"><a href="embeddings-y-graph-neural-network.html#cb292-56" tabindex="-1"></a></span>
<span id="cb292-57"><a href="embeddings-y-graph-neural-network.html#cb292-57" tabindex="-1"></a><span class="kw">def</span> train():</span>
<span id="cb292-58"><a href="embeddings-y-graph-neural-network.html#cb292-58" tabindex="-1"></a>    model.train()</span>
<span id="cb292-59"><a href="embeddings-y-graph-neural-network.html#cb292-59" tabindex="-1"></a></span>
<span id="cb292-60"><a href="embeddings-y-graph-neural-network.html#cb292-60" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb292-61"><a href="embeddings-y-graph-neural-network.html#cb292-61" tabindex="-1"></a>    <span class="cf">for</span> batch_size, n_id, adjs <span class="kw">in</span> train_loader:</span>
<span id="cb292-62"><a href="embeddings-y-graph-neural-network.html#cb292-62" tabindex="-1"></a>        <span class="co"># `adjs` holds a list of `(edge_index, e_id, size)` tuples.</span></span>
<span id="cb292-63"><a href="embeddings-y-graph-neural-network.html#cb292-63" tabindex="-1"></a>        adjs <span class="op">=</span> [adj.to(device) <span class="cf">for</span> adj <span class="kw">in</span> adjs]</span>
<span id="cb292-64"><a href="embeddings-y-graph-neural-network.html#cb292-64" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb292-65"><a href="embeddings-y-graph-neural-network.html#cb292-65" tabindex="-1"></a></span>
<span id="cb292-66"><a href="embeddings-y-graph-neural-network.html#cb292-66" tabindex="-1"></a>        out <span class="op">=</span> model(x[n_id], adjs)</span>
<span id="cb292-67"><a href="embeddings-y-graph-neural-network.html#cb292-67" tabindex="-1"></a>        out, pos_out, neg_out <span class="op">=</span> out.split(out.size(<span class="dv">0</span>) <span class="op">//</span> <span class="dv">3</span>, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb292-68"><a href="embeddings-y-graph-neural-network.html#cb292-68" tabindex="-1"></a></span>
<span id="cb292-69"><a href="embeddings-y-graph-neural-network.html#cb292-69" tabindex="-1"></a>        pos_loss <span class="op">=</span> F.logsigmoid((out <span class="op">*</span> pos_out).<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>)).mean()</span>
<span id="cb292-70"><a href="embeddings-y-graph-neural-network.html#cb292-70" tabindex="-1"></a>        neg_loss <span class="op">=</span> F.logsigmoid(<span class="op">-</span>(out <span class="op">*</span> neg_out).<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>)).mean()</span>
<span id="cb292-71"><a href="embeddings-y-graph-neural-network.html#cb292-71" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>pos_loss <span class="op">-</span> neg_loss</span>
<span id="cb292-72"><a href="embeddings-y-graph-neural-network.html#cb292-72" tabindex="-1"></a>        loss.backward()</span>
<span id="cb292-73"><a href="embeddings-y-graph-neural-network.html#cb292-73" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb292-74"><a href="embeddings-y-graph-neural-network.html#cb292-74" tabindex="-1"></a></span>
<span id="cb292-75"><a href="embeddings-y-graph-neural-network.html#cb292-75" tabindex="-1"></a>        total_loss <span class="op">+=</span> <span class="bu">float</span>(loss) <span class="op">*</span> out.size(<span class="dv">0</span>)</span>
<span id="cb292-76"><a href="embeddings-y-graph-neural-network.html#cb292-76" tabindex="-1"></a></span>
<span id="cb292-77"><a href="embeddings-y-graph-neural-network.html#cb292-77" tabindex="-1"></a>    <span class="cf">return</span> total_loss <span class="op">/</span> data.num_nodes</span>
<span id="cb292-78"><a href="embeddings-y-graph-neural-network.html#cb292-78" tabindex="-1"></a></span>
<span id="cb292-79"><a href="embeddings-y-graph-neural-network.html#cb292-79" tabindex="-1"></a></span>
<span id="cb292-80"><a href="embeddings-y-graph-neural-network.html#cb292-80" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb292-81"><a href="embeddings-y-graph-neural-network.html#cb292-81" tabindex="-1"></a><span class="kw">def</span> test():</span>
<span id="cb292-82"><a href="embeddings-y-graph-neural-network.html#cb292-82" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb292-83"><a href="embeddings-y-graph-neural-network.html#cb292-83" tabindex="-1"></a>    out <span class="op">=</span> model.full_forward(x, edge_index).cpu()</span>
<span id="cb292-84"><a href="embeddings-y-graph-neural-network.html#cb292-84" tabindex="-1"></a></span>
<span id="cb292-85"><a href="embeddings-y-graph-neural-network.html#cb292-85" tabindex="-1"></a>    clf <span class="op">=</span> LogisticRegression()</span>
<span id="cb292-86"><a href="embeddings-y-graph-neural-network.html#cb292-86" tabindex="-1"></a>    clf.fit(out[data.train_mask], data.y[data.train_mask])</span>
<span id="cb292-87"><a href="embeddings-y-graph-neural-network.html#cb292-87" tabindex="-1"></a></span>
<span id="cb292-88"><a href="embeddings-y-graph-neural-network.html#cb292-88" tabindex="-1"></a>    val_acc <span class="op">=</span> clf.score(out[data.val_mask], data.y[data.val_mask])</span>
<span id="cb292-89"><a href="embeddings-y-graph-neural-network.html#cb292-89" tabindex="-1"></a>    test_acc <span class="op">=</span> clf.score(out[data.test_mask], data.y[data.test_mask])</span>
<span id="cb292-90"><a href="embeddings-y-graph-neural-network.html#cb292-90" tabindex="-1"></a></span>
<span id="cb292-91"><a href="embeddings-y-graph-neural-network.html#cb292-91" tabindex="-1"></a>    <span class="cf">return</span> val_acc, test_acc</span>
<span id="cb292-92"><a href="embeddings-y-graph-neural-network.html#cb292-92" tabindex="-1"></a></span>
<span id="cb292-93"><a href="embeddings-y-graph-neural-network.html#cb292-93" tabindex="-1"></a></span>
<span id="cb292-94"><a href="embeddings-y-graph-neural-network.html#cb292-94" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">51</span>):</span>
<span id="cb292-95"><a href="embeddings-y-graph-neural-network.html#cb292-95" tabindex="-1"></a>    loss <span class="op">=</span> train()</span>
<span id="cb292-96"><a href="embeddings-y-graph-neural-network.html#cb292-96" tabindex="-1"></a>    val_acc, test_acc <span class="op">=</span> test()</span>
<span id="cb292-97"><a href="embeddings-y-graph-neural-network.html#cb292-97" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&#39;Epoch: </span><span class="sc">{</span>epoch<span class="sc">:03d}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">, &#39;</span></span>
<span id="cb292-98"><a href="embeddings-y-graph-neural-network.html#cb292-98" tabindex="-1"></a>          <span class="ss">f&#39;Val: </span><span class="sc">{</span>val_acc<span class="sc">:.4f}</span><span class="ss">, Test: </span><span class="sc">{</span>test_acc<span class="sc">:.4f}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>## Epoch: 001, Loss: 1.3853, Val: 0.3420, Test: 0.3540
## Epoch: 002, Loss: 1.3016, Val: 0.5320, Test: 0.5530
## Epoch: 003, Loss: 1.1676, Val: 0.5800, Test: 0.5980
## Epoch: 004, Loss: 1.1151, Val: 0.6220, Test: 0.6590
## Epoch: 005, Loss: 1.0753, Val: 0.6620, Test: 0.7030
## Epoch: 006, Loss: 1.0390, Val: 0.6920, Test: 0.7310
## Epoch: 007, Loss: 1.0165, Val: 0.7120, Test: 0.7380
## Epoch: 008, Loss: 0.9868, Val: 0.7180, Test: 0.7430
## Epoch: 009, Loss: 1.0019, Val: 0.7280, Test: 0.7430
## Epoch: 010, Loss: 0.9861, Val: 0.7160, Test: 0.7470
## Epoch: 011, Loss: 0.9904, Val: 0.7100, Test: 0.7450
## Epoch: 012, Loss: 0.9626, Val: 0.7160, Test: 0.7420
## Epoch: 013, Loss: 0.9634, Val: 0.7300, Test: 0.7410
## Epoch: 014, Loss: 0.9637, Val: 0.7300, Test: 0.7510
## Epoch: 015, Loss: 0.9657, Val: 0.7300, Test: 0.7650
## Epoch: 016, Loss: 0.9591, Val: 0.7380, Test: 0.7620
## Epoch: 017, Loss: 0.9565, Val: 0.7380, Test: 0.7670
## Epoch: 018, Loss: 0.9555, Val: 0.7500, Test: 0.7670
## Epoch: 019, Loss: 0.9236, Val: 0.7440, Test: 0.7660
## Epoch: 020, Loss: 0.9410, Val: 0.7420, Test: 0.7630
## Epoch: 021, Loss: 0.9429, Val: 0.7400, Test: 0.7620
## Epoch: 022, Loss: 0.9462, Val: 0.7460, Test: 0.7630
## Epoch: 023, Loss: 0.9241, Val: 0.7560, Test: 0.7660
## Epoch: 024, Loss: 0.9223, Val: 0.7520, Test: 0.7720
## Epoch: 025, Loss: 0.9196, Val: 0.7560, Test: 0.7690
## Epoch: 026, Loss: 0.9316, Val: 0.7480, Test: 0.7780
## Epoch: 027, Loss: 0.9113, Val: 0.7420, Test: 0.7790
## Epoch: 028, Loss: 0.9310, Val: 0.7540, Test: 0.7810
## Epoch: 029, Loss: 0.9184, Val: 0.7360, Test: 0.7700
## Epoch: 030, Loss: 0.9277, Val: 0.7340, Test: 0.7820
## Epoch: 031, Loss: 0.9187, Val: 0.7340, Test: 0.7830
## Epoch: 032, Loss: 0.9287, Val: 0.7300, Test: 0.7690
## Epoch: 033, Loss: 0.9155, Val: 0.7300, Test: 0.7630
## Epoch: 034, Loss: 0.9084, Val: 0.7300, Test: 0.7590
## Epoch: 035, Loss: 0.9044, Val: 0.7100, Test: 0.7510
## Epoch: 036, Loss: 0.9055, Val: 0.7220, Test: 0.7510
## Epoch: 037, Loss: 0.9109, Val: 0.7340, Test: 0.7550
## Epoch: 038, Loss: 0.9099, Val: 0.7400, Test: 0.7590
## Epoch: 039, Loss: 0.9214, Val: 0.7240, Test: 0.7490
## Epoch: 040, Loss: 0.8916, Val: 0.7340, Test: 0.7420
## Epoch: 041, Loss: 0.9080, Val: 0.7420, Test: 0.7450
## Epoch: 042, Loss: 0.8967, Val: 0.7460, Test: 0.7400
## Epoch: 043, Loss: 0.9053, Val: 0.7380, Test: 0.7430
## Epoch: 044, Loss: 0.8875, Val: 0.7360, Test: 0.7520
## Epoch: 045, Loss: 0.8979, Val: 0.7320, Test: 0.7420
## Epoch: 046, Loss: 0.8894, Val: 0.7240, Test: 0.7330
## Epoch: 047, Loss: 0.8795, Val: 0.7300, Test: 0.7350
## Epoch: 048, Loss: 0.8872, Val: 0.7200, Test: 0.7340
## Epoch: 049, Loss: 0.9017, Val: 0.7120, Test: 0.7290
## Epoch: 050, Loss: 0.8939, Val: 0.7160, Test: 0.7330</code></pre>
<p>De lo anterior, podemos representar el embedding de los nodos mediante T-SNE:</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb294-1"><a href="embeddings-y-graph-neural-network.html#cb294-1" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb294-2"><a href="embeddings-y-graph-neural-network.html#cb294-2" tabindex="-1"></a></span>
<span id="cb294-3"><a href="embeddings-y-graph-neural-network.html#cb294-3" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb294-4"><a href="embeddings-y-graph-neural-network.html#cb294-4" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb294-5"><a href="embeddings-y-graph-neural-network.html#cb294-5" tabindex="-1"></a>    out <span class="op">=</span> model.full_forward(x, edge_index).cpu()</span></code></pre></div>
<pre><code>## SAGE(
##   (convs): ModuleList(
##     (0): SAGEConv(1433, 64, aggr=mean)
##     (1): SAGEConv(64, 64, aggr=mean)
##   )
## )</code></pre>
<div class="sourceCode" id="cb296"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb296-1"><a href="embeddings-y-graph-neural-network.html#cb296-1" tabindex="-1"></a>palette <span class="op">=</span> {}</span>
<span id="cb296-2"><a href="embeddings-y-graph-neural-network.html#cb296-2" tabindex="-1"></a><span class="cf">for</span> n, y <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">set</span>(data.y.numpy())):</span>
<span id="cb296-3"><a href="embeddings-y-graph-neural-network.html#cb296-3" tabindex="-1"></a>    palette[y] <span class="op">=</span> <span class="ss">f&#39;C</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">&#39;</span></span>
<span id="cb296-4"><a href="embeddings-y-graph-neural-network.html#cb296-4" tabindex="-1"></a></span>
<span id="cb296-5"><a href="embeddings-y-graph-neural-network.html#cb296-5" tabindex="-1"></a>embd <span class="op">=</span> TSNE(</span>
<span id="cb296-6"><a href="embeddings-y-graph-neural-network.html#cb296-6" tabindex="-1"></a>    n_components<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb296-7"><a href="embeddings-y-graph-neural-network.html#cb296-7" tabindex="-1"></a>    perplexity<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb296-8"><a href="embeddings-y-graph-neural-network.html#cb296-8" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="st">&#39;auto&#39;</span>,</span>
<span id="cb296-9"><a href="embeddings-y-graph-neural-network.html#cb296-9" tabindex="-1"></a>    init<span class="op">=</span><span class="st">&#39;pca&#39;</span>,</span>
<span id="cb296-10"><a href="embeddings-y-graph-neural-network.html#cb296-10" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb296-11"><a href="embeddings-y-graph-neural-network.html#cb296-11" tabindex="-1"></a>).fit_transform(out.numpy())</span>
<span id="cb296-12"><a href="embeddings-y-graph-neural-network.html#cb296-12" tabindex="-1"></a></span>
<span id="cb296-13"><a href="embeddings-y-graph-neural-network.html#cb296-13" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span></code></pre></div>
<pre><code>## &lt;Figure size 1000x1000 with 0 Axes&gt;</code></pre>
<div class="sourceCode" id="cb298"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb298-1"><a href="embeddings-y-graph-neural-network.html#cb298-1" tabindex="-1"></a>sns.scatterplot(</span>
<span id="cb298-2"><a href="embeddings-y-graph-neural-network.html#cb298-2" tabindex="-1"></a>    x<span class="op">=</span>embd[:, <span class="dv">0</span>],</span>
<span id="cb298-3"><a href="embeddings-y-graph-neural-network.html#cb298-3" tabindex="-1"></a>    y<span class="op">=</span>embd[:, <span class="dv">1</span>],</span>
<span id="cb298-4"><a href="embeddings-y-graph-neural-network.html#cb298-4" tabindex="-1"></a>    hue<span class="op">=</span>data.y.cpu().numpy(),</span>
<span id="cb298-5"><a href="embeddings-y-graph-neural-network.html#cb298-5" tabindex="-1"></a>    palette<span class="op">=</span>palette</span>
<span id="cb298-6"><a href="embeddings-y-graph-neural-network.html#cb298-6" tabindex="-1"></a>    )</span></code></pre></div>
<pre><code>## &lt;Axes: &gt;</code></pre>
<div class="sourceCode" id="cb300"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb300-1"><a href="embeddings-y-graph-neural-network.html#cb300-1" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">&#39;upper left&#39;</span>)</span></code></pre></div>
<pre><code>## &lt;matplotlib.legend.Legend object at 0x7e6292e365d0&gt;</code></pre>
<div class="sourceCode" id="cb302"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb302-1"><a href="embeddings-y-graph-neural-network.html#cb302-1" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="deep_learning_course_python_files/figure-html/unnamed-chunk-190-1.png" alt="" width="960" />
Comparemoslo contra una visualización de las caracteristicas numéricas usando
T-SNE donde se aprecia más dispersion:</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb303-1"><a href="embeddings-y-graph-neural-network.html#cb303-1" tabindex="-1"></a>embd_x <span class="op">=</span> TSNE(</span>
<span id="cb303-2"><a href="embeddings-y-graph-neural-network.html#cb303-2" tabindex="-1"></a>    n_components<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb303-3"><a href="embeddings-y-graph-neural-network.html#cb303-3" tabindex="-1"></a>    perplexity<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb303-4"><a href="embeddings-y-graph-neural-network.html#cb303-4" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="st">&#39;auto&#39;</span>,</span>
<span id="cb303-5"><a href="embeddings-y-graph-neural-network.html#cb303-5" tabindex="-1"></a>    init<span class="op">=</span><span class="st">&#39;pca&#39;</span>,</span>
<span id="cb303-6"><a href="embeddings-y-graph-neural-network.html#cb303-6" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb303-7"><a href="embeddings-y-graph-neural-network.html#cb303-7" tabindex="-1"></a>).fit_transform(data.x.numpy())</span>
<span id="cb303-8"><a href="embeddings-y-graph-neural-network.html#cb303-8" tabindex="-1"></a></span>
<span id="cb303-9"><a href="embeddings-y-graph-neural-network.html#cb303-9" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span></code></pre></div>
<pre><code>## &lt;Figure size 1000x1000 with 0 Axes&gt;</code></pre>
<div class="sourceCode" id="cb305"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb305-1"><a href="embeddings-y-graph-neural-network.html#cb305-1" tabindex="-1"></a>sns.scatterplot(</span>
<span id="cb305-2"><a href="embeddings-y-graph-neural-network.html#cb305-2" tabindex="-1"></a>    x<span class="op">=</span>embd_x[:, <span class="dv">0</span>],</span>
<span id="cb305-3"><a href="embeddings-y-graph-neural-network.html#cb305-3" tabindex="-1"></a>    y<span class="op">=</span>embd_x[:, <span class="dv">1</span>],</span>
<span id="cb305-4"><a href="embeddings-y-graph-neural-network.html#cb305-4" tabindex="-1"></a>    hue<span class="op">=</span>data.y.cpu().numpy(),</span>
<span id="cb305-5"><a href="embeddings-y-graph-neural-network.html#cb305-5" tabindex="-1"></a>    palette<span class="op">=</span>palette</span>
<span id="cb305-6"><a href="embeddings-y-graph-neural-network.html#cb305-6" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## &lt;Axes: &gt;</code></pre>
<div class="sourceCode" id="cb307"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb307-1"><a href="embeddings-y-graph-neural-network.html#cb307-1" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">&#39;upper left&#39;</span>)</span></code></pre></div>
<pre><code>## &lt;matplotlib.legend.Legend object at 0x7e6292d18d50&gt;</code></pre>
<div class="sourceCode" id="cb309"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb309-1"><a href="embeddings-y-graph-neural-network.html#cb309-1" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="deep_learning_course_python_files/figure-html/unnamed-chunk-191-3.png" alt="" width="960" /></p>

</div>
</div>
</div>









<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script>

$('.pipehover_incremental tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).prevAll().addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});


$('.pipehover_select_one_row tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});

</script>
            </section>

          </div>
        </div>
      </div>
<a href="graph-neural-network.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["deep_learning_course_python.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
