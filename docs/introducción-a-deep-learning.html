<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 1 Introducción a Deep Learning | Deep Learning</title>
  <meta name="description" content="Capítulo 1 Introducción a Deep Learning | Deep Learning" />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 1 Introducción a Deep Learning | Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Capítulo 1 Introducción a Deep Learning | Deep Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 1 Introducción a Deep Learning | Deep Learning" />
  
  <meta name="twitter:description" content="Capítulo 1 Introducción a Deep Learning | Deep Learning" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="preliminares.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="img/aif-logo.png" width="280"></a></li|
|:-:|  
<center>Curso de Redes Neuronales Artificiales</center>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>BIENVENIDA</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#objetivo"><i class="fa fa-check"></i>Objetivo</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#alcances-del-programa"><i class="fa fa-check"></i>Alcances del Programa</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#temario"><i class="fa fa-check"></i>Temario:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#c%C3%B3digo"><i class="fa fa-check"></i>Código</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#duraci%C3%B3n-y-evaluaci%C3%B3n-del-programa"><i class="fa fa-check"></i>Duración y evaluación del programa</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recursos-y-din%C3%A1mica"><i class="fa fa-check"></i>Recursos y dinámica</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agenda"><i class="fa fa-check"></i>Agenda</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bibliograf%C3%ADa"><i class="fa fa-check"></i>Bibliografía</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Parte 1: Bases y Preeliminares</b></span></li>
<li class="chapter" data-level="1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html"><i class="fa fa-check"></i><b>1</b> Introducción a Deep Learning</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#introducci%C3%B3n-al-aprendizaje-autom%C3%A1tico"><i class="fa fa-check"></i><b>1.1</b> Introducción al aprendizaje automático</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#motivaci%C3%B3n-los-paradigmas-del-conocimiento"><i class="fa fa-check"></i><b>1.1.1</b> Motivación: Los paradigmas del conocimiento</a></li>
<li class="chapter" data-level="1.1.2" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#machine-learning-supervisado"><i class="fa fa-check"></i><b>1.1.2</b> Machine learning supervisado</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#por-qu%C3%A9-deep-learning"><i class="fa fa-check"></i><b>1.2</b> ¿Por qué Deep Learning?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#dimensi%C3%B3n-vc"><i class="fa fa-check"></i><b>1.2.1</b> Dimensión VC</a></li>
<li class="chapter" data-level="1.2.2" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#el-truco-del-kernel-svm"><i class="fa fa-check"></i><b>1.2.2</b> El truco del Kernel (SVM)</a></li>
<li class="chapter" data-level="1.2.3" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#ingenier%C3%ADa-de-caracteristicas"><i class="fa fa-check"></i><b>1.2.3</b> Ingeniería de caracteristicas</a></li>
<li class="chapter" data-level="1.2.4" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#beneficios-del-deep-learning"><i class="fa fa-check"></i><b>1.2.4</b> Beneficios del deep learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="preliminares.html"><a href="preliminares.html"><i class="fa fa-check"></i><b>2</b> Preliminares</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminares.html"><a href="preliminares.html#algebra-lineal"><i class="fa fa-check"></i><b>2.1</b> Algebra Lineal</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="preliminares.html"><a href="preliminares.html#introducci%C3%B3n"><i class="fa fa-check"></i><b>2.1.1</b> Introducción</a></li>
<li class="chapter" data-level="2.1.2" data-path="preliminares.html"><a href="preliminares.html#motivaci%C3%B3n"><i class="fa fa-check"></i><b>2.1.2</b> Motivación</a></li>
<li class="chapter" data-level="2.1.3" data-path="preliminares.html"><a href="preliminares.html#escalares"><i class="fa fa-check"></i><b>2.1.3</b> Escalares</a></li>
<li class="chapter" data-level="2.1.4" data-path="preliminares.html"><a href="preliminares.html#vectores-1"><i class="fa fa-check"></i><b>2.1.4</b> Vectores</a></li>
<li class="chapter" data-level="2.1.5" data-path="preliminares.html"><a href="preliminares.html#matrices-1"><i class="fa fa-check"></i><b>2.1.5</b> Matrices</a></li>
<li class="chapter" data-level="2.1.6" data-path="preliminares.html"><a href="preliminares.html#tensores"><i class="fa fa-check"></i><b>2.1.6</b> Tensores</a></li>
<li class="chapter" data-level="2.1.7" data-path="preliminares.html"><a href="preliminares.html#reducciones-sobre-tensores"><i class="fa fa-check"></i><b>2.1.7</b> Reducciones sobre tensores</a></li>
<li class="chapter" data-level="2.1.8" data-path="preliminares.html"><a href="preliminares.html#producto-punto"><i class="fa fa-check"></i><b>2.1.8</b> Producto punto</a></li>
<li class="chapter" data-level="2.1.9" data-path="preliminares.html"><a href="preliminares.html#producto-entre-matrices-y-vectores"><i class="fa fa-check"></i><b>2.1.9</b> Producto entre matrices y vectores</a></li>
<li class="chapter" data-level="2.1.10" data-path="preliminares.html"><a href="preliminares.html#multiplicaci%C3%B3n-matricial"><i class="fa fa-check"></i><b>2.1.10</b> Multiplicación matricial</a></li>
<li class="chapter" data-level="2.1.11" data-path="preliminares.html"><a href="preliminares.html#normas"><i class="fa fa-check"></i><b>2.1.11</b> Normas</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="preliminares.html"><a href="preliminares.html#c%C3%A1lculo-diferencial"><i class="fa fa-check"></i><b>2.2</b> Cálculo Diferencial</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="preliminares.html"><a href="preliminares.html#introducci%C3%B3n-1"><i class="fa fa-check"></i><b>2.2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2.2" data-path="preliminares.html"><a href="preliminares.html#motivaci%C3%B3n-1"><i class="fa fa-check"></i><b>2.2.2</b> Motivación</a></li>
<li class="chapter" data-level="2.2.3" data-path="preliminares.html"><a href="preliminares.html#derivadas-y-diferenciaci%C3%B3n"><i class="fa fa-check"></i><b>2.2.3</b> Derivadas y diferenciación</a></li>
<li class="chapter" data-level="2.2.4" data-path="preliminares.html"><a href="preliminares.html#regla-de-la-cadena"><i class="fa fa-check"></i><b>2.2.4</b> Regla de la cadena</a></li>
<li class="chapter" data-level="2.2.5" data-path="preliminares.html"><a href="preliminares.html#interpretaci%C3%B3n-geom%C3%A9trica-de-la-derivada"><i class="fa fa-check"></i><b>2.2.5</b> Interpretación geométrica de la derivada</a></li>
<li class="chapter" data-level="2.2.6" data-path="preliminares.html"><a href="preliminares.html#teorema-fundamental-del-c%C3%A1lculo"><i class="fa fa-check"></i><b>2.2.6</b> Teorema fundamental del cálculo</a></li>
<li class="chapter" data-level="2.2.7" data-path="preliminares.html"><a href="preliminares.html#autodiferenciaci%C3%B3n"><i class="fa fa-check"></i><b>2.2.7</b> Autodiferenciación</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html"><i class="fa fa-check"></i><b>3</b> Redes neuronales Lineales para Regresión</a></li>
<li class="chapter" data-level="4" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html"><i class="fa fa-check"></i><b>4</b> Redes neuronales Lineales para Clasificación</a>
<ul>
<li class="chapter" data-level="4.1" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#modelos-lineales-generalizados"><i class="fa fa-check"></i><b>4.1</b> Modelos Lineales Generalizados</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#historia-1"><i class="fa fa-check"></i><b>4.1.1</b> Historia</a></li>
<li class="chapter" data-level="4.1.2" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#definici%C3%B3n"><i class="fa fa-check"></i><b>4.1.2</b> Definición</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#regresi%C3%B3n-log%C3%ADstica-para-clasificaci%C3%B3n"><i class="fa fa-check"></i><b>4.2</b> Regresión Logística para Clasificación</a></li>
<li class="chapter" data-level="4.3" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#redes-neuronales"><i class="fa fa-check"></i><b>4.3</b> Redes neuronales</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#historia-2"><i class="fa fa-check"></i><b>4.3.1</b> Historia</a></li>
<li class="chapter" data-level="4.3.2" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#definici%C3%B3n-1"><i class="fa fa-check"></i><b>4.3.2</b> Definición</a></li>
<li class="chapter" data-level="4.3.3" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#neurona"><i class="fa fa-check"></i><b>4.3.3</b> Neurona</a></li>
<li class="chapter" data-level="4.3.4" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#redes-neuronales-1"><i class="fa fa-check"></i><b>4.3.4</b> Redes Neuronales</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#notas-adicionales"><i class="fa fa-check"></i>Notas adicionales</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html"><i class="fa fa-check"></i><b>5</b> Perceptrón Multicapa</a>
<ul>
<li class="chapter" data-level="5.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#perceptrones-multicapa"><i class="fa fa-check"></i><b>5.1</b> Perceptrones multicapa</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#capas-ocultas"><i class="fa fa-check"></i><b>5.1.1</b> Capas ocultas</a></li>
<li class="chapter" data-level="5.1.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#funciones-de-activaci%C3%B3n"><i class="fa fa-check"></i><b>5.1.2</b> Funciones de activación</a></li>
<li class="chapter" data-level="5.1.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#discusi%C3%B3n"><i class="fa fa-check"></i><b>5.1.3</b> Discusión</a></li>
<li class="chapter" data-level="5.1.4" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios"><i class="fa fa-check"></i><b>5.1.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n-de-perceptrones-multicapa"><i class="fa fa-check"></i><b>5.2</b> Implementación de Perceptrones Multicapa</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n-desde-cero"><i class="fa fa-check"></i><b>5.2.1</b> Implementación desde Cero</a></li>
<li class="chapter" data-level="5.2.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n"><i class="fa fa-check"></i><b>5.2.2</b> Implementación</a></li>
<li class="chapter" data-level="5.2.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios-1"><i class="fa fa-check"></i><b>5.2.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#forward-propagation-backward-propagation"><i class="fa fa-check"></i><b>5.3</b> Forward Propagation, Backward Propagation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#forward-propagation"><i class="fa fa-check"></i><b>5.3.1</b> Forward Propagation</a></li>
<li class="chapter" data-level="5.3.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#backpropagation"><i class="fa fa-check"></i><b>5.3.2</b> Backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#estabilidad-num%C3%A9rica-e-inicializaci%C3%B3n"><i class="fa fa-check"></i><b>5.4</b> Estabilidad Numérica e Inicialización</a></li>
<li class="chapter" data-level="5.5" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#generalizaci%C3%B3n-en-deep-learning"><i class="fa fa-check"></i><b>5.5</b> Generalización en Deep Learning</a></li>
<li class="chapter" data-level="5.6" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#dropout"><i class="fa fa-check"></i><b>5.6</b> Dropout</a></li>
<li class="chapter" data-level="5.7" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejemplo"><i class="fa fa-check"></i><b>5.7</b> Ejemplo</a></li>
</ul></li>
<li class="part"><span><b>Parte 2: Técnicas Modernas de Deep Learning</b></span></li>
<li class="chapter" data-level="6" data-path="guía-del-constructor.html"><a href="guía-del-constructor.html"><i class="fa fa-check"></i><b>6</b> Guía del Constructor</a></li>
<li class="chapter" data-level="7" data-path="redes-neuronales-convolucionales.html"><a href="redes-neuronales-convolucionales.html"><i class="fa fa-check"></i><b>7</b> Redes Neuronales Convolucionales</a></li>
<li class="chapter" data-level="8" data-path="redes-neuronales-convolucionales-modernas.html"><a href="redes-neuronales-convolucionales-modernas.html"><i class="fa fa-check"></i><b>8</b> Redes Neuronales Convolucionales Modernas</a></li>
<li class="chapter" data-level="9" data-path="redes-neuronales-recurrentes.html"><a href="redes-neuronales-recurrentes.html"><i class="fa fa-check"></i><b>9</b> Redes Neuronales Recurrentes</a></li>
<li class="chapter" data-level="10" data-path="redes-neuronales-recurrentes-modernas.html"><a href="redes-neuronales-recurrentes-modernas.html"><i class="fa fa-check"></i><b>10</b> Redes Neuronales Recurrentes Modernas</a></li>
<li class="chapter" data-level="11" data-path="mecanismos-de-atención-y-transformers.html"><a href="mecanismos-de-atención-y-transformers.html"><i class="fa fa-check"></i><b>11</b> Mecanismos de Atención y Transformers</a></li>
<li class="part"><span><b>Parte 3: Escalabilidad, Eficiencia y Aplicaciones</b></span></li>
<li class="chapter" data-level="12" data-path="algoritmos-de-optimización.html"><a href="algoritmos-de-optimización.html"><i class="fa fa-check"></i><b>12</b> Algoritmos de Optimización</a></li>
<li class="divider"></li>
<li><a href="./"><img src="img/aif-logo.png" width="280"></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introducción-a-deep-learning" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Capítulo 1</span> Introducción a Deep Learning<a href="introducción-a-deep-learning.html#introducci%C3%B3n-a-deep-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>El aprendizaje profundo (<em>deep learning</em>) ha transformado radicalmente campos como la visión por computadora, el procesamiento del lenguaje natural, la robótica y la bioinformática. En este capítulo examinaremos qué ha impulsado su crecimiento exponencial, presentaremos algunos de los problemas más comunes que resuelve y haremos un recorrido por sus raíces históricas.</p>
<div id="introducción-al-aprendizaje-automático" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Introducción al aprendizaje automático<a href="introducción-a-deep-learning.html#introducci%C3%B3n-al-aprendizaje-autom%C3%A1tico" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El aprendizaje automático se define como el campo de estudio que da a las computadoras la capacidad de aprender sin ser programadas explícitamente (Arthur Samuel, 1959). Formalmente, un algoritmo de <em>machine learning</em> mejora su desempeño en una tarea <span class="math inline">\(T\)</span>, medido por una métrica <span class="math inline">\(P\)</span>, a medida que adquiere más experiencia <span class="math inline">\(E\)</span>.</p>
<div id="motivación-los-paradigmas-del-conocimiento" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Motivación: Los paradigmas del conocimiento<a href="introducción-a-deep-learning.html#motivaci%C3%B3n-los-paradigmas-del-conocimiento" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A lo largo de la historia, la humanidad ha transitado por distintas formas de generar y utilizar conocimiento:</p>
<ul>
<li><strong>Era empírica</strong>: basada en la observación directa y la experiencia.</li>
<li><strong>Era teórica</strong>: dominada por modelos matemáticos y leyes físicas (por ejemplo, la mecánica newtoniana).</li>
<li><strong>Era computacional</strong>: donde simulaciones numéricas permiten explorar sistemas complejos.</li>
<li><strong>Era de los datos</strong>: en la que los patrones emergen no de ecuaciones, sino de grandes volúmenes de datos observados.</li>
</ul>
<p><img src="img/01-intro/1-01.png" width="665" style="display: block; margin: auto;" /></p>
<p>Para entender un poco más sobre estos paradigmas del conocimiento un paper muy interesante que estaba leyendo hace un tiempo <a href="https://www.nature.com/articles/s41524-022-00810-x">“Machine learning in concrete science: applications, challenges, and best practices”</a> habla acerca de la evolución de la ciencia del concreto en estos paradigmas y la relevancia exponencial que han jugado los algoritmos de aprendizaje automatico al hacer nuevos descubrimientos en estos campos.</p>
<p><img src="img/01-intro/1-02.png" width="585" style="display: block; margin: auto;" /></p>
<p>Es en esta última era donde el <em>machine learning</em> cobra protagonismo: en lugar de programar reglas explícitas, enseñamos a las máquinas a descubrir patrones a partir de ejemplos.</p>
</div>
<div id="machine-learning-supervisado" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Machine learning supervisado<a href="introducción-a-deep-learning.html#machine-learning-supervisado" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Los problemas de <em>machine learning</em> se clasifican según la naturaleza de los datos y la supervisión disponible:</p>
<div id="aprendizaje-supervisado" class="section level4 hasAnchor" number="1.1.2.1">
<h4><span class="header-section-number">1.1.2.1</span> Aprendizaje supervisado<a href="introducción-a-deep-learning.html#aprendizaje-supervisado" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Se dispone de pares entrada-salida <span class="math inline">\((x, y)\)</span>. Subtipos comunes incluyen: - <strong>Regresión</strong>: predecir una variable continua (ej. precio de una casa). - <strong>Clasificación</strong>: asignar una etiqueta discreta (ej. detección de spam). - <strong>Aprendizaje de secuencias</strong>: modelar datos ordenados en el tiempo o en secuencia (ej. traducción automática, reconocimiento de voz).</p>
</div>
<div id="componentes-del-aprendizaje-supervisado" class="section level4 hasAnchor" number="1.1.2.2">
<h4><span class="header-section-number">1.1.2.2</span> Componentes del aprendizaje supervisado<a href="introducción-a-deep-learning.html#componentes-del-aprendizaje-supervisado" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>En el aprendizaje supervisado participan los siguientes componentes.</p>
<ol style="list-style-type: decimal">
<li>Función desconocida: <span class="math inline">\(f: X-&gt;Y\)</span> Es la relación que queremos aprender entre la entrada <span class="math inline">\(X\)</span> y las salidas <span class="math inline">\(Y\)</span></li>
<li>Muestras de entrenamiento (data): Es el conjunto de pares observados <span class="math inline">\((x_n,y_n)\)</span></li>
<li>Conjunto de hipótesis: Denotado como <span class="math inline">\(H\)</span> es el conjunto de funciones candidatas que el algoritmo puede elegir para aproximar f.</li>
<li>Algoritmo de aprendizaje: Denotado como <span class="math inline">\(A\)</span> Toma como entrada los ejemplos de entrenamiento y el conjunto de hipotesis para seleccionar una <span class="math inline">\(g \in H\)</span> que mejor se ajuste a los datos.</li>
<li>Hipotesis final: Denotada como <span class="math inline">\(g \approx f\)</span> que es la función aprendida por el algoritmo</li>
</ol>
<p><img src="img/01-intro/1-03.png" width="390" style="display: block; margin: auto;" /></p>
</div>
<div id="tipos-de-datos-en-el-aprendizaje-supervisado" class="section level4 hasAnchor" number="1.1.2.3">
<h4><span class="header-section-number">1.1.2.3</span> Tipos de datos en el aprendizaje supervisado<a href="introducción-a-deep-learning.html#tipos-de-datos-en-el-aprendizaje-supervisado" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Si bien los algoritmos de aprendizaje automático operan exclusivamente con números, la estructura y el tipo de las variables (por ejemplo, categóricas, continuas, ordinales, etc.) influyen directamente en la elección del modelo más adecuado para capturar patrones y realizar predicciones efectivas.</p>
<div id="númericos" class="section level5 hasAnchor" number="1.1.2.3.1">
<h5><span class="header-section-number">1.1.2.3.1</span> Númericos<a href="introducción-a-deep-learning.html#n%C3%BAmericos" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>También conocido como datos cuantitativos, refleja la cuantización de algo medible y codificable numéricamente donde existe una relación de orden total.</p>
<p><img src="img/01-intro/1-04.png" width="592" style="display: block; margin: auto;" /></p>
</div>
<div id="categóricos" class="section level5 hasAnchor" number="1.1.2.3.2">
<h5><span class="header-section-number">1.1.2.3.2</span> Categóricos<a href="introducción-a-deep-learning.html#categ%C3%B3ricos" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Los datos categóricos se utilizan para etiquetar características que no son medibles, conocido como datos cualitativos. Por lo general se emplean números como etiquetas sin que estos tengan una relación de orden total en su significado.</p>
<p><img src="img/01-intro/1-05.png" width="588" style="display: block; margin: auto;" /></p>
<p>A menudo un mismo atributo lo podemos representar de diversas manera y la representación que elegimos podrá ser de diferente indole.</p>
<p><img src="img/01-intro/1-06.png" width="566" style="display: block; margin: auto;" /></p>
</div>
<div id="series-de-tiempo" class="section level5 hasAnchor" number="1.1.2.3.3">
<h5><span class="header-section-number">1.1.2.3.3</span> Series de tiempo<a href="introducción-a-deep-learning.html#series-de-tiempo" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Son una secuencia de números coleccionados con un intervalo regular de tiempo sobre un periodo de tiempo. En este tipo de conjunto de datos las muestras pueden estar asociadas entre si.</p>
<p><img src="img/01-intro/1-07.png" width="306" style="display: block; margin: auto;" /></p>
<p>Diversos objetos que solemos representar en este mundo como imagenes, videos o audios se pueden describir con los tipos que definimos anteriormente.</p>
</div>
</div>
<div id="los-problemas-de-machine-learning" class="section level4 hasAnchor" number="1.1.2.4">
<h4><span class="header-section-number">1.1.2.4</span> Los problemas de machine learning<a href="introducción-a-deep-learning.html#los-problemas-de-machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Los dos desafíos fundamentales que definen la naturaleza de los problemas en <em>machine learning</em>:</p>
<div id="el-problema-inverso" class="section level5 hasAnchor" number="1.1.2.4.1">
<h5><span class="header-section-number">1.1.2.4.1</span> El problema inverso<a href="introducción-a-deep-learning.html#el-problema-inverso" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>A diferencia de la física, donde las leyes predicen observaciones, en <em>machine learning</em> partimos de observaciones para inferir las reglas subyacentes. Esto es inherentemente ambiguo y puede derivar en un gran número de problemas. Por listar algunos tenemos.</p>
<p><strong>Falta de información:</strong> Se intentan identificar patrones para intentar disminuir el error basado en alguna regla, pero no hay garantia de que la regla que se este intentando cumplir sea la correcta para representar al sistema.</p>
<p><img src="img/01-intro/1-08.png" width="530" style="display: block; margin: auto;" /></p>
<p><strong>No unicidad de la solución (información inconsistente):</strong> Dado un resultado observado <span class="math inline">\(y\)</span> , puede haber múltiples configuraciones de entrada <span class="math inline">\(x_1 x_2, ...\)</span> tales que <span class="math inline">\(f(x_i) = y\)</span></p>
<p><img src="img/01-intro/1-09.png" width="566" style="display: block; margin: auto;" /></p>
</div>
<div id="generación-del-modelo-que-aproxime-al-sistema" class="section level5 hasAnchor" number="1.1.2.4.2">
<h5><span class="header-section-number">1.1.2.4.2</span> Generación del modelo que aproxime al sistema<a href="introducción-a-deep-learning.html#generaci%C3%B3n-del-modelo-que-aproxime-al-sistema" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><strong>Falta de modelo directo preciso</strong> : En muchos casos, la función directa <span class="math inline">\(f\)</span> (que mapea causas a efectos) no se conoce analíticamente, sino que se aproxima mediante simulaciones o modelos empíricos. Esto tambien se puede por no tener el conjunto de hipotesis adecuado.</p>
<p><img src="img/01-intro/1-10.png" width="458" style="display: block; margin: auto;" /></p>
<p><strong>Problema de optimización</strong>: el aprendizaje se formula como la búsqueda de parámetros que minimicen una función de pérdida a mayor número de parámetros a optimizar es menos probable obtener un minimo global, lo cual se puede realizar tambien con el problema “the curse of dimensionality”</p>
<p><img src="img/01-intro/1-11.png" width="548" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
</div>
<div id="por-qué-deep-learning" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> ¿Por qué Deep Learning?<a href="introducción-a-deep-learning.html#por-qu%C3%A9-deep-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Una pregunta que nos podriamos hacer es ¿por qué deep learning? en esta sección abordaremos una perspectiva de contraste e historica para intentar acercarnos a esta respuesta.</p>
<div id="dimensión-vc" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Dimensión VC<a href="introducción-a-deep-learning.html#dimensi%C3%B3n-vc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La <strong>dimensión de Vapnik-Chervonenkis (VC)</strong> es una medida de la capacidad de un modelo de clasificación (o conjunto de hipótesis) para distinguir entre diferentes clases. Formalmente, la dimensión VC de un conjunto de hipótesis <span class="math inline">\(H\)</span> se define de la siguiente manera:</p>
<p>Sea <span class="math inline">\(H\)</span> un conjunto de hipótesis; y <span class="math inline">\(X\)</span> un conjunto de instancias (espacio de entrada). La dimensión <span class="math inline">\(VC\)</span> de <span class="math inline">\(H\)</span> es el tamaño del mayor subconjunto de <span class="math inline">\(X\)</span> que puede ser destrozado (shattering) por <span class="math inline">\(H\)</span></p>
<p><strong>Shattering:</strong> Un conjunto de puntos <span class="math inline">\(S={x1,x2,…,xd}⊆X\)</span>, se dice que es <strong>shattered</strong> por <span class="math inline">\(H\)</span> si, para cada posible partición binaria de <span class="math inline">\(S\)</span> (es decir, cada una de las posibles asignaciones de etiquetas en <span class="math inline">\(S\)</span>), existe una hipótesis <span class="math inline">\(h \in H\)</span> tal que <span class="math inline">\(h\)</span> clasifica correctamente todos los puntos de <span class="math inline">\(S\)</span> según esa partición.</p>
<p>En otras palabras <strong>VC Dimension</strong> mide la capacidad de un modelo para ajustarse a conjuntos arbitrarios de datos. Cuanto mayor sea la dimensión VC, más complejos son los patrones que puede aprender, pero también mayor el riesgo de sobreajuste (<em>overfitting</em>).</p>
<p><img src="img/01-intro/2-01.png" width="944" style="display: block; margin: auto;" /></p>
<p>Es importante que el conjunto de puntos se encuentre en posición general, para evitar configuraciones precisas que se ajuntes de forma exacta a la forma del conjunto de hipotesis.</p>
<p><img src="img/01-intro/2-02.png" width="262" style="display: block; margin: auto;" /></p>
<p>Algo importante a remarcar es que el VC-Dimension como su nombre lo indica representa la capacidad en una instancia númerica basada en el número de dimensiones de los puntos a separar, así que si el número de dimensiones incrementa, el número de puntos maximos a clasificar correcto tambien lo hace.</p>
<p><img src="img/01-intro/2-03.png" width="938" style="display: block; margin: auto;" /></p>
</div>
<div id="el-truco-del-kernel-svm" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> El truco del Kernel (SVM)<a href="introducción-a-deep-learning.html#el-truco-del-kernel-svm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Una <strong>SVM (Máquina de Vectores de Soporte)</strong> busca separar las clases de datos encontrando una frontera óptima (hiperplano) que las divida. Sin embargo, cuando los datos no son separables linealmente en su espacio original, utiliza el <strong>truco del kernel</strong>, que consiste en proyectarlos implícitamente a un <strong>espacio de mayor dimensión</strong> donde sí puedan separarse mediante una frontera lineal. De esta forma, sin transformar los datos directamente, la SVM aplica funciones como los <strong>kernels polinomial o RBF</strong> para capturar relaciones no lineales y lograr una mejor clasificación.</p>
<p><img src="img/01-intro/2-04.png" width="950" style="display: block; margin: auto;" /></p>
<p>Sin embargo, encontrar el kernel adecuado presenta desafíos como la necesidad de probar distintas funciones, ajustar cuidadosamente sus parámetros para evitar sobreajuste o subajuste, enfrentar altos costos computacionales en grandes volúmenes de datos y lidiar con la menor interpretabilidad del modelo al trabajar en espacios de alta dimensión.</p>
</div>
<div id="ingeniería-de-caracteristicas" class="section level3 hasAnchor" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Ingeniería de caracteristicas<a href="introducción-a-deep-learning.html#ingenier%C3%ADa-de-caracteristicas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La <strong>extracción de características</strong> ayuda a resolver problemas de <strong>aprendizaje automático</strong> al transformar los datos brutos en representaciones más relevantes y compactas que facilitan el trabajo del modelo. Este proceso permite <strong>destacar la información más útil</strong>, eliminar ruido y reducir la dimensionalidad, lo que mejora la <strong>precisión</strong>, <strong>eficiencia</strong> y <strong>capacidad de generalización</strong> del algoritmo. Al enfocarse en las propiedades más representativas de los datos —por ejemplo, patrones visuales en imágenes, frecuencias en audio o variables derivadas en datos tabulares—, la extracción de características permite que los modelos aprendan de manera más efectiva y requieran menos recursos para lograr un buen desempeño.</p>
<p><img src="img/01-intro/2-05.png" width="597" style="display: block; margin: auto;" /></p>
<p>La extracción de caracteristicas resulta especialmente util para combinar variables que por si mismo no aportan tanto valor al describir un sistema, pero combinadas de cierta forma tiene valor. Por ejemplo el peso y la estatura combinada pueden crear el <strong>IMC</strong> que es un indicador clave a la hora de evaluar el sobrepeso.</p>
</div>
<div id="beneficios-del-deep-learning" class="section level3 hasAnchor" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> Beneficios del deep learning<a href="introducción-a-deep-learning.html#beneficios-del-deep-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="teorema-de-aproximación-universal" class="section level4 hasAnchor" number="1.2.4.1">
<h4><span class="header-section-number">1.2.4.1</span> Teorema de aproximación universal<a href="introducción-a-deep-learning.html#teorema-de-aproximaci%C3%B3n-universal" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><p>Establece que una <strong>red neuronal</strong> con al menos una <strong>capa oculta</strong> y un número suficiente de <strong>neuronas</strong> puede aproximar cualquier <strong>función continua</strong> en un espacio de <strong>dimensión finita</strong> con un error arbitrariamente pequeño. En otras palabras, las <strong>redes neuronales</strong> son <strong>aproximadores universales</strong>.</p></li>
<li><p>Este <strong>teorema</strong> proporciona una base teórica para la capacidad de las <strong>redes neuronales</strong> de aprender y representar <strong>funciones complejas</strong>. Sin embargo, en la práctica, el <strong>teorema</strong> no garantiza que una <strong>red entrenada</strong> encontrará la <strong>aproximación óptima</strong>, ni especifica cuántas **neuron</p></li>
</ol>
<p><img src="img/01-intro/2-06.png" width="592" style="display: block; margin: auto;" /></p>
<p><a href="https://playground.tensorflow.org/">“Jugemos un poco con las redes multicapa”</a></p>
</div>
<div id="extracción-automática-de-características-feature-extraction" class="section level4 hasAnchor" number="1.2.4.2">
<h4><span class="header-section-number">1.2.4.2</span> Extracción automática de características (feature extraction):<a href="introducción-a-deep-learning.html#extracci%C3%B3n-autom%C3%A1tica-de-caracter%C3%ADsticas-feature-extraction" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A diferencia de los métodos tradicionales, donde los ingenieros diseñaban manualmente características (ej. bordes, texturas, frecuencias), las redes profundas <strong>aprenden representaciones útiles directamente de los datos brutos</strong>. Cada capa construye representaciones de mayor nivel a partir de las anteriores (píxeles → bordes → formas → objetos).</p>
<p><img src="img/01-intro/2-07.png" width="494" style="display: block; margin: auto;" /></p>
<table>
<colgroup>
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="37%" />
</colgroup>
<thead>
<tr class="header">
<th>Tipo de capa</th>
<th>Rol / función típica</th>
<th>Qué tipo de representaciones aprende</th>
<th>Apoyo en la literatura / ejemplos</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Primeras capas</strong><br>(cercanas a la entrada)</td>
<td>Capturan patrones de bajo nivel y locales del dato bruto</td>
<td>Bordes, texturas, frecuencias básicas, características locales</td>
<td>Se habla de “aprendizaje jerárquico de características”, donde los niveles inferiores capturan elementos simples primero. (<a href="https://openstax.org/books/principles-data-science/pages/7-3-introduction-to-deep-learning?utm_source=chatgpt.com">OpenStax</a>)</td>
</tr>
<tr class="even">
<td><strong>Capas intermedias</strong><br>(ocultas medias)</td>
<td>Combinan características simples en representaciones más abstractas</td>
<td>Motivos, formas, combinaciones de las características de los niveles más bajos</td>
<td>El trabajo “Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination” estudia cómo las capas progresivas comprimen características dentro de clase y discriminan entre clases a medida que avanza la profundidad. (<a href="https://arxiv.org/abs/2311.02960?utm_source=chatgpt.com">arXiv</a>)</td>
</tr>
<tr class="odd">
<td><strong>Últimas capas</strong><br>(cercanas a la salida)</td>
<td>Transforman las representaciones abstractas hacia la decisión o la salida específica de la tarea</td>
<td>Conceptos de alto nivel, clases, predicciones finales</td>
<td>En redes de reconocimiento de imágenes, las capas finales “interpretan” las características abstractas en etiquetas o decisiones. Por ejemplo, arquitecturas como AlexNet usan capas finales totalmente conectadas para clasificar objetos. (<a href="https://en.wikipedia.org/wiki/AlexNet?utm_source=chatgpt.com">Wikipedia</a>)</td>
</tr>
</tbody>
</table>
<p><a href="https://arxiv.org/pdf/1411.1792">“How transferable are features in deep neural networks?”</a></p>
</div>
<div id="escalabilidad-con-datos-y-cómputo" class="section level4 hasAnchor" number="1.2.4.3">
<h4><span class="header-section-number">1.2.4.3</span> Escalabilidad con datos y cómputo<a href="introducción-a-deep-learning.html#escalabilidad-con-datos-y-c%C3%B3mputo" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Mientras que muchos algoritmos clásicos saturan su rendimiento con más datos, el <em>deep learning</em> <strong>mejora continuamente</strong> al aumentar el tamaño del conjunto de entrenamiento y la capacidad computacional.</p>
<p><img src="img/01-intro/2-08.png" width="360" style="display: block; margin: auto;" /></p>

<!-- ::: watermark -->
<!-- <img src="img/aif-logo.png" width="400"/> -->
<!-- ::: -->
</div>
</div>
</div>
</div>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script>

$('.pipehover_incremental tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).prevAll().addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});


$('.pipehover_select_one_row tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});

</script>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="preliminares.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["deep_learning_course_python.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
