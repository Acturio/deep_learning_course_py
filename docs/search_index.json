[["index.html", "Deep Learning BIENVENIDA Objetivo Alcances del Programa Código Duración y evaluación del programa Recursos y dinámica", " Deep Learning BIENVENIDA Objetivo Brindar al participante los elementos teóricos y prácticos básicos alrededor de la programación de REDES NEURONALES ARTIFICIALES. Aprenderá las definiciones y aprenderá a distinguir estrategias y diferentes soluciones a problemas que pueden resolverse con algoritmos de deep learning y aprenderá a usar el conjunto de librerías en Python más novedoso, estructuradas y ampliamente usadas para la creación de estructuras neuronales aplicadas a problemas predictivos, clasificación y segmentación de imagenes, series de tiempo, procesamiento de lenguaje natural (NLP), etc. Alcances del Programa Al finalizar este curso, el participante será capaz de consumir, manipular y visualizar información para resolver problemas de propósito general asociados a los datos. Apenderá a implementar diferentes algoritmos de machine learning y mejorar su desempeño predictivo en problemas de clasificación, regresión y segmentación. Requisitos: Computadora con al menos 8Gb Ram Instalar Python con versión 3.11 o superior Instalar un IDE preferido. Jupyter, RStudio, Spyder, VSCode, Colab Temario: 00. Instalación 01. Introducción a Deep Learning 02. Preliminares 03. Redes Neuronales Lineales para Regresión 04. Redes neuronales Lineales para Clasificación 05. Perceptrón Multicapa 06. Guía del Constructor 07. Redes Neuronales Convolucionales 08. Redes Neuronales Convolucionales Modernas 09. Redes Neuronales Recurrentes 10. Redes Neuronales Recurrentes Modernas 11. Mecanismos de Atención y Transformers 12. Algoritmos de Optimización 13. Desempeño Computacional 14. Visión por Computadora 15. Procesamiento de Lenguaje Natural: Pre-entrenamiento 16. Procesamiento de Lenguaje Natural: Aplicaciones 17. Aprendizaje por Refuerzo 18. Procesos Gausianos 19. Optimización paramétrica 20. Redes Generativas Adversarias Código La mayoría de las secciones de este material presentan código ejecutable. En definitiva algunas intuiciones se desarrollan mejor mediante ensayo y error, modificando el código poco a poco y observando los resultados. El código será presentado en chunks visibles y detacados respecto del resto del texto. Este puede ser copiador mediante el botón superior del lado derecho para su revisión y replicación en algún otro ambiente de prueba. import collections import hashlib import inspect import math import os import random import re import shutil import sys import tarfile import time import zipfile from collections import defaultdict import pandas as pd import requests from IPython import display from matplotlib import pyplot as plt from matplotlib_inline import backend_inline Duración y evaluación del programa El programa tiene una duración de XXX hrs. Las sesiones serán atendidas los días XxXx, de 4:30 pm a 5:45 pm Serán asignados ejercicios que el participante deberá resolver entre una semana y otra. Durante todo el programa se realizarán prácticas para reforzar el aprendizaje. Recursos y dinámica Agenda Todos los participantes del Hub podrán participar aprendiendo y compartiendo el conocimiento. Este es nuestro documento para organizarnos internamente en cuanto a los temas a impartir, fechas y orden en que se irá compartiendo cada tema. Software En esta clase estaremos usando: Python da click aquí si aún no lo descargas R &amp; RStudio da click aquí también Reticulate da click para aprender Bookdown da click aquí también Bibliografía Dive into Deep Learning Autor: Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. Editorial: Cambridge University Press Año: 2023 Deep Learning Foundations and Concepts Autor: Christopher M. Bishop with Hugh Bishop Editorial: Springer Año: 2023 ISBN: 978-3-031-45467-7 Hands On Machine Learning with Scikit-Learn, Keras and TensorFlow Autor: Aurélien Géron Editorial: O´REILLY Año: 2019 ISBN: 978-1-492-03264-9 Reinforcement Learning Autor: Richard S. Sutton, Andrew G. Barto Editorial: The MIT Press Año: 2018 ISBN: 978-0-262-19398-6 Bookdown Autor: Yihui Xie Editorial: The R Series Año: 2025 "],["introducción-a-deep-learning.html", "Capítulo 1 Introducción a Deep Learning 1.1 Introducción al aprendizaje automático 1.2 ¿Por qué Deep Learning?", " Capítulo 1 Introducción a Deep Learning El aprendizaje profundo (deep learning) ha transformado radicalmente campos como la visión por computadora, el procesamiento del lenguaje natural, la robótica y la bioinformática. En este capítulo examinaremos qué ha impulsado su crecimiento exponencial, presentaremos algunos de los problemas más comunes que resuelve y haremos un recorrido por sus raíces históricas. 1.1 Introducción al aprendizaje automático El aprendizaje automático se define como el campo de estudio que da a las computadoras la capacidad de aprender sin ser programadas explícitamente (Arthur Samuel, 1959). Formalmente, un algoritmo de machine learning mejora su desempeño en una tarea \\(T\\), medido por una métrica \\(P\\), a medida que adquiere más experiencia \\(E\\). 1.1.1 Motivación: Los paradigmas del conocimiento A lo largo de la historia, la humanidad ha transitado por distintas formas de generar y utilizar conocimiento: Era empírica: basada en la observación directa y la experiencia. Era teórica: dominada por modelos matemáticos y leyes físicas (por ejemplo, la mecánica newtoniana). Era computacional: donde simulaciones numéricas permiten explorar sistemas complejos. Era de los datos: en la que los patrones emergen no de ecuaciones, sino de grandes volúmenes de datos observados. Para entender un poco más sobre estos paradigmas del conocimiento un paper muy interesante que estaba leyendo hace un tiempo “Machine learning in concrete science: applications, challenges, and best practices” habla acerca de la evolución de la ciencia del concreto en estos paradigmas y la relevancia exponencial que han jugado los algoritmos de aprendizaje automatico al hacer nuevos descubrimientos en estos campos. Es en esta última era donde el machine learning cobra protagonismo: en lugar de programar reglas explícitas, enseñamos a las máquinas a descubrir patrones a partir de ejemplos. 1.1.2 Machine learning supervisado Los problemas de machine learning se clasifican según la naturaleza de los datos y la supervisión disponible: 1.1.2.1 Aprendizaje supervisado Se dispone de pares entrada-salida \\((x, y)\\). Subtipos comunes incluyen: - Regresión: predecir una variable continua (ej. precio de una casa). - Clasificación: asignar una etiqueta discreta (ej. detección de spam). - Aprendizaje de secuencias: modelar datos ordenados en el tiempo o en secuencia (ej. traducción automática, reconocimiento de voz). 1.1.2.2 Componentes del aprendizaje supervisado En el aprendizaje supervisado participan los siguientes componentes. Función desconocida: \\(f: X-&gt;Y\\) Es la relación que queremos aprender entre la entrada \\(X\\) y las salidas \\(Y\\) Muestras de entrenamiento (data): Es el conjunto de pares observados \\((x_n,y_n)\\) Conjunto de hipótesis: Denotado como \\(H\\) es el conjunto de funciones candidatas que el algoritmo puede elegir para aproximar f. Algoritmo de aprendizaje: Denotado como \\(A\\) Toma como entrada los ejemplos de entrenamiento y el conjunto de hipotesis para seleccionar una \\(g \\in H\\) que mejor se ajuste a los datos. Hipotesis final: Denotada como \\(g \\approx f\\) que es la función aprendida por el algoritmo 1.1.2.3 Tipos de datos en el aprendizaje supervisado Si bien los algoritmos de aprendizaje automático operan exclusivamente con números, la estructura y el tipo de las variables (por ejemplo, categóricas, continuas, ordinales, etc.) influyen directamente en la elección del modelo más adecuado para capturar patrones y realizar predicciones efectivas. 1.1.2.3.1 Númericos También conocido como datos cuantitativos, refleja la cuantización de algo medible y codificable numéricamente donde existe una relación de orden total. 1.1.2.3.2 Categóricos Los datos categóricos se utilizan para etiquetar características que no son medibles, conocido como datos cualitativos. Por lo general se emplean números como etiquetas sin que estos tengan una relación de orden total en su significado. A menudo un mismo atributo lo podemos representar de diversas manera y la representación que elegimos podrá ser de diferente indole. 1.1.2.3.3 Series de tiempo Son una secuencia de números coleccionados con un intervalo regular de tiempo sobre un periodo de tiempo. En este tipo de conjunto de datos las muestras pueden estar asociadas entre si. Diversos objetos que solemos representar en este mundo como imagenes, videos o audios se pueden describir con los tipos que definimos anteriormente. 1.1.2.4 Los problemas de machine learning Los dos desafíos fundamentales que definen la naturaleza de los problemas en machine learning: 1.1.2.4.1 El problema inverso A diferencia de la física, donde las leyes predicen observaciones, en machine learning partimos de observaciones para inferir las reglas subyacentes. Esto es inherentemente ambiguo y puede derivar en un gran número de problemas. Por listar algunos tenemos. Falta de información: Se intentan identificar patrones para intentar disminuir el error basado en alguna regla, pero no hay garantia de que la regla que se este intentando cumplir sea la correcta para representar al sistema. No unicidad de la solución (información inconsistente): Dado un resultado observado \\(y\\) , puede haber múltiples configuraciones de entrada \\(x_1 x_2, ...\\) tales que \\(f(x_i) = y\\) 1.1.2.4.2 Generación del modelo que aproxime al sistema Falta de modelo directo preciso : En muchos casos, la función directa \\(f\\) (que mapea causas a efectos) no se conoce analíticamente, sino que se aproxima mediante simulaciones o modelos empíricos. Esto tambien se puede por no tener el conjunto de hipotesis adecuado. Problema de optimización: el aprendizaje se formula como la búsqueda de parámetros que minimicen una función de pérdida a mayor número de parámetros a optimizar es menos probable obtener un minimo global, lo cual se puede realizar tambien con el problema “the curse of dimensionality” 1.2 ¿Por qué Deep Learning? Una pregunta que nos podriamos hacer es ¿por qué deep learning? en esta sección abordaremos una perspectiva de contraste e historica para intentar acercarnos a esta respuesta. 1.2.1 Dimensión VC La dimensión de Vapnik-Chervonenkis (VC) es una medida de la capacidad de un modelo de clasificación (o conjunto de hipótesis) para distinguir entre diferentes clases. Formalmente, la dimensión VC de un conjunto de hipótesis \\(H\\) se define de la siguiente manera: Sea \\(H\\) un conjunto de hipótesis; y \\(X\\) un conjunto de instancias (espacio de entrada). La dimensión \\(VC\\) de \\(H\\) es el tamaño del mayor subconjunto de \\(X\\) que puede ser destrozado (shattering) por \\(H\\) Shattering: Un conjunto de puntos \\(S={x1,x2,…,xd}⊆X\\), se dice que es shattered por \\(H\\) si, para cada posible partición binaria de \\(S\\) (es decir, cada una de las posibles asignaciones de etiquetas en \\(S\\)), existe una hipótesis \\(h \\in H\\) tal que \\(h\\) clasifica correctamente todos los puntos de \\(S\\) según esa partición. En otras palabras VC Dimension mide la capacidad de un modelo para ajustarse a conjuntos arbitrarios de datos. Cuanto mayor sea la dimensión VC, más complejos son los patrones que puede aprender, pero también mayor el riesgo de sobreajuste (overfitting). Es importante que el conjunto de puntos se encuentre en posición general, para evitar configuraciones precisas que se ajuntes de forma exacta a la forma del conjunto de hipotesis. Algo importante a remarcar es que el VC-Dimension como su nombre lo indica representa la capacidad en una instancia númerica basada en el número de dimensiones de los puntos a separar, así que si el número de dimensiones incrementa, el número de puntos maximos a clasificar correcto tambien lo hace. 1.2.2 El truco del Kernel (SVM) Una SVM (Máquina de Vectores de Soporte) busca separar las clases de datos encontrando una frontera óptima (hiperplano) que las divida. Sin embargo, cuando los datos no son separables linealmente en su espacio original, utiliza el truco del kernel, que consiste en proyectarlos implícitamente a un espacio de mayor dimensión donde sí puedan separarse mediante una frontera lineal. De esta forma, sin transformar los datos directamente, la SVM aplica funciones como los kernels polinomial o RBF para capturar relaciones no lineales y lograr una mejor clasificación. Sin embargo, encontrar el kernel adecuado presenta desafíos como la necesidad de probar distintas funciones, ajustar cuidadosamente sus parámetros para evitar sobreajuste o subajuste, enfrentar altos costos computacionales en grandes volúmenes de datos y lidiar con la menor interpretabilidad del modelo al trabajar en espacios de alta dimensión. 1.2.3 Ingeniería de caracteristicas La extracción de características ayuda a resolver problemas de aprendizaje automático al transformar los datos brutos en representaciones más relevantes y compactas que facilitan el trabajo del modelo. Este proceso permite destacar la información más útil, eliminar ruido y reducir la dimensionalidad, lo que mejora la precisión, eficiencia y capacidad de generalización del algoritmo. Al enfocarse en las propiedades más representativas de los datos —por ejemplo, patrones visuales en imágenes, frecuencias en audio o variables derivadas en datos tabulares—, la extracción de características permite que los modelos aprendan de manera más efectiva y requieran menos recursos para lograr un buen desempeño. La extracción de caracteristicas resulta especialmente util para combinar variables que por si mismo no aportan tanto valor al describir un sistema, pero combinadas de cierta forma tiene valor. Por ejemplo el peso y la estatura combinada pueden crear el IMC que es un indicador clave a la hora de evaluar el sobrepeso. 1.2.4 Beneficios del deep learning 1.2.4.1 Teorema de aproximación universal Establece que una red neuronal con al menos una capa oculta y un número suficiente de neuronas puede aproximar cualquier función continua en un espacio de dimensión finita con un error arbitrariamente pequeño. En otras palabras, las redes neuronales son aproximadores universales. Este teorema proporciona una base teórica para la capacidad de las redes neuronales de aprender y representar funciones complejas. Sin embargo, en la práctica, el teorema no garantiza que una red entrenada encontrará la aproximación óptima, ni especifica cuántas **neuron “Jugemos un poco con las redes multicapa” 1.2.4.2 Extracción automática de características (feature extraction): A diferencia de los métodos tradicionales, donde los ingenieros diseñaban manualmente características (ej. bordes, texturas, frecuencias), las redes profundas aprenden representaciones útiles directamente de los datos brutos. Cada capa construye representaciones de mayor nivel a partir de las anteriores (píxeles → bordes → formas → objetos). Tipo de capa Rol / función típica Qué tipo de representaciones aprende Apoyo en la literatura / ejemplos Primeras capas(cercanas a la entrada) Capturan patrones de bajo nivel y locales del dato bruto Bordes, texturas, frecuencias básicas, características locales Se habla de “aprendizaje jerárquico de características”, donde los niveles inferiores capturan elementos simples primero. (OpenStax) Capas intermedias(ocultas medias) Combinan características simples en representaciones más abstractas Motivos, formas, combinaciones de las características de los niveles más bajos El trabajo “Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination” estudia cómo las capas progresivas comprimen características dentro de clase y discriminan entre clases a medida que avanza la profundidad. (arXiv) Últimas capas(cercanas a la salida) Transforman las representaciones abstractas hacia la decisión o la salida específica de la tarea Conceptos de alto nivel, clases, predicciones finales En redes de reconocimiento de imágenes, las capas finales “interpretan” las características abstractas en etiquetas o decisiones. Por ejemplo, arquitecturas como AlexNet usan capas finales totalmente conectadas para clasificar objetos. (Wikipedia) “How transferable are features in deep neural networks?” 1.2.4.3 Escalabilidad con datos y cómputo Mientras que muchos algoritmos clásicos saturan su rendimiento con más datos, el deep learning mejora continuamente al aumentar el tamaño del conjunto de entrenamiento y la capacidad computacional. "],["preliminares.html", "Capítulo 2 Preliminares 2.1 Algebra Lineal 2.2 Cálculo Diferencial", " Capítulo 2 Preliminares 2.1 Algebra Lineal 2.1.1 Introducción El álgebra lineal es la rama de las matemáticas que estudia los espacios vectoriales y transformaciones lineales así como los elementos que son parte de estas como son los vectores, matrices, bases, operadores así como las propiedades geométricas que ocurren en los espacios vectoriales. Enfocandose en como se representan y manipulan los datos que pueden tener múltiples dimensiones o características de una forma estructurada y dando así, interpretaciones de estos con una serie de observaciones, estructura y rigor matemático. 2.1.2 Motivación El aprendizaje profundo se basa en procesar y transformar datos numéricos (imágenes, sonidos, textos, etc) y dichas transformaciones son operaciones lineales y no lineales aplicadas repetidamente, encontramos una forma muy natural de hacer uso del álgebra lineal para el aprendizaje profundo, representado como el procesamiento de redes neuronales. De forma natural, podemos ver como las entradas, salidas, neuronas y funciones de activación hacen uso de estructuras como vectores, matrices y transformaciones lineales: 2.1.2.1 Vectores Representan entradas, salidas y parámetros. 2.1.2.2 Matrices Representan conexiones entre neuronas así como transformaciones entre espacios. Esto porque una red neuronal puede expresarse como \\[ y = Wx + b \\] donde: \\(x\\) es el vector de entrada, \\(W\\) es una matriz de pesos. \\(b\\) es el sesgo, \\(y\\) es la salida. 2.1.2.3 Transformaciones lineales Cada capa lineal en una red neuronal transforma el espacio de las entradas. Estas transformaciones cambian orientación, posición, o escala de los datos en un espacio multidimensional. 2.1.3 Escalares 2.1.3.1 Campos Algebraicos Un campo algebraico (o simplemente campo) es una estructura matemática formada por un conjunto de elementos en el que se pueden realizar las operaciones de suma, resta, multiplicación y división (excepto por cero), cumpliendo ciertas reglas de consistencia. Algunos ejemplos de campos algebraicos son los números reales (denotado por \\(\\mathbb{R}\\)) y los números complejos (denotado por \\(\\mathbb{C}\\)). En este contexto, cuando hablamos de escalares, nos referiremos a un elemento del campo sobre el que trabajamos, usualmente sobre los números reales, o en ocasiones particulares, números imaginarios. 2.1.3.2 Dato curioso Si en lugar de un campo tenemos un anillo algebraico (a diferencia del campo, no necesariamente existe un inverso multiplicativo), entonces los escalares perteneceran al anillo, y en lugar de tener un espacio vectorial, diremos que tendremos un módulo y eso es campo de estudio de otra rama de las matemáticas. Por lo que en este contexto, únicamente trataremos con los números reales como campo y posiblemente con complejos. 2.1.4 Vectores Un vector es un objeto matemático que posee magnitud y dirección. Estos pertenecen a un espacio vectorial que está definido sobre un campo y por ende, las entradas de un vector, son elementos de este campo (denotado como escalares), así como dichas entradas es una lista ordenada. \\[ v = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix} \\] Estos números son las coordenadas del vector respecto a la base del espacio. 2.1.5 Matrices Una matríz es un arreglo rectangular de escalares (o con entradas en el campo vectorial), organizada en filas y columnas de la siguiente forma: \\[ A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\dots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\dots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\dots &amp; a_{mn} \\end{pmatrix} \\] donde \\(A\\) tiene \\(m\\) filas y \\(n\\) columnas, y se dice que es de tamaño \\(m \\times n\\). 2.1.5.1 Alternativas conceptuales para entenderlas Una matríz puede entenderse como: Un conjunto de vectores organizados de manera estructurada. Una herramienta para transformar un vector en otro mediante operaciones lineales. Una representación compacta de un sistema de ecuaciones lineales. Por dar un ejemplo, la expresión \\(y = Ax\\) con \\(x, y\\) como vectores y \\(A\\) como matríz, denota como se transforma \\(x\\) en \\(y\\) por medio de \\(A\\), o un sistema de ecuaciones lineales. 2.1.5.2 Propiedades matemáticas Las matrices permiten realizar: Suma y multiplicación de transformaciones lineales. Cálculo de determinante e inversas de matrices (en caso de existir). Cambios de base, rotaciones, escalamientos y proyecciones. En esencia podemos pensar que son la forma algebraica de expresar operaciones lineales entre espacios vectoriales. 2.1.5.3 En el aprendizaje profundo Las matrices son un componente clave en el núcleo operativo de las redes neuronales considerando las siguientes observaciones: Cada capa lineal se representa por una matríz de pesos que transforma las entradas en salidas. Las operaciones de multiplicación matricial permite combinar miles de variables de entrada con miles de parámetros a la vez. Durante el entrenamiento, las matrices cambian, siendo ajustadas para minimizar el error de predicción. 2.1.5.4 Resumen Las matrices son arreglos ordenados de escalares o vectores que suelen representar transformaciones lineales así como bases vectoriales. Esta es la forma matemática que se usa para describir el procesamiento y transformación numérica que ocurre en una red neuronal. 2.1.6 Tensores De forma simétrica a las estructuras de escalares, vectores y matrices, un tensor es una estructura matemática que generaliza estos conceptos para describir relaciones lineales y multidimensionales entre conjuntos de datos numéricos. Una forma de ejemplificar usando como base la relación entre puntos, líneas, cuadrados y cubos puede ser la siguiente: una matríz es a un tensor (arreglo de matrices), como un vector es a una matríz (arreglo de vectores), como un escalar es a un vector (arreglo de escalares). 2.1.6.1 Intuición geométrica Un tensor puede verse como un objeto que transforma o relaciona vectores y covectores de manera multilineal, conservando coherencia bajo cambios de coordenadas. Esto en espacios físicos permite describir propiedades como: Fuerza y dirección (vectores). Tensiones, deformaciones o inercia (tensores de segundo orden). Curvaturas o transformaciones complejas (tensores de orden superior). 2.1.6.2 Su rol en el aprendizaje profundo En el contexto del aprendizaje profundo, los tensores suelen usarse en un sentido computacional más que geométrico. Por ejemplo: Una imagen puede representarse como un arreglo dimensional de dos dimensiones, pero donde sus elementos son vectores de dimensión 3 (pixeles con sus 3 colores). Un lote de imágenes forma un tensor de orden 4: número o etiqueta de la imagen, alto, ancho y colores. Los pesos de una red neuronal y sus gradientes también son tensores. 2.1.6.3 Propiedades básicas de la aritmética de tensores Escalares, vectores, matrices y tensores de orden superior tienen propiedades que son útiles, por ejemplo operaciones elemento a elemento producen salidas que tienen las mismas dimensiones que sus entradas. El producto de elemento a elemento de dos matrices es llamado producto Hadamard y es denotado con \\(\\odot\\). Podemos ver esto de la siguiente forma para dos matrices \\(A, B \\in \\mathbb{R}^{m \\times n}\\): \\[ A \\odot B = \\begin{pmatrix} a_{11} b_{11} &amp; a_{12} b_{12} &amp; \\dots &amp; a_{1n} b_{1n} \\\\ a_{21} b_{21} &amp; a_{22} b_{22} &amp; \\dots &amp; a_{2n} b_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} b_{m1} &amp; a_{m2} b_{m2} &amp; \\dots &amp; a_{mn} b_{mn} \\end{pmatrix} \\] 2.1.7 Reducciones sobre tensores Una reducción es una operación que toma un tensor y “reduce” una o más de sus dimensiones, combinando los elementos mediante una operación como la suma, el promedio, máximo o cualquier otra función agregadora 2.1.7.1 Ejemplos Suponiendo que tenemos la siguiente matriz \\[ A = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{pmatrix} \\] Tenemos las siguientes posibilidades de sumas: Suma total: \\(\\mbox{sum}(A) = 1 + 2 + 3 + 4 + 5 + 6 = 21\\) Suma por filas: \\(\\mbox{sum}(A, \\mbox{axis} = 1) = [6, 16]\\) que es un vector (tensor de orden 1).# Suma por columnas: \\(\\mbox{sum}(A, \\mbox{axis} = 0) = [5, 7, 9]\\) 2.1.7.2 Intuición geométrica Una reducción colapsa una dirección del espacio del tensor. Dada una matriz, reducir sobre las filas equivale a proyectar todo el espacio de datos sobre el eje de las columnas. En tensores de orden mayor, esto trata de aplanar parte de la estructura multidimensional. 2.1.7.3 En aprendizaje profundo Las reducciones son operaciones fundamentales en el cálculo dentro de las redes neuronales. Consideremos las siguientes: Función de pérdida: Se calcula reduciendo las diferencias entre predicciones y etiquetas. Por ejemplo: \\[ \\mbox{loss} = \\mbox{mean}((y_{\\mbox{pred}} - y_{\\mbox{true}})^2) \\] reducción con media sobre todos los ejemplos. Normalización: Reducir sobre ciertas dimensiones (por ejemplo, el canal o lote) para calcular medias y desviaciones estándar. Gradientes: Durante el entrenamiento, los gradientes a menudo se acumulan (se reducen) sobre los lotes de datos. 2.1.7.4 Tipos comunes de reducciones Operación Resultado Uso típico suma Suma de elementos Energía total, acumulaciones promedio Promedio Pérdidas, normalización máximo, mínimo Valor extremo Selección, agrupamiento, submuestreo producto Producto total Escalados, combinaciones norma Magnitud del tensor Regularización, análisis geométrico 2.1.7.5 Resumen Podemos pensar en la reducción como el proceso de combinar las entradas a lo largo de ciertas dimensiones, para producir una representación más simple o resumida del tensor original. Matemáticamente, una reducción es una contracción parcial de índices y computacionalmente, una operación de agregación que permite manejar de forma eficiente grandes volúmenes de datos en redes neuronales. 2.1.8 Producto punto De momento hemos estado revisando operaciones tanto de reducción como operaciones elemento a elemento. Pero para el producto punto, tenemos una combinación muy interesante. Ya que por un lado, es una operación binaria que toma dos vectores, y nos regresa una agregación, siendo esta la suma total del producto elemento a elemento. O más matemáticamente, sea \\(d \\in \\mathbb{N}\\), dado \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d\\) y \\(\\mathbf{x}^\\top\\) la transpuesta de \\(\\mathbf{x}\\), entonces su producto punto \\(\\mathbf{x} \\cdot \\mathbf{y}\\) ó \\(\\mathbf{x}^\\top \\mathbf{y}\\) (o también conocido como producto interno \\(\\langle \\mathbf{x}, \\mathbf{y} \\rangle\\)) se define así: \\[ \\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i \\] Este resultado sin embargo, tiene repercusiones importantes así como interpretaciones geométricas de gran importancia. A este producto también se puede ver como una forma bilineal, es decir, que en ambas entradas, es linear (abre sumas y saca escalares: \\(f(ax + by) = af(x) + bf(y)\\)). 2.1.8.1 Interpretación geométrica El producto punto mide cuanto apunta un vector en la dirección de otro o el ángulo que hay entre estos: \\[ \\mathbf{a} \\cdot \\mathbf{b} = \\|a\\| \\|b\\| \\cos(\\theta) \\] donde: Las longitudes de los vectores están representadas por \\(\\|a\\|\\) y \\(\\|b\\|\\), el ángulo entre estos es \\(\\theta\\). Si: Apuntan en direcciones similares: \\(\\mathbf{a} \\cdot \\mathbf{b} &gt; 0\\). Son perpendiculares \\(\\mathbf{a} \\cdot \\mathbf{b} = 0\\). Apuntan en direcciones opuestas: \\(\\mathbf{a} \\cdot \\mathbf{b} &lt; 0\\). 2.1.8.2 En el aprendizaje profundo El producto punto aparece de forma constante en redes neuronales: En cada neurona, el valor de activación se calcula como un producto punto entre el vector de entrada y el vector de pesos: \\[ z = \\mathbf{w} \\cdot \\mathbf{x} + b \\] En el contexto de atención (transformers), se usa para medir la similitud entre representaciones: \\[ \\mathbf{q} \\sim \\mathbf{k} = \\mbox{sim}(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q} \\cdot \\mathbf{k} \\] 2.1.8.3 Resumen El producto punto es la operación fundamental para medir la relación de direcciones y magnitud entre dos vectores, y en redes neuronales, es la base matemática de como las neuronas evalúan la información que procesan. 2.1.9 Producto entre matrices y vectores Para definir el producto entre matrices y vectores, tomemos \\(m, n \\in \\mathbb{N}\\), una matriz \\(A \\in \\mathbb{R}^{n \\times m}\\) y un vector \\(x \\in \\mathbb{R}^m\\), entonces lo definiremos de la siguiente forma: \\[ A\\cdot x = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1m} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nm} \\\\ \\end{pmatrix} \\cdot \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{pmatrix} = \\begin{pmatrix} a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1m}x_m \\\\ a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2m}x_m \\\\ \\vdots \\\\ a_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nm}x_m \\\\ \\end{pmatrix} \\] Si recordamos que \\(A\\) puede verse como un arreglo o lista de vectores \\(n\\) vectores \\(\\mathbf{a}_j\\) con \\(j \\in {1, \\cdots, n}\\), entonces lo anterior lo podemos reescribir de la siguiente forma: \\[ A \\cdot x = \\begin{pmatrix} \\mathbf{a}_1^\\top \\cdot x \\\\ \\mathbf{a}_2^\\top \\cdot x \\\\ \\vdots \\\\ \\mathbf{a}_n^\\top \\cdot x \\\\ \\end{pmatrix} = \\begin{pmatrix} \\mathbf{a}_1^\\top \\\\ \\mathbf{a}_2^\\top \\\\ \\vdots \\\\ \\mathbf{a}_n^\\top \\\\ \\end{pmatrix} \\cdot x \\] donde podemos ver como esta multiplicación matricial es una transformación que proyecta vectores de \\(\\mathbb{R}^m\\) hacia \\(\\mathbb{R}^n\\) (viendo como toma \\(x\\), y lo transforma en \\(A\\cdot x\\)). 2.1.9.1 Interpretación geométrica Las transformaciones que puede hacer \\(A\\) sobre \\(x\\) son: Rotarlo, escalarlo, reflejarlo o cambiar su dimensión. Si \\(A\\) es una matriz cuadrada (\\(A \\in \\mathbb{R}^{n \\times n}\\)), entonces la transformación es hacia el mismo espacio Cuando \\(A\\cdot x = 0\\), hablaremos del núcleo de la transformación (\\(\\ker(A)\\)), o espacio nulo y este será todos los vectores \\(x\\) tales que satisfacen esa condición. 2.1.9.1.1 Teorema del rango y nulidad El teorema del rango y nulidad nos habla de la relación que existe entre el núcleo y la imagen de \\(A\\): Sea \\(A: V \\rightarrow W\\) una transformación lineal entre espacios vectoriales de dimensión finita, donde: \\(\\dim(V) = n\\). \\(\\mbox{rango}(A)\\) es la imagen (también llamada rango), \\(\\ker(A)\\) es el núcleo o kernel (también llamado espacio nulo). entonces: \\[ \\dim(V) = dim(\\mbox{rango(A)}) + \\dim(\\ker(A)) \\] o de forma equivalente: \\[ n = \\mbox{rango}(A) + \\ker(A) \\] 2.1.9.2 En aprendizaje profundo En una neurona o capa lineal, el cálculo principal es precisamente este producto: \\[ \\mathbf{y} = A\\mathbf{x} + \\mathbf{b} \\] donde los elementos son los siguientes: vector de entradas: \\(\\mathbf{x}\\), matriz de pesos: \\(A\\), vector de sesgos: \\(\\mathbf{b}\\), salida o activación previa: \\(\\mathbf{y}\\). Cada multiplicación entre matriz y vector permite a la red combinar las características y extraer patrones de los datos, siendo una de las operaciones más repetidas en todo el aprendizaje profundo. En relación al teorema de rango y nulidad en el contexto de redes neuronales, podemos ver lo siguiente: Qué parte de la información de entrada puede conservarse (rango). Qué parte se pierde o colapsa (nulidad). Como el modelo puede aprender representaciones comprimidas al pasar de un espacio con alta dimensión a otro más pequeño. 2.1.9.3 Resumen El producto de matriz con vector aplica una transformación lineal al vector, combinando sus componentes mediante productos punto con las filas de la matriz. Esta es la base matemática de como las redes neuronales procesan la información y generan nuevas representaciones. En conjunto con el teorema de rango y nulidad, podemos ver que en toda transformación lineal, la suma del número de direcciones que se conservan y las que se anulan es igual a la dimensión del espacio original. 2.1.10 Multiplicación matricial Siguiendo el patrón de partir de lo particular y empezar a generalizar, y teniendo presente que una matriz es un arreglo ordenado de vectores, naturalmente podemos preguntarnos por el producto matricial, y respondernos de forma directa. Sean \\(n, k, m \\in \\mathbb{N}\\) y dos matrices \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times k}\\) y \\(\\mathbf{B} \\in \\mathbb{R}^{k \\times m}\\) con la estructura que ya conocemos: \\[ \\begin{split}\\mathbf{A}=\\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1k} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nk} \\\\ \\end{pmatrix},\\quad \\mathbf{B}=\\begin{pmatrix} b_{11} &amp; b_{12} &amp; \\cdots &amp; b_{1m} \\\\ b_{21} &amp; b_{22} &amp; \\cdots &amp; b_{2m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{k1} &amp; b_{k2} &amp; \\cdots &amp; b_{km} \\\\ \\end{pmatrix}.\\end{split} \\] Sea \\(\\mathbf{a}_i^\\top \\in \\mathbb{R}^k\\) el \\(i\\)-ésimo vector fila de la matriz \\(\\mathbf{A}\\) y sea \\(\\mathbf{b}_j \\in \\mathbb{R}^k\\) el \\(j\\)-ésimo vector columna de \\(\\mathbf{B}\\), entonces: \\[ \\begin{split}\\mathbf{A}= \\begin{pmatrix} \\mathbf{a}^\\top_{1} \\\\ \\mathbf{a}^\\top_{2} \\\\ \\vdots \\\\ \\mathbf{a}^\\top_n \\\\ \\end{pmatrix}, \\quad \\mathbf{B}=\\begin{pmatrix} \\mathbf{b}_{1} &amp; \\mathbf{b}_{2} &amp; \\cdots &amp; \\mathbf{b}_{m} \\\\ \\end{pmatrix}.\\end{split} \\] Para formar el producto matricial \\(\\mathbf{C} \\in \\mathbb{R}^{n \\times m}\\), vamos a calcular el elemento \\(c_{ij}\\) como el producto punto entre la \\(i\\)-ésima fila de \\(\\mathbf{A}\\) y la \\(j\\)-ésima columna de \\(\\mathbf{B}\\), es decir \\(\\mathbf{a}_i^\\top \\mathbf{b}_j\\): \\[ \\begin{split}\\mathbf{C} = \\mathbf{AB} = \\begin{pmatrix} \\mathbf{a}^\\top_{1} \\\\ \\mathbf{a}^\\top_{2} \\\\ \\vdots \\\\ \\mathbf{a}^\\top_n \\\\ \\end{pmatrix} \\begin{pmatrix} \\mathbf{b}_{1} &amp; \\mathbf{b}_{2} &amp; \\cdots &amp; \\mathbf{b}_{m} \\\\ \\end{pmatrix} = \\begin{pmatrix} \\mathbf{a}^\\top_{1} \\mathbf{b}_1 &amp; \\mathbf{a}^\\top_{1}\\mathbf{b}_2&amp; \\cdots &amp; \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\ \\mathbf{a}^\\top_{2}\\mathbf{b}_1 &amp; \\mathbf{a}^\\top_{2} \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots\\\\ \\mathbf{a}^\\top_{n} \\mathbf{b}_1 &amp; \\mathbf{a}^\\top_{n}\\mathbf{b}_2&amp; \\cdots&amp; \\mathbf{a}^\\top_{n} \\mathbf{b}_m \\end{pmatrix}.\\end{split} \\] De esta forma, podemos pensar en el producto matricial como realizar \\(m\\) productos entre matrices y vectores o \\(m \\times n\\) productos punto y recolectandolos todos para formar una matriz de \\(n \\times m\\). Cabe mencionar que este no es el producto Hadamard mencionado previamente. 2.1.10.1 Interpretación geométrica Dado que cada elemento del producto matricial, es el producto punto entre las filas de la matriz de la izquierda y las columnas de la matriz de la derecha, entonces el producto punto puede verse como una composición de transformaciones lineales: \\[ (AB)x = A(Bx). \\] Esto significa que aplicar primero \\(B\\) y luego \\(A\\) a un vector es equivalente a aplicar una sola transformación representada por \\(AB\\). 2.1.10.2 Propiedades importantes Sean \\(A, B, C\\) matrices con las dimensiones compatibles para el producto matricial, hay algunos puntos muy importantes que se tienen que mencionar: Las matrices no necesariamente conmutan: \\(AB \\neq BA\\). Las matrices son asociativas: \\((AB)C = A(BC)\\). Las matrices son distributivas: \\(A(B+C) = AB + AC\\). Compatibilidad con escalares: \\((\\alpha A)B = A(\\alpha B) = \\alpha(AB)\\) para \\(\\alpha\\) escalar. 2.1.10.3 En el aprendizaje profundo El producto matricial es la operación más esencial en el cálculo de redes neuronales: Cada capa lineal o densa se expresa como: \\[ \\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{B} \\] donde \\(\\mathbf{A}\\) (matriz de pesos) multiplica la matriz o vector de entrada \\(\\mathbf{X}\\). En redes convolucionales, transformadores o encajes vectoriales, los productos matriciales generalizan a productos tensoriales u operaciones de atención. 2.1.10.4 Producto matricial como cambio de base del espacio vectorial Una interpretación adicional al producto matricial es la de cambio de base del espacio vectorial y esto puede verse de la siguiente forma 2.1.10.5 Resumen El producto entre dos matrices es la combinación de filas y columnas entre la primera y segunda matriz para formar una nueva matriz, que representa la composición de transformaciones lineales: una idea central en álgebra lineal y por lo tanto, en aprendizaje profundo. 2.1.11 Normas Una norma es una función que asigna a cada vector un número real no negativo que representa su longitud, magnitud o tamaño dentro de un espacio vectorial. Descrito de forma matemática, sea \\(V\\) un espacio vectorial sobre un campo \\(\\mathbb{R}\\) ó \\(\\mathbb{C}\\). Una forma es una función tal que: \\[ \\|\\cdot\\|:V \\rightarrow \\mathbb{R}_{\\ge0} \\] que a cada vector \\(v \\in V\\) le asigna el número \\(\\|v\\|\\), cumpliendo las siguientes tres propiedades: 1. No negatividad y definitud: \\[ \\|v\\| \\ge 0, \\space \\mbox{y} \\space \\|v\\| = 0 \\Leftrightarrow v = 0 \\] (la longitud nunca es negativa y solamente es cero con el vector cero). 2. Homogeneidad (o multiplicación por escalar): \\[ \\|\\alpha v\\| = |\\alpha| \\|v\\| \\] (Escalar un vector cambia su longitud en el mismo factor absoluto). 3. Desigualdad triangular: \\[ \\|u + v\\| \\le \\|u\\| + \\|v\\| \\] (la longitud de la suma nunca excede la suma de las longitudes, como ocurre en un triángulo). 2.1.11.1 Ejemplos de normas Sea \\(V\\) un espacio vectorial de dimensión \\(n \\in \\mathbb{N}\\), con entradas en los reales o complejos, y sea \\(v \\in V\\) un vector, entonces las siguientes normas están definidas de la siguiente forma 2.1.11.1.1 Norma Euclidiana (o \\(L^2\\)): \\[ \\|v\\|_2 = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} \\] Observa que: \\[ \\langle v, v \\rangle= v^\\top \\cdot v = v_1^2 + v_2^2 + \\cdots + v_n^2 = \\|v\\|_2^2, \\] por lo tanto: \\[ \\|v\\|_2 = \\sqrt{\\langle v, v\\rangle} = \\sqrt{v^\\top \\cdot v}. \\] 2.1.11.1.2 Norma Manhattan (o \\(L^1\\)) \\[ \\|v\\|_1 = |v_1| + |v_2| + \\cdots + |v_n| \\] Esta norma también es conocida como la norma del taxista. 2.1.11.1.3 Norma máxima (\\(L^{\\infty}\\)) \\[ \\|v\\|_{\\infty} = \\max_{i}|v_i| \\] 2.1.11.1.4 Norma Frobenius En el caso de las matrices, la situación es un poco más compleja. Esto porque las matrices pueden ser vistas como arreglos de vectores, o arreglos de números, al mismo tiempo que son transformaciones lineales. Por ejemplo, podríamos preguntarnos cuál es la relación de distancia para un producto de matriz y vector \\(Xv\\) con relación a \\(v\\). Esta línea de pensamiento nos lleva a algo llamado norma espectral. Pero por ahora presentamos la norma Frobenius, la cuál es mucho más fácil de calcular y queda definida de la siguiente forma \\[ \\|X\\|_F = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}x_{ij}^2}. \\] Esta norma se comporta como la norma \\(L^2\\) para vectores, pero en caso matricial. 2.1.11.2 Interpretación geométrica Una norma define la noción de la distancia y magnitud (o topología) dentro del espacio vectorial. Diferentes normas producen distintas geometrías del espacio (círculos, cuadrados, octágonos, etc). 2.1.11.3 En aprendizaje profundo Las normas son esenciales para: Medir errores (como ejemplo, usando la norma \\(L^2\\) para el error cuadrático medio). Regularización de modelos (normas como \\(L^1\\) y \\(L^2\\) reducen el sobreajuste penalizando los pesos grandes). Normalizar datos o vectores de características, para estabilizar el entrenamiento y mejorar la interpretación de la similitud entre vectores. 2.1.11.4 Resumen Una norma mide la longitud o tamaño de un vector de forma consistente con las reglas del espacio vectorial, permitiendo cuantificar distancias, magnitudes y relaciones entre vectores. 2.2 Cálculo Diferencial 2.2.1 Introducción Él cálculo diferencial es la rama de la matemática que surge para estudiar los cambios que ocurren en la naturaleza representado como fenómenos dinámicos, partiendo de las leyes Newtonianas estudiando el cambio de movimiento para definir así la velocidad, y posteriormente el cambio de la velocidad para entender la aceleración. En el contexto del aprendizaje profundo, este tema es de vital importancia ya que las redes neuronales se calibran optimizando funciones mediante la medición y ajuste de cambios infinitesimales usando conceptos como mínimos locales y globales por medio de un concepto llamado gradiente, minimizando algo conocido como función de pérdida y recurriendo a técnicas modernas como la autodiferenciación para optimizar estas funciones usando la propagación hacia atrás. Podemos pensar que una red neuronal aprende observando como las variaciones pequeñas en los pesos afectan el error o la función de pérdida. Esta relación, se describe con derivadas y gradientes. De esta forma, el cálculo diferencial proporciona el marco de trabajo para: comprender como fluye la información y el error a través de la red. Analizar la convergencia y estabilidad de los algoritmos de optimización. Desarrollar modelos más eficientes y estables desde el punto de vista numérico y teórico. 2.2.2 Motivación La motivación directa es precisamente el optimizar las funciones de pérdida que están relacionadas con los pesos de las redes neuronales para mejorar como desempeña nuestra red neuronal. 2.2.3 Derivadas y diferenciación Para proceder con las derivadas, primero definiremos rápidamente (sin entrar en tanto detalle) el concepto de función para una variable y posteriormente, como función de múltiples variables. 2.2.3.1 Funciones Sean \\(X, Y\\) dos conjuntos, una función es una relación que existe entre dos conjuntos, de tal forma que para \\(x \\in X\\) y \\(y \\in Y\\), \\(f(x) = y\\). Esto podemos pensarlo como una regla o mapeo para relacionar ambos conjuntos y lo denotamos como \\(f: X \\rightarrow Y\\). Para el cálculo de una variable, usualmente nuestro dominio será algún subconjunto \\(X\\) de los números reales, y este estará relacionado con otro número real que de momento diremos pertenecerá a un subconjunto \\(Y\\) de los reales, el cuál será el contradominio. Es decir: \\(f: X \\subset\\mathbb{R} \\rightarrow Y \\subset\\mathbb{R}\\). Para cuando trabajemos con cálculo vectorial, diremos que el dominio será precisamente un espacio vectorial \\(\\mathbb{R}^n\\), para \\(n \\in \\mathbb{N}\\). Pero el contradominio será en los números reales. Es decir, las funciones para el cálculo vectorial serán: \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\). 2.2.3.2 Derivada La derivada de una función \\(f\\) la vamos a definir de la siguiente forma: \\[ f&#39;(x) = \\lim_{h \\rightarrow 0}\\frac{f(x + h) - f(x)}{h}. \\] A esta expresión se le llama límite y se interpreta como hacer una pequeña perturbación que tiende a cero en el parámetro \\(h\\) y cuando este límite existe, se dice que \\(f\\) es diferenciable en \\(x\\), siendo así \\(f&#39;(x)\\) su derivada. Esto se dice que debe ocurrir en al menos un subconjunto ya que al considerar las perturbaciones del parámetro \\(h\\), estamos considerando números del orden infinitesimal ya que estamos tratando con los números reales, y estos poseen una propiedad de densidad que nos permite encontrar estos elementos para realizar dicho límite. Para mantener el foco en el aprendizaje profundo y no desviarnos a un curso tradicional de ciencias o ingeniería, de momento lo dejaremos hasta aquí en relación a las propiedades matemáticas de los reales, límites, y otros temas que tienen mucho material para ser desarrollados. Como dato útil, podemos pensar también en la derivada de la derivada, nombrando esta como la segunda derivada y sus propiedades serán las mismas por definición. Sin embargo, esta recursividad nos permite poder obtener información a partir de una función con sus diferentes derivadas de acuerdo al contexto. Por ejemplo, si registramos la aceleración de un objeto, la derivada nos indicará la velocidad del objeto, y su segunda derivada, la posición del objeto. 2.2.3.3 Derivada vectorial De forma simétrica a como hemos definido la derivada en una variable, podemos ver para funciones escalar de varias variables \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), que el gradiente queda definido de la siguiente forma: \\[ \\nabla f(\\vec{x}) = \\begin{pmatrix} \\frac{\\partial}{\\partial x_1}f(\\vec{x}) \\\\ \\frac{\\partial}{\\partial x_2}f(\\vec{x}) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n}f(\\vec{x}) \\end{pmatrix} \\] donde para \\(0 \\le i \\le n\\), tenemos que: \\[ \\frac{\\partial}{\\partial x_i}f(\\vec{x}) \\] es la derivada de \\(f\\) en la \\(i\\)-ésima dirección (o coordenada o variable) definida de la siguiente forma considerando \\(y = f(x_1, x_2, \\dots, x_n)\\): \\[ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}. \\] Como ejemplo, consideremos la función \\(f(x, y) = 3x^2 y\\). Entonces tenemos lo siguiente: \\[ \\begin{align} \\frac{\\partial}{\\partial x}3x^2y = 6xy \\\\ \\frac{\\partial}{\\partial y}3x^2y = 3x^2 \\\\ \\end{align} \\] Por lo tanto, su gradiente es: \\[ \\nabla f(x, y) = \\begin{pmatrix} 6xy \\\\ 3x^2 \\end{pmatrix} \\] y este nos dirá como se comporta \\(f\\) de forma análoga al caso de 1 variable. Otros casos como funciones vectoriales de una variable real, o funciones vectoriales de varias variables quedan fuera por el momento de este estudio. 2.2.3.4 Algunas formulas y reglas A continuación hay algunas formulas que nos pueden ayudar a calcular algunas derivadas: \\[ \\begin{split}\\begin{aligned} \\frac{d}{dx} C &amp; = 0 &amp;&amp; \\textrm{para cualquier constante $C$} \\\\ \\frac{d}{dx} x^n &amp; = n x^{n-1} &amp;&amp; \\textrm{para } n \\neq 0 \\\\ \\frac{d}{dx} e^x &amp; = e^x \\\\ \\frac{d}{dx} \\ln x &amp; = x^{-1}. \\end{aligned}\\end{split} \\] Y continuamos con algunas reglas que nos permitirán trabajar con funciones compuestas: \\[ \\begin{split}\\begin{aligned} \\frac{d}{dx} [C f(x)] &amp; = C \\frac{d}{dx} f(x) &amp;&amp; \\textrm{La constante sale} \\\\ \\frac{d}{dx} [f(x) + g(x)] &amp; = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x) &amp;&amp; \\textrm{Suma de derivadas} \\\\ \\frac{d}{dx} [f(x) g(x)] &amp; = f(x) \\frac{d}{dx} g(x) + g(x) \\frac{d}{dx} f(x) &amp;&amp; \\textrm{Producto o multiplicación de derivadas} \\\\ \\frac{d}{dx} \\frac{f(x)}{g(x)} &amp; = \\frac{g(x) \\frac{d}{dx} f(x) - f(x) \\frac{d}{dx} g(x)}{g^2(x)} &amp;&amp; \\textrm{Cociente o división de derivadas} \\end{aligned}\\end{split} \\] De esta forma, podemos hacer uso de diferentes reglas para calcular derivadas. Más aún, con el teorema fundamental del cálculo, y al ver la integral como anti-derivada, podemos aprovechar algunos resultados del cálculo integral. 2.2.3.4.1 Reglas útiles para derivar funciones multivariable Para toda \\(A \\in \\mathbb{R}^{m \\times n}\\), tenemos \\(\\nabla_x Ax = A^\\top\\) y \\(\\nabla_x x^\\top A = A\\). Así como para matrices cuadradas \\(A \\in \\mathbb{R}^{n \\times n}\\), tenemos que \\(\\nabla_x x^\\top A x = (A + A^\\top)x\\), y en particular: \\(\\nabla_x \\|x\\|^2 = \\nabla_x x^\\top x = 2x\\). De forma similar para cualquier matriz \\(X\\), tenemos \\(\\nabla_x \\|X\\|_F^2 = 2X\\). 2.2.4 Regla de la cadena En general, las funciones suelen tener composiciones más elaboradas y sofisticadas de lo que hemos enlistado antes, haciendo que sea complicado realizar el cálculo de la derivada en estos casos. Por fortuna existe algo llamado regla de la cadena y que nos apoya completamente en estos escenarios. De momento pensando en derivadas de una variable, partamos de la siguiente función \\(y = f(g(x))\\), resaltando que \\(y = f(u)\\) así como \\(u = g(x)\\) son ambas diferenciables. Entonces la regla de la cadena afirma que: \\[ \\frac{dy}{dx} = \\frac{dy}{du}\\frac{du}{dx}. \\] De esta forma, viendo el escenario desde el contexto de funciones multivariable, supón que \\(y = f(\\vec{u})\\) tiene variables \\(u_1, u_2, \\dots, u_m\\) donde para cada \\(u_i = g_i(\\vec{x})\\) tiene variables \\(x_1, x_2, \\dots, x_n\\), es decir \\(\\vec{u} = g(\\vec{x})\\), entonces la regla de la cadena afirma que: \\[ \\frac{\\partial y}{\\partial x_{i}} = \\frac{\\partial y}{\\partial u_{1}} \\frac{\\partial u_{1}}{\\partial x_{i}} + \\frac{\\partial y}{\\partial u_{2}} \\frac{\\partial u_{2}}{\\partial x_{i}} + \\ldots + \\frac{\\partial y}{\\partial u_{m}} \\frac{\\partial u_{m}}{\\partial x_{i}} \\ \\textrm{ y así } \\ \\nabla_{\\mathbf{x}} y = \\mathbf{A} \\nabla_{\\mathbf{u}} y, \\] donde \\(A \\in \\mathbb{R}^{n \\times m}\\) es una matriz que contiene la derivada del vector \\(\\vec{u}\\) con respecto al vector \\(\\vec{x}\\). Por lo tanto, para evaluar el gradiente requiere calcular un producto entre vector y matriz. Esta es una de las razones principales de por qué el álgebra lineal es un componente vital en la construcción de sistemas de aprendizaje profundo. 2.2.5 Interpretación geométrica de la derivada Calcular la derivada representa más que la definición que hemos dado, y esta sin duda tiene una interpretación geométrica: el ángulo de la línea tangente en el punto \\(x\\) se le asocia con \\(f&#39;(x)\\). De esta forma podemos ver otras interpretaciones geométricas ya que a medida que las tangentes son positivas, podemos ver que la función se incrementa en valor, así como con tangentes negativas, significa que la función decrementa en valor. De esta forma, cuando encontramos que las tangentes son cero, significa que hay una meseta o valor constante en la función en ese punto. También podemos ver que para funciones escalares de varias variables, que su gradiente tiene una estructura similar, diciendonos estos comportamientos en sus variables o direcciones, pudiendo así conceptualizar como el pensar en gradiente descendiente nos lleva a buscar las direcciones de decrecimiento de una función para así, buscar sus valores mínimos, pensando que al llegar a estas mesetas hundidas o valles, encontraremos los mínimos que buscamos. 2.2.6 Teorema fundamental del cálculo Un teorema muy importante en el cálculo es el teorema fundamental de este, estableciendo la conexión entre derivadas e integrales. Sirviendo así como puente para conectar las derivadas con las integrales, viendo a una integral como antiderivada. Sea \\(f: [a, b]: \\rightarrow \\mathbb{R}\\) una función continua. Definamos una nueva función \\(F(x)\\) como: \\[ F(x) = \\int_{a}^{x}f(t)dt, \\] entonces \\[ F&#39;(x) = f(x). \\] Es decir, derivar la integral de una función continua nos regresa la función original. Las interpretaciones de este resultado son: \\(F(x)\\) acumula el área bajo la curva de \\(f\\) desde \\(a\\) hasta \\(x\\). La derivada \\(F&#39;(x)\\) mide cuán rápido crece esa área al mover el límite superior \\(x\\). De esta forma, la derivada y la integral son operaciones inversas. 2.2.7 Autodiferenciación Esta idea es un concepto poderoso que conecta el cálculo diferencial con la computación moderna y el aprendizaje profundo. Esto porque computacionalmente, no es eficiente realizar cálculos simbólicos, así como la forma de calcular derivadas numéricas solía ser con una forma más antigua en un contexto de diferencias finitas. 2.2.7.1 Historia Mientras tanto, el concepto de autodiferenciación es un tema más reciente en la historia de las matemáticas, encontrando las primeras referencias en el trabajo de Wengert en 1964 1 . Las ideas núcleo para la propagación hacia atrás se pueden trazar a una tesis de doctorado (PhD) de Speelpenning en 1980 2 y fueron trabajadas y desarrolladas a finales de los 80’s por Griewank 3. Aunque la propagación hacia atrás es el método por eleccción para calcular gradientes, no es la única opción que existe. Un ejemplo lo encontramos en el lenguaje de programación Julia, que emplea una propagación hacia adelante 4. 2.2.7.2 ¿Qué es? Es un conjunto de técnicas que permiten calcular derivadas de funciones expresadas como programas de manera exacta y eficiente. Utilizando la regla de la cadena a nivel de operaciones elementales. No tratándose de: Diferenciación simbólica, donde se manipulan las expresiones algebraicamente (como en el caso de SymPy), resultando altamente ineficiente. Ni de diferenciación numérica, que aproxima derivadas con diferencias finitas y es propensa a errores de redondeo. De esta forma la autodiferenciación combina la precisión del cálculo analítico con la eficiencia computacional del cálculo numérico. 2.2.7.3 Fundamento matemático Toda función implementada computacionalmente se puede descomponer como una composición de operaciones elementales (suma, multiplicación, exponencial, etc). Consideremos el siguiente ejemplo: \\[ y = f(x_1, x_2) = e^{x_1x_2} + \\sin(x_1). \\] El cálculo de su derivada implica aplicar sistemáticamente la regla de la cadena: \\[ \\frac{dy}{dx_i} = \\frac{\\partial f}{\\partial x_i}. \\] La autodiferenciación recorre ese mismo proceso de forma automática, propagando derivadas locales a lo largo de la gráfica o grafo computacional que representa dicha función. 2.2.7.4 Dos modos principales 2.2.7.4.1 Modo directo Calcula las derivadas de salida respecto a una entrada. Ideal cuando tienes pocas entradas y muchas salidas. Consideremos el siguiente ejemplo: \\[ f(x, y) = \\begin{pmatrix} x^2 + \\sin(y) \\\\ e^{xy} \\end{pmatrix} \\] using ForwardDiff ### f: ℝ² → ℝ² f(x::AbstractVector) = [ x[1]^2 + sin(x[2]); exp(x[1] * x[2]) ] ### g: ℝ² → ℝ (para gradiente y Hessiano) g(x::AbstractVector) = 0.5 * (x[1]^2 + 3x[2]^2) + sin(x[1]*x[2]) x = [1.0, 0.5] ### Jacobiano de f en x J = ForwardDiff.jacobian(f, x) ### Gradiente de g en x ∇g = ForwardDiff.gradient(g, x) ### Hessiano de g en x H = ForwardDiff.hessian(g, x) println(&quot;J =\\n&quot;, J) println(&quot;∇g = &quot;, ∇g) println(&quot;H =\\n&quot;, H) ###= Resultados: J = [2.0 0.8775825618903728; 0.8243606353500641 1.6487212707001282] ∇g = [1.4387912809451864, 2.3775825618903728] H = [0.8801436153489492 0.6378697925882713; 0.6378697925882713 2.520574461395797] =# Los resultados son los siguientes: 2.2.7.4.1.1 El Jacobiano \\(J_f(x, y)\\): \\[ \\left.J_f(x_0, y_0)\\right|_{x_0 = 1,\\ y_0 = 0.5} = \\begin{pmatrix} 2.0 &amp; 0.8775825618903728 \\\\ 0.8243606353500641 &amp; 1.6487212707001282 \\end{pmatrix} \\] Lo que nos dice la sensibilidad al variar cerca del punto \\((x_0, y_0)\\). 2.2.7.4.1.2 El gradiente \\(\\nabla g(x, y)\\): \\[ \\left.\\nabla g(x_0, y_0)\\right|_{(x_0, y_0) = (1, 0.5)} = \\begin{pmatrix} 1.4387912809451864 \\\\ 2.3775825618903728 \\end{pmatrix} \\] Esto nos dice la dirección de máximo incremento local de \\(g\\); para un paso pequeño \\(\\Delta x\\), tenemos que \\(g(x + \\Delta x) \\approx g(x) + \\nabla g(x) \\cdot \\Delta x\\). 2.2.7.4.1.3 El hessiano \\(H_g(x, y)\\) \\[ \\left.H_g(x_0, y_0)\\right|_{(x_0, y_0) = (1, 0.5)} = \\begin{pmatrix} 0.8801436153489492 &amp; 0.6378697925882713 \\\\ 0.6378697925882713 &amp; 2.520574461395797 \\end{pmatrix} \\] Lo que nos dice esto es la curvatura local de \\(g\\). Si \\(H\\) es definida positiva en \\((x, y)\\), el punto es un mínimo local. 2.2.7.4.1.4 Uso de estos resultados Estos cálculos nos sirven para los siguientes conceptos en la práctica: Linealizar \\(f\\) alrededor de \\(x\\): \\(f(x + \\Delta x) \\approx f(x) + J \\Delta x\\). La derivada es una buena aproximación que tiene un comportamiento lineal de forma local. Buscar descenso de \\(g\\): mover \\(x\\) en la dirección \\(-\\nabla g\\), Aprovechar la curvatura: \\(\\Delta x = -H^{-1} \\nabla g\\) si \\(H\\) es bien condicionada y positiva definida (\\(H \\succ 0\\)). Qué pasa: ForwardDiff propaga números duales (pares valor-derivada) por cada operación elemental y arma automáticamente el Jacobiano/gradiente/Hessiano. 2.2.7.4.2 Modo inverso Calcula derivadas de una salida respecto a todas las entradas. Este modo es ideal cuando tienes una salida escalar (como es el caso de una función de pérdida en aprendizaje profundo) y muchas entradas o parámetros. using Zygote, LinearAlgebra, Statistics, Random Random.seed!(0) ### Definimos un MLP Perceptrón Multicapa o Multilayer Perceptron ### sencillo &quot;a mano&quot; struct MLP W1::Matrix{Float64} b1::Vector{Float64} W2::Matrix{Float64} b2::Vector{Float64} end ### Inicialización function MLP(inDim::Int, hid::Int, outDim::Int; σ = 0.1) W1 = σ .* randn(hid, inDim) b1 = zeros(hid) W2 = σ .* randn(outDim, hid) b2 = zeros(outDim) MLP(W1, b1, W2, b2) end ### Forward pass predict(m::MLP, X::Matrix) = m.W2 * tanh.(m.W1 * X .+ m.b1) .+ m.b2 ### Pérdida MSE escalar mse(m::MLP, X::Matrix, Y::Matrix) = mean((predict(m, X) .- Y).^2) ### Datos de juguete: 2 entradas, 1 salida, N muestras N = 5 X = randn(2, N) trueW = [2.0 -1.0] # fila 1x2 Y = trueW * X .+ 0.1 .* randn(1, N) # 1xN ### Modelo: 2 neuronas de entrada, 8 en la capa oculta y 1 de salida. m = MLP(2, 8, 1) ### Gradiente de la pérdida respecto a todos los parámetros loss(m) = mse(m, X, Y) grads = Zygote.gradient(loss, m) ### grads es una tupla con ∂loss/∂(W1,b1,W2,b2) empaquetada como MLP ∂W1 = grads[1].W1 ∂b1 = grads[1].b1 ∂W2 = grads[1].W2 ∂b2 = grads[1].b2 println(&quot;‖∂W1‖ = &quot;, norm(∂W1)) println(&quot;‖∂b1‖ = &quot;, norm(∂b1)) println(&quot;‖∂W2‖ = &quot;, norm(∂W2)) println(&quot;‖∂b2‖ = &quot;, norm(∂b2)) Las normas calculadas son las siguientes: Parámetro Norma Capa Tipo ‖∂W1‖ 0.34 Oculta Pesos ‖∂b1‖ 0.53 Oculta Sesgos ‖∂W2‖ 0.25 Salida Pesos ‖∂b2‖ 1.77 Salida Sesgos Con lo que podemos ver que no existe explosión ni desvanecimiento de gradiente. Y parece indicar que el modelo comienza cerca de un mínimo local o en una zona de pérdida menos pronunciada. Wengert, R. E. (1964). A simple automatic derivative evaluation program. Communications of the ACM, 7(8), 463–464.↩︎ Speelpenning, B. (1980). Compiling fast partial derivatives of functions given by algorithms (Doctoral dissertation). University of Illinois at Urbana-Champaign.↩︎ Griewank, A. (1989). On automatic differentiation. Mathematical Programming: Recent Developments and Applications (pp. 83–107). Kluwer.↩︎ Revels, J., Lubin, M., &amp; Papamarkou, T. (2016). Forward-mode automatic differentiation in Julia. ArXiv:1607.07892.↩︎ "],["redes-neuronales-lineales-para-regresión.html", "Capítulo 3 Redes neuronales Lineales para Regresión 3.1 Introducción 3.2 Regresiones lineales simples 3.3 Función de pérdida y función de costo 3.4 Supuestos en la regresión lineal 3.5 Regresión Lineal Múltiple 3.6 Estimación de los parámetros 3.7 Bondad de ajuste 3.8 Supuestos del modelo lineal múltiple 3.9 Regularización en la Regresión Lineal 3.10 Consideraciones 3.11 Sesgo y Varianza 3.12 Conclusión de la sección", " Capítulo 3 Redes neuronales Lineales para Regresión 3.1 Introducción El propósito de la regresión lineal es modelar la relación entre una variable que queremos predecir (por ejemplo, el precio de una vivienda) y una o más variables que podrían influir en ella (como los metros cuadrados, la ubicación o el número de habitaciones). Este tipo de análisis nos permite identificar patrones, hacer predicciones y entender cómo cambian los valores de una variable cuando las otras se modifican. En esencia, la regresión lineal busca encontrar una línea que mejor describa la tendencia general en los datos, ofreciendo una forma sencilla pero poderosa de explorar relaciones cuantitativas. El término “regresión” proviene del trabajo del científico y estadístico británico Francis Galton a finales del siglo XIX. Galton, mientras estudiaba la relación entre la altura de padres e hijos, observó que los hijos de padres muy altos tendían a ser más bajos que ellos, y los hijos de padres muy bajos tendían a ser más altos. Es decir, las alturas “regresaban” hacia un valor promedio de la población. Para describir este fenómeno, Galton utilizó la expresión “regresión hacia la media”. Con el tiempo, este concepto se generalizó y dio origen al término “regresión” en estadística, que hoy usamos para describir modelos que ajustan relaciones entre variables. 3.2 Regresiones lineales simples Las regresiones lineales buscan predecir una variable númerica (variable independiente), a partir de una o más variables (variables independientes), con el supuesto de que la relación que se da, entre la variable dependiente, con la independientes, es aproximadamente lineal. Partamos del siguiente problema en el que se busca establecer el precio de una vivienda a partir de los metros cuadrados que lo conforman. Al graficar las dos características mencionadas se observa lo siguiente: Al observar el gráfico se puede observar que el precio de la vivienda crece conforme los metros de la propiedad aumentan, manteniendo una relación “lineal”. Dicha relación la representamos entonces con la siguiente fórmula: \\[ Precio=\\beta*metros^2+b \\] Si bien la relación no es lineal a la perfección, podemos pensar en la recta que mejor se ajuste, para así, obtener predicciones de la variable dependiente a partir de la variable independiente. 3.3 Función de pérdida y función de costo 3.3.0.1 Función de pérdida (Loss Function) La función de pérdida (o loss function) es una herramienta que mide qué tan bien o mal está funcionando un modelo de predicción para una observación individual. En otras palabras, compara el valor real con el predicho y asigna un número que representa el error cometido. \\[ \\mathcal{L}(y,\\hat{y}) \\] 3.3.0.2 Función de costo (Cost Function) La función de costo es una medida global del error del modelo completo. Se obtiene al promediar (o combinar de otra forma) las pérdidas de todas las observaciones del conjunto de datos. \\[ \\mathcal{J}(\\theta) \\] Existen diferentes funciones de pérdida y costo, se elige cual debe considerarse en base al problema que se quiere resolver. Regresando a la identidad de la regresión lineal, dado que la recta queda completamente definida por la pendiente y por la ordenada al origen, nuestro problema consiste en encontrar valores para estas, tales que mejor ajustan la recta a los datos. Definimos las funciones de pérdida y costo, en la regresión lineal, de la siguiente forma: \\[ \\bullet \\quad\\mathcal{L}=(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2 \\] \\[ \\bullet \\quad \\mathcal{J}(\\theta,b) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathcal{L}(y_i, \\hat{y}_i) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta x_i+b))^2 \\] Parámetros de la regresión lineal —————————————————————————————————————————————————————————- \\[ \\textbf{Modelo:}\\qquad y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i,\\quad i=1,\\dots,n \\] \\[ \\text{Minimizamos la suma de cuadrados de residuos:}\\qquad S(\\beta_0,\\beta_1) = \\sum_{i=1}^n \\big(y_i - \\beta_0 - \\beta_1 x_i\\big)^2 \\] \\[ \\textbf{Derivadas parciales y ecuaciones normales:} \\] \\[ \\frac{\\partial S}{\\partial \\beta_0} = -2 \\sum_{i=1}^n \\big(y_i - \\beta_0 - \\beta_1 x_i\\big) = 0 \\;\\;\\Longrightarrow\\;\\; \\sum_{i=1}^n y_i = n\\beta_0 + \\beta_1 \\sum_{i=1}^n x_i . \\] \\[ \\frac{\\partial S}{\\partial \\beta_1} = -2 \\sum_{i=1}^n x_i \\big(y_i - \\beta_0 - \\beta_1 x_i\\big) = 0 \\;\\;\\Longrightarrow\\;\\; \\sum_{i=1}^n x_i y_i = \\beta_0 \\sum_{i=1}^n x_i + \\beta_1 \\sum_{i=1}^n x_i^2 \\] \\[ \\text{Definiendo }\\bar{x}=\\frac{1}{n}\\sum x_i,\\;\\bar{y}=\\frac{1}{n}\\sum y_i, \\text{ la primera ecuación da }\\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\] \\[ \\text{Sustituyendo en la segunda:}\\quad \\sum x_i y_i = (\\bar{y} - \\beta_1 \\bar{x}) \\sum x_i + \\beta_1 \\sum x_i^2 = n\\bar{x}\\bar{y} - n\\beta_1 \\bar{x}^2 + \\beta_1 \\sum x_i^2 \\] \\[ \\Longrightarrow\\quad \\sum x_i y_i - n\\bar{x}\\bar{y} = \\beta_1 \\Big(\\sum x_i^2 - n\\bar{x}^2\\Big) \\] \\[ \\text{Notando que }\\; S_{xy} := \\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y}) = \\sum x_i y_i - n\\bar{x}\\bar{y}, \\quad S_{xx} := \\sum_{i=1}^n (x_i-\\bar{x})^2 = \\sum x_i^2 - n\\bar{x}^2, \\] \\[ \\text{obtenemos}\\qquad \\boxed{\\,\\hat{\\beta}1 = \\dfrac{S{xy}}{S_{xx}}\\,}. \\] \\[ \\text{Finalmente}\\qquad \\boxed{\\,\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\,}. \\] \\[ \\text{Formas equivalentes:}\\qquad \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2} = \\frac{\\operatorname{Cov}(X,Y)}{\\operatorname{Var}(X)}, \\qquad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}. \\] —————————————————————————————————————————————————————————- 3.4 Supuestos en la regresión lineal Linealidad La relación entre las variables independientes (predictoras) y la variable dependiente debe ser lineal. Esto implica que los efectos de los predictores sobre la respuesta son proporcionales y aditivos. Comprobación: Gráficos de dispersión o residuos vs. predicciones. Independencia de los errores Los residuos (errores) deben ser independientes entre sí. Homocedasticidad La varianza de los errores debe ser constante para todos los valores de las variables independientes. Normalidad de los errores Los errores del modelo deben seguir una distribución normal con media cero. Este supuesto es importante para la inferencia (p-valores, intervalos de confianza). Comprobación: Histograma o gráfico Q-Q de los residuos, prueba de Shapiro-Wilk. Ausencia de multicolinealidad Las variables independientes no deben estar altamente correlacionadas entre sí. La multicolinealidad puede inflar las varianzas de los coeficientes. Comprobación: VIF (Variance Inflation Factor), matriz de correlaciones. No presencia de valores atípicos influyentes Los outliers o puntos de alta influencia pueden distorsionar el ajuste del modelo. Es importante identificarlos y tratarlos adecuadamente. Comprobación: Distancia de Cook, leverage, o gráfico de residuos estandarizados. 3.5 Regresión Lineal Múltiple La regresión lineal múltiple es una extensión de la regresión lineal simple, que permite modelar la relación entre una variable dependiente y dos o más variables independientes. Su objetivo es estimar cómo cada variable explicativa contribuye, en promedio, al comportamiento de la variable objetivo. Dada una varieble dependiente Y, que busca ser explicada por p características. Para un registro de la variable dependiente, el modelo se expresa como: \\[ Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ip} + \\varepsilon_i \\] donde: - \\(Y_i\\): valor de la variable dependiente para la observación \\(i\\), - \\(X_{ij}\\): valor de la variable explicativa \\(j\\) para la observación \\(i\\), - \\(\\beta_0\\): intercepto, - \\(\\beta_j\\): coeficiente asociado a la variable \\(X_j\\), - \\(\\varepsilon_i\\): término de error (diferencia entre el valor real y el predicho). Así considerando todos los registros y la forma en la que se operan los vectores y las matrices, tenemos: \\[ \\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\] 3.6 Estimación de los parámetros El método más común para estimar los parámetros es el de Mínimos Cuadrados Ordinarios (OLS), que busca minimizar la suma de los errores cuadráticos: \\[ (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\] La solución matricial es: \\[ \\boxed{\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}} \\] Para encontrar los valores de \\(\\boldsymbol{\\beta}\\) que minimizan la suma de los errores cuadráticos: \\[ \\min_{\\boldsymbol{\\beta}} \\; (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\] 3.6.1 Derivación paso a paso Expandir la expresión: \\[ S(\\boldsymbol{\\beta}) = \\mathbf{y}&#39;\\mathbf{y} - 2\\mathbf{y}&#39;\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{X}\\boldsymbol{\\beta} \\] Derivar respecto a \\(\\boldsymbol{\\beta}\\): \\[ \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}&#39;\\mathbf{y} + 2\\mathbf{X}&#39;\\mathbf{X}\\boldsymbol{\\beta} \\] Igualar a cero (condición de mínimo): \\[ \\mathbf{X}&#39;\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}&#39;\\mathbf{y} \\] \\[ \\boxed{\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}} \\] 3.7 Bondad de ajuste Una vez estimado el modelo, se puede evaluar qué tan bien explica los datos mediante el coeficiente de determinación \\(R^2\\): \\[ R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} \\] \\(R^2\\) cercano a 1 → el modelo explica gran parte de la variabilidad de \\(Y\\). \\(R^2\\) cercano a 0 → el modelo explica poco o nada. Sin embargo la \\(R^2\\) no penaliza la complejidad del modelo. Esto es, al aumentar el número de regresores, el factor \\(\\sum (y_i - \\hat{y}_i)^2\\), no puede aumentar El valor de la R^2 ajustada, penaliza la inclusión de variables irrelevantes, que pueden ser altamente colineales. Para esto considera el número de predictores y el tamaño de la muestra. Para hallar el valor de la R^2 ajustada se tienes que calcular primero el valor de la R^2, así entonces se tiene: \\[ R^2_{ajustada}=1-\\frac{(1-R^2)(n-1)}{n-p-1} \\] La diferencia entre ambas métricas es que r-cuadrada siempre aumenta o permanece cuando se agregan predictores, aunque los predictores que se añaden no mejoren significativamente el modelo. En cambio la R-cuadrada ajustada, puede disminuir si un nuev predictor no mejora el modelo. — 3.8 Supuestos del modelo lineal múltiple Para que los resultados del modelo sean válidos, se asume que: Linealidad: la relación entre \\(Y\\) y las \\(X_j\\) es lineal. Independencia: los errores son independientes entre sí. Esperanza condicional cero: \\(\\mathbb{E}[\\varepsilon_i \\mid X] = 0\\). Homocedasticidad: la varianza de los errores es constante. No multicolinealidad: las variables independientes no están perfectamente correlacionadas. Normalidad: los errores se distribuyen normalmente. 3.9 Regularización en la Regresión Lineal Cuando un modelo de regresión lineal se ajusta demasiado a los datos (sobreajuste), o existen muchas variables correlacionadas, los coeficientes pueden volverse inestables. Para controlar este problema, existen métodos de regularización, que agregan una penalización a la función de costo. Los tres enfoques más comunes son Ridge, Lasso y Elastic Net. 3.9.1 Ridge Regression Agrega una penalización cuadrática a los coeficientes para evitar sobreajuste.Los coeficientes al aportar a la función de costo, buscan ser más pequeño y con esto variables que aportaban mucho a la explicabilidad de la variable dependiente pierden relevancia. \\[ \\mathcal{J}_{\\text{ridge}}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\] Penaliza los coeficientes grandes. Los reduce, pero no los lleva exactamente a cero. Útil cuando existe multicolinealidad entre las variables. 3.9.2 Lasso Regression Agrega una penalización absoluta (valor absoluto) que puede llevar algunos coeficientes a cero. \\[ \\mathcal{J}_{\\text{lasso}}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\] Además de reducir los coeficientes, elimina algunos por completo. Actúa como un método de selección automática de variables. Es útil en problemas de alta dimensionalidad. 3.9.3 Elastic Net Regression Combina las penalizaciones de Ridge y Lasso, logrando un balance entre ambos enfoques. \\[ \\mathcal{J}_{\\text{elastic}}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\lambda \\left( \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1 - \\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right) \\] donde: - \\(\\lambda\\) controla la intensidad de la regularización, - \\(\\alpha \\in [0,1]\\) controla el balance entre Lasso y Ridge. Casos especiales: - \\(\\alpha = 1\\) → modelo Lasso - \\(\\alpha = 0\\) → modelo Ridge 3.9.4 Comparación de métodos Método Penalización Efecto sobre los coeficientes Selecciona variables Principal uso Ridge \\(\\lambda \\sum \\beta_j^2\\) Los reduce (sin anularlos) No Multicolinealidad Lasso \\(\\lambda \\sum |\\beta_j|\\) Algunos se vuelven 0 sí Alta dimensionalidad Elastic Net \\(\\lambda [\\alpha \\sum |\\beta_j| + (1-\\alpha)\\sum \\beta_j^2]\\) Combina ambos efectos Parcial Balance entre Ridge y Lasso La regularización busca un equilibrio entre ajuste y simplicidad. Mientras Ridge controla la magnitud de los coeficientes, Lasso puede eliminarlos. Elastic Net combina ambos enfoques para lograr un modelo más robusto y equilibrado. 3.10 Consideraciones Es útil escalar los valores de la variables dependientes, sobre todo al regularizar las regresiones. La correlación de Pearson, es una buena herramienta para seleccionar variables independientes que modelen a una variable dependiente. 3.11 Sesgo y Varianza El desempeño de un modelo de regresión (o de cualquier modelo de aprendizaje) depende de su capacidad para equilibrar el sesgo y la varianza. Estos dos conceptos explican los errores que comete el modelo al predecir valores nuevos. 3.11.1 Sesgo El sesgo mide cuánto se alejan las predicciones promedio del modelo del valor real esperado. En términos simples, representa el error por simplificación del modelo. Alta sesgo → modelo demasiado simple, que no logra capturar la complejidad de los datos (subajuste). 3.11.2 Varianza La varianza mide cuánto cambian las predicciones del modelo si se entrena con diferentes conjuntos de datos. Representa la sensibilidad del modelo al ruido. Alta varianza → modelo demasiado complejo, que se ajusta demasiado a los datos de entrenamiento (sobreajuste). Consecuencia: Predicciones muy diferentes en nuevos datos. 3.11.3 Compromiso sesgo–varianza Existe un equilibrio entre ambos conceptos: Tipo de modelo Sesgo Varianza Resultado Modelo simple (pocos parámetros) Alto Bajo Subajuste Modelo complejo (muchos parámetros) Bajo Alto Sobreajuste Modelo equilibrado Medio Medio Buen ajuste 3.11.4 En regresión lineal La regresión lineal clásica tiende a tener bajo sesgo y baja varianza si el número de variables es razonable. Cuando se agregan muchas variables o hay multicolinealidad, la varianza aumenta, porque los coeficientes se vuelven inestables. Métodos de regularización (como Ridge o Lasso) se usan precisamente para reducir la varianza, a costa de introducir un poco de sesgo. 3.12 Conclusión de la sección Finalmente buscando conectar el tema con las redes neuronales. La regresión lineal es el primer modelo que se ajusta al construir una neurona, pues recibe las variables de entrada (variables independientes), una vez ajustada la regresión lineal, esta pasará por una función de activación la cual rompe con la relación lineal entre las variables independientes con la variable dependiente. "],["redes-neuronales-lineales-para-clasificación.html", "Capítulo 4 Redes neuronales Lineales para Clasificación 4.1 Modelos Lineales Generalizados 4.2 Regresión Logística para Clasificación 4.3 Redes neuronales Notas adicionales", " Capítulo 4 Redes neuronales Lineales para Clasificación 4.1 Modelos Lineales Generalizados 4.1.1 Historia Regresión lineal El primer método de regresión lineal documentado es el método de los mínimos cuadrados, publicado por Legendre en 1805. Posteriormente, Gauss publicó un trabajo donde se desarrolla con mayor detalle el tema. Modelo Lineal Generalizado (GLM) Los GLM fueron formulados por John Nelder y Robert Wedderburn en 1972 como una manera de unificar modelos estadísticos. Un modelo lineal generalizado es una generalización flexible de la regresión lineal ordinaria. 4.1.2 Definición Los Modelos Lineales Generalizados (GLM) son una extensión de los modelos lineales tradicionales que permiten modelar variables respuesta que no necesariamente siguen una distribución normal (por ejemplo, binarias, de conteo o proporciones). Relacionan la distribución aleatoria de la variable dependiente (variable objetivo) en el experimento, con la parte sistemática a través de una función llamada función de enlace. Por lo tanto, un modelo lineal generalizado, tiene tres componentes básicos: Componente aleatorio (Y ~ Distribución de probabilidad): Identifica la variable de objetivo y su distribución de probabilidad. Componente sistemática (𝜂 = xW): Especifica las variables explicativas de la función predictora lineal. Función de enlace (𝑔(𝝁) = 𝜂): Es una función del valor esperado de Y como una combinación lineal de las variables explicativas Algebraicamente: \\[ 𝔼(𝒀)=𝝁=𝑔^{−1}(WX) \\] Los estudios de Wedderburn se limitaron los componentes aleatorios a la familia exponencial, cuya aplicación se puede dar de la siguiente manera: Normal: regresión lineal Binomial: regresión logística Poisson: regresión de conteo Gamma: regresión para tiempos o tasas positivas Inversa Gaussiana, etc. Distribución Función de Enlace Nombre de modelo Normal g(μ) = μ Regresión lineal Binomial g(μ) = log(μ /(1−μ)) Regresión logística Poisson g(μ) = log(μ) Regresión de Poisson Representación gráfica Podemos apreciar una regresión lineal como una neurona: Haciendo uso de una regresión logística, podemos pasar de how much? a wich class?, el diagrama sería el siguiente: 4.2 Regresión Logística para Clasificación Iniciemos recordando que los algoritmos de Machine Learning pueden ser clasificados como: Los métodos de clasificación, métodos predictivos, reconocimiento de patrones, aprendizaje supervisado son aquellos algoritmos en los cuales vamos a clasificar a los objetos en un conjunto de categorías previamente definidas. En estos métodos existe una variable, conocida como variable objetivo con la cual se buscarán dos resultados: Evidenciar la razón por la cual existen las clases en cuestión, es decir, encontrar factores que puedan agrupar a los individuos en sus respectivos grupos. Proyectar el ingreso de nuevos individuos en alguna de las características anteriores. Definición La clasificación es la tarea de aprender una función, f, que pueda mapear cada conjunto de atributos a una categoría predefinida. Busca predecir la clase a la cual pertenece un registro. La diferencia principal contra el análisis de conglomerados es el deseo de predecir a nuevos individuos. Buscamos una función que vaya de D a C \\[ 𝑓:𝐷→ Y \\] Donde: \\(𝐷={(𝑥_𝑖,𝑦_𝑖) | 𝑖=1,2,…,𝑁}\\) es el conjunto de individuos, incluyendo \\(𝑥_𝑖\\) corresponde a las características explicativas de un individuo y \\(𝑦_𝑖\\) corresponde al valor de la clase \\(Y={y _1, y_2,…, y_𝑚}\\) es el conjunto de clases. La función resultante puede ser un árbol de decisión, SVM, etc. La función que clasificará el conjunto X a la clase Y, comúnmente es llamada modelo de clasificación. Una vez recordado lo anterior, podemos definir a una regresión logística como un modelo en el cual se recibe una tabla relacional con d características descriptivas y una clase a la cual pertenece cada individuo de la tabla. Como resultado, el modelo nos proporciona la probabilidad de pertenecer a alguna clase. Planteamiento del problema El primer paso, consistiría en construir el modelo de regresión, de tal forma que el componente aleatorio (\\(y_i\\)) se conecte con la componente sistémica (regresión lineal): \\[ 𝑦_𝑖↔𝛽_0+𝛽_1 𝑥_𝑖+𝜖_𝑖 \\] Vamos a suponer un modelo de regresión lineal clásica de tal forma que: \\[ 𝔼(𝑦_𝑖 | 𝑥_𝑖 )=𝛽_0+𝛽_1 𝑥_𝑖 \\] Al intentar buscar probabilidades a través de una regresión lineal, obtendríamos lo siguiente: Sea \\(𝑝_𝑖\\) la probabilidad de pertenecer a la clase 1 dadas las características del individuo: \\[ 𝑝_𝑖=ℙ(𝑦=1|𝑥_𝑖 ) \\] Podemos asumir que \\(𝑝_𝑖\\) sigue una distribución Bernoulli, de tal forma que su esperanza sea: \\[ 𝔼(𝑦|𝑥_𝑖 )=𝑝_𝑖×1+(1−𝑝_𝑖 )×0=𝑝_𝑖 \\] De tal forma que: \\[ 𝑝_𝑖= 𝛽_0+𝛽_1 𝑥_𝑖 \\] No obstante, los resultados pueden resultar absurdos debido a que nuestra fórmula nos puede proporcionar probabilidades negativas o mayores a cero \\(!\\) Las probabilidades no tienen valores superiores a 1 ni negativas, pero el concepto de momios \\(p / (1-p)\\) tiene como imagen de cero a infinito. La función exponencial tiene la misma imagen, entonces, la regresión logística relaciona ambos concpetos a través de la siguiente igualdad \\[ ℙ(𝑦=1|𝑥_𝑖 )/(1−ℙ(𝑦=1|𝑥_𝑖 ) )=𝑒^{(𝛽_0+𝛽_1 𝑥_𝑖 )} \\] Al aplicar el logaritmo natual, podemos apreciar la definición de la función logit: \\[ logit[𝑝_𝑖 ]=ln[𝑝_𝑖/(1−𝑝_𝑖 )]=𝛽_0+𝛽_1 𝑥_𝑖 \\] Recordando que la función logit es la inversa de la función logistica (o sigmoide) La función logit es apropiada para los problemas de clasificación con dos clases. No obstante, la generalización para múltiples clases se hace con la función Softmax() \\[ \\sigma_{softmax} = \\frac{e^{z_m}}{\\sum_{m=1}^M e^{z_m}} \\] La estructura aplicada en Redes Neuronales es la siguiente: 4.3 Redes neuronales 4.3.1 Historia Redes Neuronales En 1943 Warren McCulloch (neurocientífico, médico, neurólogo y fisiólogo) y Walter Pitts (matemático, psicólogo, filósofo y neurocientífico) crearon un modelo para redes neuronales basados en la lógica de umbral. Este modelo señaló el camino para que la investigación de redes neuronales se divida en dos enfoques distintos: Un enfoque centrado en los procesos biológicos en el cerebro. Otro en la aplicación de redes neuronales para la inteligencia artificial. Para la aplicación de la inteligencia artificial, fue Marvin Minsky quien dio una gran aportación con el libro “Perceptrones” en 1969. Minsky es considerado como uno de los padres de las ciencias de la computación y fue cofundador del laboratorio de inteligencia artificial del MIT 4.3.2 Definición Las redes neuronales son algoritmos que tienen como objetivo el simular en una computadora la forma en la cual funcionan las redes neuronales en los humanos. Puede recibir variables cuantitativas y cualitativas. Es un algoritmo sensible a cambios en los datos y parámetros 4.3.3 Neurona Célula del sistema nerviosos formada por un núcleo y una serie de prolongaciones, una de las cuales es más larga que las demás. Están especializadas en la recepción de estímulos y conducción del impulso nerviosos entre ellas o con otros tipos celulas Dentrita: Es la fuente de un impulso nerviosos. De hecho el impulso nervioso es unidireccional, es decir, solamente se transmite desde las dendritas hacia el axón (canal de entrada) Axón: Prolongación que arranca del cuerpo de la neurona y termina en una ramificación que está en contacto con otras células. Es la vía por la cual circulan los impulsos eléctricos (canal de salida) Sinapsis: Región de comunicación entre el axón de una neurona y las dendritas o el cuerpo de otra Emulación: Las redes neuronales, tratan de modelar una red neuronal donde cada nodo es una neurona, y los arcos representan la sinapsis entre las neuronas. Como algoritmo, las redes neuronales reciben n entradas y pueden otorgar n salidas. Un perceptrón puede entenderse como la unidad básica de inferencia en forma de discriminador lineal. Un perceptrón va a separar a los puntos a través de un hiperplano Es el modelo biológico más sencillo hace referencia a una sola neurona. El perceptrón consiste de dos tipos de nodos: Nodos de entrada: ingreso de los atributos Nodos de salida: representa a los resultados del modelo Un perceptrón genera sus resultados realizando una suma ponderada de sus atributos de entrada, restando un valor t y examinando el signo del resultado. Supongamos la siguiente tabla: El perceptrón sería aquella red neuronal que recibiera de entrada cada uno de los valores x y diera como resultado y \\[ y = \\begin{cases} \\text{1, si: } 0.3𝑥_1+0.3𝑥_1+0.3𝑥_1−0.4 ≥ 0 \\\\ \\text{-1, si: } 0.3𝑥_1+0.3𝑥_1+0.3𝑥_1−0.4 &lt; 0 \\\\ \\end{cases} \\] Como el resultado depende del signo, la función se representa como: \\[ 𝑦=𝑠𝑖𝑔𝑛(𝑤,𝑥) \\] Cuando intentamos hacer la separación de un XOr, podemos notar que necesitamos dos discriminadores lineales: 4.3.4 Redes Neuronales Las redes neuronales, son una estructura más compleja que un perceptrón. Algunas de las complejidades son: Las redes neuronales tienen varios nodos intermedios entre los nodos de entrada y los nodos de salida. Las capas ocultas pueden utilizar funciones de activación diferentes Con una capa oculta, es posible generar una función de discriminación para una tabla XOr Funciones de activación comunes Notas adicionales Curso de Git: https://www.youtube.com/watch?v=3GymExBkKjE&amp;t=195s "],["perceptrón-multicapa.html", "Capítulo 5 Perceptrón Multicapa 5.1 Perceptrones multicapa 5.2 Implementación de Perceptrones Multicapa 5.3 Forward Propagation, Backward Propagation 5.4 Estabilidad Numérica e Inicialización 5.5 Generalización en Deep Learning 5.6 Dropout", " Capítulo 5 Perceptrón Multicapa 5.1 Perceptrones multicapa En la Sección anterior, se presentó la regresión softmax, implementando el algoritmo desde cero. Esto permitió entrenar clasificadores capaces de reconocer 10 categorías de prendas de vestir a partir de imágenes de baja resolución. Durante el proceso, aprendimos a manipular los datos, a convertir las salidas en una distribución de probabilidad válida, a aplicar una función de pérdida adecuada y a minimizarla con respecto a los parámetros del modelo. Ahora que se conocen estos aspectos en el contexto de modelos lineales simples, podemos comenzar la exploración de las redes neuronales profundas, la clase de modelos relativamente rica que constituye el tema principal de este curso. 5.1.1 Capas ocultas En la sección 3.1., se describen las transformaciones afines como transformaciones lineales con un sesgo añadido. Para empezar, recordemos la arquitectura del modelo correspondiente a nuestro ejemplo de regresión softmax, ilustrado en la figura 4.1.1. Este modelo asigna directamente las entradas a las salidas mediante una única transformación afín, seguida de una operación softmax. Si nuestras etiquetas estuvieran realmente relacionadas con los datos de entrada mediante una simple transformación afín, este enfoque sería suficiente. Sin embargo, la linealidad (en las transformaciones afines) es una suposición muy restrictiva. 5.1.1.1 Limitaciones de los modelos lineales La linealidad supone una relación monotónica: si una variable aumenta, la salida del modelo siempre sube o baja según el signo del peso. Esto a veces es razonable, como al predecir el pago de un préstamo según ingresos, aunque la relación no sea estrictamente lineal. En estos casos, funciones como la logística pueden hacer más plausible la linealidad. Sin embargo, muchas relaciones reales no son monotónicas. Por ejemplo, la temperatura corporal: valores por encima o por debajo de 37 °C aumentan el riesgo, por lo que conviene transformar la variable (p. ej., usar la distancia a 37 °C). En problemas como la clasificación de imágenes, la linealidad es claramente insuficiente: cambiar el brillo de un solo píxel no determina si hay un gato o un perro. Aquí el significado de cada píxel depende de su contexto, y no existe un preprocesamiento simple que lo capture. Las redes neuronales profundas resuelven esto aprendiendo simultáneamente una representación adecuada y un predictor lineal sobre esa representación. La necesidad de modelar no linealidades se conoce desde hace un siglo y ha dado lugar a enfoques como árboles de decisión, métodos kernel, splines y, más recientemente, redes neuronales, inspiradas en las conexiones jerárquicas entre neuronas del cerebro. 5.1.1.2 Incorporación de Capas Ocultas Para superar las limitaciones de los modelos lineales, podemos agregar capas ocultas. La idea más básica es apilar varias capas totalmente conectadas: cada capa envía su salida a la siguiente. Las primeras capas aprenden una representación de los datos y la última capa actúa como un modelo lineal sobre esa representación. A esta arquitectura se le llama perceptrón multicapa o MLP. Observa la siguiente imagen que contempla un MLP con 4 entradas, 1 capa oculta con 5 neuronas y 3 salidas. Como la capa de entrada no calcula nada, realmente el modelo tiene 2 capas de cálculo: la oculta y la de salida. En un MLP, cada neurona de una capa está conectada con todas las neuronas de la siguiente. Es decir, cada entrada afecta a todas las neuronas ocultas, y cada una de estas afecta a todas las neuronas de salida. 5.1.1.3 De lo Lineal a lo No Lineal Como antes, denotamos con la matriz \\(X \\in \\mathbb{R}^{n \\times d}\\) un minibatch de \\(n\\) ejemplos, donde cada ejemplo tiene \\(d\\) entradas (características). Para un MLP de una capa oculta cuya capa oculta tiene \\(h\\) unidades ocultas, denotamos por \\(H \\in \\mathbb{R}^{n \\times h}\\) las salidas de la capa oculta, que son representaciones ocultas. Dado que las capas oculta y de salida están completamente conectadas, tenemos pesos de capas ocultas \\(W^{(1)} \\in \\mathbb{R}^{d \\times h}\\) y sesgos \\(b^{(1)} \\in \\mathbb{R}^{1 \\times h}\\) y pesos \\(W^{(2)} \\in \\mathbb{R}^{h \\times q}\\) y sesgos \\(b^{(2)} \\in \\mathbb{R}^{1 \\times q}\\) de la capa de salida. Esto nos permite calcular las salidas \\(O \\in \\mathbb{R}^{n \\times q}\\) del MLP de una capa oculta de la siguiente manera: \\[ H=XW^{(1)}+b^{(1)}, \\] \\[ O=HW^{(2)}+b^{(2)}. \\] ¡¡ RECORDATORIO !! Una función “Afín” está dada por: \\(f(x)=Ax+b\\), Mientras que una función lineal está dado por: \\(f(x)=Ax\\) Hay que tener en cuenta que, tras añadir la capa oculta, el modelo requiere que se rastree y actualicen conjuntos adicionales de parámetros. ¿Qué se ha ganado a cambio? Sorprendentemente, el modelo definido anteriormente, ¡NO GANA NADA! y la razón es sencilla. Las unidades ocultas anteriores se dan mediante una función afín de las entradas, y las salidas (pre-softmax) son simplemente una función afín de las unidades ocultas. La composición de funciones afines es todavía una función afín. Además, el modelo lineal ya era capaz de representar cualquier función afín. Para verlo formalmente, se puede simplemente colapsar la capa oculta en la definición anterior, lo que genera un modelo equivalente de una sola capa con parámetros. \\[ W=W^{(1)}W^{(2)} \\text{ and } b=b^{1}W^{(2)}+b^{(2)}: \\] \\[ O=(XW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}=XW^{(1)}W^{(2)}+b^{(1)}W^{(2)}+b^{(2)}=XW+b \\] Para aprovechar el potencial de las arquitecturas multicapa, se necesita un ingrediente clave adicional: una función de activación no lineal que se aplique a cada unidad oculta tras la transformación afín. na opción popular es la función de activación ReLU (unidad lineal rectificada) (Nair y Hinton, 2010) \\(\\sigma(x)=max(0, x)\\), que opera sobre sus argumentos elemento por elemento. Las salidas de las funciones de activación \\(\\sigma(\\cdot)\\) se denominan activaciones. En general, con las funciones de activación establecidas, ya no es posible convertir el MLP en un modelo lineal: \\[ H=\\sigma(XW^{(1)}+b^{(1)}), \\] \\[ O=HW^{(2)}+b^{(2)}. \\] Dado que cada fila en \\(X\\) corresponde a un ejemplo en el minibatch, con cierto abuso de notación, definimos la no linealidad \\(\\sigma\\) para aplicarla a sus entradas por filas, es decir, un ejemplo a la vez. Con frecuencia, las funciones de activación se aplican no solo por filas, sino también por elementos. Esto significa que, tras calcular la parte lineal de la capa, se calcula cada activación sin tener en cuenta los valores tomados por las demás unidades ocultas. 5.1.1.4 Aproximadores Universales ¿Qué es un aproximador universal? Es un resultado teórico que dice que una red neuronal con una sola capa oculta puede representar prácticamente cualquier función, si tiene suficientes neuronas y parámetros. Pero ojo: Que pueda representar cualquier función no significa que sea fácil aprenderla. Tener una sola capa oculta puede requerir muchísimas neuronas (algo poco práctico). A veces es mejor usar: métodos de kernels, cuando aplican, porque resuelven el problema de forma exacta; redes más profundas, que permiten representar las mismas funciones de manera más compacta y eficiente. Idea didáctica clave La profundidad importa. Con suficiente profundidad, una red puede aprender funciones complejas de forma más eficiente que una red muy ancha con una sola capa. 5.1.2 Funciones de activación Las funciones de activación determinan si una neurona debe activarse o no calculando la suma ponderada y agregándole un sesgo. Son operadores diferenciables que transforman señales de entrada en salidas, y la mayoría añade no linealidad. Dado que las funciones de activación son fundamentales para el aprendizaje profundo, se revisarán algunas de las más comunes. 5.1.2.1 Función ReLU La opción más popular, debido tanto a su simplicidad de implementación como a su buen rendimiento en diversas tareas predictivas, es la unidad lineal rectificada (ReLU) (Nair y Hinton, 2010). ReLU proporciona una transformación no lineal muy simple. Dado un elemento , la función se define como el máximo de ese elemento y 0: \\[ ReLU(x)=max(x, 0). \\] De manera informal, la función ReLU conserva solo los elementos positivos y descarta todos los negativos estableciendo las activaciones correspondientes a 0. Para una mayor comprensión, podemos representar gráficamente la función. Como se puede observar, la función de activación es lineal por partes. import torch import matplotlib.pyplot as plt # Datos x = torch.arange(-8.0, 8.0, 0.1) y = torch.relu(x) # Gráfica plt.figure(figsize=(5, 2.5)) plt.plot(x, y) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;ReLU(x)&quot;) plt.title(&quot;Función de activación ReLU&quot;) plt.grid(True) plt.show() Cuando la entrada es negativa, la derivada de la función ReLU es 0, y cuando la entrada es positiva, la derivada de la función ReLU es 1. Nótese que la función ReLU no es diferenciable cuando la entrada toma valor exactamente igual a 0. En estos casos, se usa por defecto la derivada del lado izquierdo y decimos que la derivada es 0 cuando la entrada es 0. Podemos evitar esto porque la entrada puede que nunca sea realmente cero (los matemáticos dirían que no es diferenciable en un conjunto de medida cero). Hay un viejo adagio que dice que si las condiciones de contorno sutiles importan, probablemente estemos haciendo matemáticas (reales), no ingeniería. Esa sabiduría convencional puede aplicarse aquí, o al menos, el hecho de que no estamos realizando optimización restringida (Mangasarian, 1965, Rockafellar, 1970). se traza la derivada de la función ReLU a continuación. x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True) y = torch.relu(x) y.backward(torch.ones_like(x)) plt.figure(figsize=(5, 2.5)) plt.plot(x.detach(), x.grad) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;grad ReLU(x)&#39;) plt.title(&#39;Derivada de ReLU&#39;) plt.grid(True) plt.show() La razón para usar ReLU es que sus derivadas se comportan particularmente bien: o se anulan o simplemente dejan pasar el argumento. Esto mejora el comportamiento de la optimización y mitiga el problema bien documentado de los gradientes evanescentes que afectaba a versiones anteriores de redes neuronales (más sobre esto más adelante). Cabe destacar que existen muchas variantes de la función ReLU, incluyendo la función ReLU parametrizada (pReLU) (He et al., 2015). Esta variación añade un término lineal a ReLU, por lo que parte de la información se transmite, incluso cuando el argumento es negativo: \\[ pReLU(x)=max(0,x)+\\alpha \\cdot min(0,x). \\] 5.1.2.2 Función Sigmoide La función sigmoidea transforma las entradas cuyos valores se encuentran en el dominio \\(\\mathbb{R}\\), en salidas que se encuentran en el intervalo (0, 1). Por esta razón, la función sigmoidea suele denominarse función de aplastamiento (squashing function): aplasta cualquier entrada en el rango (-inf, inf) a un valor en el rango (0, 1): \\[ \\text{sigmoid}(x)=\\frac{1}{1+\\text{exp}(-x)} \\] A continuación, se grafica la función sigmoidea. Nótese que cuando la entrada se acerca a 0, la función sigmoidea se aproxima a una transformación lineal. x = torch.arange(-8.0, 8.0, 0.1) y = torch.sigmoid(x) plt.figure(figsize=(5, 2.5)) plt.plot(x.detach(), y.detach()) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;sigmoid(x)&quot;) plt.title(&quot;Función Sigmoid&quot;) plt.grid(True) plt.show() La derivada de la función sigmoide está dada por la siguiente ecuación: \\[ \\frac{d}{dx}\\text{sigmoid}(x)=\\frac{\\text{exp(-x)}}{(1+\\text{exp}(-x))^2}=\\text{sigmoid}(x)(1-\\text{sigmoid}(x)) \\] La derivada de la función sigmoidea se grafica a continuación. Nótese que cuando la entrada es 0, la derivada de la función sigmoidea alcanza un máximo de 0,25. A medida que la entrada diverge de 0 en cualquier dirección, la derivada tiende a 0. # Definir x con gradientes x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True) y = torch.sigmoid(x) # Borrar gradientes previos (si los hubiera) if x.grad is not None: x.grad.zero_() # Backprop para obtener la derivada y.backward(torch.ones_like(x), retain_graph=True) # Graficar plt.figure(figsize=(5, 2.5)) plt.plot(x.detach(), x.grad.detach()) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;grad sigmoid(x)&quot;) plt.title(&quot;Derivada de la función Sigmoid&quot;) plt.grid(True) plt.show() 5.1.2.3 Función Tanh Al igual que la función sigmoidea, la función tangente hiperbólica también reduce sus valores de entrada, transformándolos en elementos del intervalo entre -1 y 1. \\[ \\text{tanh}(x)=\\frac{1-\\text{exp}(-2x)}{1+\\text{exp}(-2x)} \\] Nótese que, a medida que la entrada se acerca a 0, la función tanh se aproxima a una transformación lineal. Aunque la forma de la función es similar a la de la función sigmoidea, la función tanh presenta simetría puntual respecto al origen del sistema de coordenadas (Kalman y Kwasny, 1992). # Datos x = torch.arange(-8.0, 8.0, 0.1) y = torch.tanh(x) # Gráfica plt.figure(figsize=(5, 2.5)) plt.plot(x.detach(), y.detach()) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;tanh(x)&quot;) plt.title(&quot;Función tanh&quot;) plt.grid(True) plt.show() La derivada de la función tanh es: \\[ \\frac{d}{dx}\\text{tanh}(x)=1-\\text{tanh}^{2}(x) \\] A medida que la entrada se acerca a 0, la derivada de la función tanh se acerca a un máximo de 1. Y, como se vio con la función sigmoidea, a medida que la entrada se aleja de 0 en cualquier dirección, la derivada de la función tanh se acerca a 0. # Definir x con gradientes activados x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True) y = torch.tanh(x) # Borrar gradientes anteriores if x.grad is not None: x.grad.zero_() # Backprop para obtener la derivada y.backward(torch.ones_like(x), retain_graph=True) # Graficar la derivada plt.figure(figsize=(5, 2.5)) plt.plot(x.detach(), x.grad.detach()) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;grad tanh(x)&quot;) plt.title(&quot;Derivada de la función tanh&quot;) plt.grid(True) plt.show() 5.1.3 Discusión Ahora sabemos cómo incorporar no linealidades para construir arquitecturas expresivas de redes neuronales multicapa. Una ventaja de la función ReLU es que es significativamente más fácil de optimizar que la función sigmoidea o la función tanh. Se podría argumentar que esta fue una de las innovaciones clave que impulsaron el resurgimiento del aprendizaje profundo en la última década. Cabe destacar, sin embargo, que la investigación en funciones de activación no se ha detenido. Por ejemplo, la función de activación GELU (unidad lineal de error gaussiano) \\(x\\phi(x)\\) de Hendrycks y Gimpel (2016) \\(\\phi(x)\\) (es la función de distribución acumulativa gaussiana estándar) y la función de activación Swish \\(\\sigma(x)=x\\cdot\\text{sigmoid}(\\beta x)\\), propuesta por Ramachandran et al. (2017), pueden ofrecer una mayor precisión en muchos casos. 5.1.4 Ejercicios Demuestre que añadir capas a una red lineal profunda, es decir, una red sin no linealidad \\(\\sigma\\), nunca puede aumentar su potencia expresiva. Dé un ejemplo donde la reduzca activamente. Calcule la derivada de la función de activación pReLU. Calcule la derivada de la función de activación Swish \\(x\\cdot \\text{sigmoid}(\\beta x)\\). Demuestre que una MLP que utiliza solo ReLU (o pReLU) construye una función lineal continua por partes. Sigmoid y tanh son muy similares. Demuestre que \\(\\text{tanh}(x)+1 = 2\\text{sigmoid}(2x)\\). Demuestre que las clases de función parametrizadas por ambas no linealidades son idénticas. Pista: las capas afines también tienen términos de sesgo. Suponga que tenemos una no linealidad que se aplica a un minibatch a la vez, como la normalización por lotes (Ioffe y Szegedy, 2015). ¿Qué tipo de problemas espera que esto cause? Dé un ejemplo donde los gradientes se anulen para la función de activación sigmoide. 5.2 Implementación de Perceptrones Multicapa Los perceptrones multicapa (MLP) no son mucho más complejos de implementar que los modelos lineales simples. La diferencia conceptual clave radica en que ahora se concatenan múltiples capas. 5.2.1 Implementación desde Cero Comencemos de nuevo implementando dicha red desde cero. 5.2.1.1 Inicialización de parámetros del modelo El conjunto de datos Fashion-MNIST contiene 10 clases de imágenes y cada imagen consiste en una cuadrícula de valores de píxeles en escala de grises. Como antes, por ahora ignoraremos la estructura espacial entre los píxeles, por lo que podemos considerarlo como un conjunto de datos de clasificación con 784 características de entrada y 10 clases. Para comenzar, implementaremos un MLP con una capa oculta y 256 unidades ocultas. Tanto el número de capas como su ancho son ajustables (se consideran hiperparámetros). Tip: Normalmente, se busca que el ancho de las capas (número de neuronas) sea divisible por potencias mayores de 2, es decir: 4, 8, 16, 32, 64, 128, 256, 512, etc. Esto es computacionalmente eficiente debido a que las GPUs trabajan internamente con bloques de tamaño 32 y potencias de 2, haciendo que esos anchos sean: Más rápidos Más fáciles de paralelizar Más eficientes en memoria De nuevo, representaremos nuestros parámetros con varios tensores. Tenga en cuenta que, para cada capa, debemos registrar una matriz de ponderación y un vector de sesgo. Es importante tomar en cuenta la asignación de memoria para los gradientes de pérdida con respecto a estos parámetros. import torch from torch import nn import matplotlib.pyplot as plt import inspect class Module(nn.Module): def __init__(self): super().__init__() self.metrics = {} def save_hyperparameters(self, ignore=[]): frame = inspect.currentframe().f_back _, _, _, local_vars = inspect.getargvalues(frame) for k, v in local_vars.items(): if k not in ignore and k != &quot;self&quot;: setattr(self, k, v) def plot(self, name, value, train=True): &quot;&quot;&quot;Guarda valores para graficarlos al final.&quot;&quot;&quot; key = f&quot;{name}_{&#39;train&#39; if train else &#39;val&#39;}&quot; if key not in self.metrics: self.metrics[key] = [] self.metrics[key].append(value) def forward(self, X): raise NotImplementedError(&quot;Define forward() in subclass&quot;) class Classifier(Module): def loss(self, y_hat, y): return nn.CrossEntropyLoss()(y_hat, y) def accuracy(self, y_hat, y): preds = y_hat.argmax(dim=1) return (preds == y).float().mean() # def train_step(self, batch): # X, y = batch # y_hat = self.forward(X) # loss = self.loss(y_hat, y) # # # backprop # self.optimizer.zero_grad() # loss.backward() # self.optimizer.step() # # return loss.item(), self.accuracy(y_hat, y).item() def evaluation_step(self, X, y): with torch.no_grad(): y_hat = self(X) return ( self.loss(y_hat, y).item(), self.accuracy(y_hat, y).item(), ) # ------ Decorador para agregar métodos ------ def add_to_class(cls): def decorator(fn): setattr(cls, fn.__name__, fn) return fn return decorator En el código siguiente usamos nn.Parameter para registrar automáticamente un atributo de clase como un parámetro que será rastreado por autograd. class MLPScratch(Classifier): def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01): super().__init__() self.save_hyperparameters() self.lr = lr self.W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma) self.b1 = nn.Parameter(torch.zeros(num_hiddens)) self.W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma) self.b2 = nn.Parameter(torch.zeros(num_outputs)) 5.2.1.2 Modelo Para asegurarnos de que sabemos cómo funciona todo, implementaremos la activación de ReLU nosotros mismos en lugar de invocar directamente la función relu incorporada. def relu(X): a = torch.zeros_like(X) return torch.max(X, a) Como ignoramos la estructura espacial, transformamos cada imagen bidimensional en un vector plano de longitud num_inputs. Finalmente, implementamos nuestro modelo con solo unas pocas líneas de código. Como usamos el framework autograd integrado, esto es todo lo que necesitamos. # Añadir método usando el decorador @add_to_class(MLPScratch) def forward(self, X): X = X.reshape((-1, self.num_inputs)) H = relu(torch.matmul(X, self.W1) + self.b1) O = torch.matmul(H, self.W2) + self.b2 return O 5.2.1.3 Entrenamiento Afortunadamente, el ciclo de entrenamiento para las MLP es exactamente el mismo que para la regresión softmax. Definimos el modelo, los datos y el entrenador, y finalmente invocamos el método de ajuste en el modelo y los datos. Primero, unas clases auxiliares: DataLoader: from torch.utils.data import DataLoader from torchvision import datasets, transforms class FashionMNISTData: def __init__(self, batch_size=256): self.batch_size = batch_size self.transform = transforms.ToTensor() self.train = datasets.FashionMNIST( root=&quot;./data&quot;, train=True, download=True, transform=self.transform ) self.test = datasets.FashionMNIST( root=&quot;./data&quot;, train=False, download=True, transform=self.transform ) self.train_loader = DataLoader(self.train, batch_size=batch_size, shuffle=True) self.test_loader = DataLoader(self.test, batch_size=batch_size) def __iter__(self): return iter(self.train_loader) def __len__(self): return len(self.train_loader) Trainer minimalista class Trainer: def __init__(self, max_epochs=10): self.max_epochs = max_epochs def fit(self, model, data): model.metrics = {} optimizer = torch.optim.SGD(model.parameters(), lr=model.lr) # Asegura que exista test_loader if not hasattr(data, &quot;test_loader&quot;): raise ValueError(&quot;El dataset debe tener data.test_loader para validación&quot;) for epoch in range(self.max_epochs): total_loss, total_acc, count = 0, 0, 0 for X, y in data: optimizer.zero_grad() y_hat = model(X) loss = model.loss(y_hat, y) loss.backward() optimizer.step() total_loss += loss.item() * X.size(0) total_acc += model.accuracy(y_hat, y).item() * X.size(0) count += X.size(0) # Al final de cada epoch: guardar promedios de entrenamiento model.plot(&quot;loss&quot;, total_loss / count, train=True) model.plot(&quot;acc&quot;, total_acc / count, train=True) # ---------- VALIDACIÓN ---------- val_loss, val_acc, val_count = 0, 0, 0 for X, y in data.test_loader: l, a = model.evaluation_step(X, y) val_loss += l * X.size(0) val_acc += a * X.size(0) val_count += X.size(0) # guardar métricas de validación por epoch model.plot(&quot;loss&quot;, val_loss / val_count, train=False) model.plot(&quot;acc&quot;, val_acc / val_count, train=False) # -------- PRINT FULL METRICS -------- print( f&quot;Epoch {epoch+1}: &quot; f&quot;train_loss={total_loss/count:.3f}, &quot; f&quot;val_loss={val_loss/val_count:.3f}, &quot; f&quot;train_acc={total_acc/count:.3f}, &quot; f&quot;val_acc={val_acc/val_count:.3f}&quot; ) # Al final → graficar exactamente como el libro self._plot_metrics(model) def _plot_metrics(self, model): metrics = model.metrics n = len(metrics[&quot;loss_train&quot;]) epochs = range(n) plt.plot(epochs, metrics[&quot;loss_train&quot;], label=&quot;train_loss&quot;) #plt.plot(epochs, metrics[&quot;loss_val&quot;], label=&quot;val_loss&quot;) plt.plot(epochs, metrics[&quot;acc_train&quot;], label=&quot;train_acc&quot;) plt.plot(epochs, metrics[&quot;acc_val&quot;], label=&quot;val_acc&quot;) plt.xlabel(&quot;epoch&quot;) plt.legend() plt.title(&quot;Training and Validation Metrics&quot;) plt.show() plt.clf() # Limpia la figura actual plt.close() # Crear modelo model = MLPScratch( num_inputs=784, num_outputs=10, num_hiddens=256, lr=0.1 ) # Cargar datos (reemplaza d2l.FashionMNIST) data = FashionMNISTData(batch_size=256) trainer = Trainer(max_epochs=10) #trainer.fit(model, data) 5.2.2 Implementación La implementación pasada es bastante útil para entender cómo funciona el modelo, no obstante, al implementar las API de alto nivel, podemos crear MLP de forma aún más concisa. 5.2.2.1 Modelo En comparación con la implementación concisa de la regresión softmax, la única diferencia radica en que se añaden dos capas completamente conectadas donde antes solo se añadía una. La primera es la capa oculta y la segunda, la capa de salida. class MLP(Classifier): def __init__(self, num_outputs, num_hiddens, lr): super().__init__() self.save_hyperparameters() self.net = nn.Sequential( nn.Flatten(), nn.LazyLinear(num_hiddens), nn.ReLU(), nn.LazyLinear(num_outputs) ) def forward(self, X): return self.net(X) Anteriormente, definimos métodos forward para que los modelos transformen la entrada utilizando los parámetros del modelo. Estas operaciones son esencialmente una secuencia: se toma una entrada y se aplica una transformación (por ejemplo, multiplicación de matrices con pesos seguida de la adición de sesgos), y luego se utiliza repetidamente la salida de la transformación actual como entrada para la siguiente transformación. Sin embargo, es posible notar que aquí no se define ningún método forward. De hecho, MLP hereda el método de forward de la clase Module para simplemente invocar self.net(X) (X es la entrada), que ahora se define como una secuencia de transformaciones mediante la clase Sequential. La clase Sequential abstrae el proceso de avance (forward), lo que nos permite centrarnos en las transformaciones. Analizaremos con más detalle el funcionamiento de la clase Sequential en secciones posteriores. 5.2.2.2 Entrenamiento El ciclo de entrenamiento es exactamente el mismo que cuando implementamos la regresión softmax. Esta modularidad permite separar los aspectos relacionados con la arquitectura del modelo de las consideraciones ortogonales. model = MLP(num_outputs=10, num_hiddens=256, lr=0.1) #trainer.fit(model, data) 5.2.3 Ejercicios Cambie el número de unidades ocultas num_hiddens y grafique cómo su número afecta la precisión del modelo. ¿Cuál es el mejor valor para este hiperparámetro? Intente añadir una capa oculta para ver cómo afecta a los resultados. ¿Por qué es una mala idea insertar una capa oculta con una sola neurona? ¿Qué podría salir mal? ¿Cómo altera los resultados el cambio de la tasa de aprendizaje? Con todos los demás parámetros fijos, ¿qué tasa de aprendizaje ofrece los mejores resultados? ¿Cómo se relaciona esto con el número de épocas? Optimicemos todos los hiperparámetros conjuntamente: tasa de aprendizaje, número de épocas, número de capas ocultas y número de unidades ocultas por capa. ¿Cuál es el mejor resultado que se puede obtener optimizando todos ellos? ¿Por qué es mucho más difícil gestionar varios hiperparámetros? Describa una estrategia eficiente para optimizar varios parámetros conjuntamente. Compare la velocidad del framework y la implementación desde cero para un problema complejo. ¿Cómo cambia con la complejidad de la red? Mide la velocidad de las multiplicaciones tensor-matriz para matrices bien alineadas y desalineadas. Por ejemplo, prueba matrices con dimensiones 1024, 1025, 1026, 1028 y 1032. ¿Cómo cambia esto entre GPU y CPU? Determina el ancho del bus de memoria de tu CPU y GPU. Prueba diferentes funciones de activación. ¿Cuál funciona mejor? ¿Existe alguna diferencia entre las inicializaciones de peso de la red? ¿Tiene importancia? 5.3 Forward Propagation, Backward Propagation Hasta ahora hemos entrenado redes con descenso de gradiente usando minibatches, pero siempre confiando en que el framework calcule los gradientes por nosotros. Gracias a la diferenciación automática, no necesitamos derivar a mano expresiones complicadas, como se hacía antes en los artículos académicos. Pero para entender realmente cómo aprenden las redes, es importante saber qué ocurre detrás de escena. Por eso, en esta sección vamos a estudiar backpropagation, el mecanismo que calcula cómo cambia cada parámetro según el error cometido. Usaremos matemáticas básicas y grafos computacionales, trabajando con un ejemplo sencillo: un MLP de una capa oculta con regularización (weight decay \\(l_2\\)). Esto nos permitirá entender no sólo el qué, sino el cómo del aprendizaje profundo. 5.3.1 Forward Propagation La propagación hacia adelante (forward propagation o pase hacia adelante) se refiere al cálculo y almacenamiento de variables intermedias (incluidas las salidas) para una red neuronal, desde la capa de entrada hasta la capa de salida. A continuación, se explica paso a paso la mecánica de una red neuronal con una capa oculta. Para simplificar, supongamos que el ejemplo de entrada es \\(x\\in \\mathbb{R}^{d}\\) y que nuestra capa oculta no incluye un término de sesgo. En este caso, la variable intermedia es: \\[\\mathbb{z}=W^{(1)}x,\\] donde \\(W^{(1)} \\in \\mathbb{R}^{h\\times d}\\) es el peso del parámetro de las capas ocultas. Luego de ejecutar la variable intermedia \\(z\\in \\mathbb{R}^{h}\\) a través de la función de activación \\(\\phi\\), se obtiene el vector de activación oculto de longitud \\(h\\): \\[h=\\phi(z).\\] La salida de la capa oculta \\(h\\) también es una variable intermedia. Suponiendo que los parámetros de la capa de salida solo tienen un peso de \\(W^{(2)}\\in \\mathbb{R}^{q\\times h}\\), se puede obtener una variable de capa de salida con un vector de longitud \\(q\\): \\[\\mathbb{o}=W^{(2)}h\\] Asumiendo que la función de pérdida es \\(l\\) y la etiqueta de ejemplo es \\(y\\), se puede calcular el término de pérdida para un solo ejemplo de datos, \\[L=l(o,y)\\] Como veremos en la definición de regularización (\\(l_2\\)) que se presentará más adelante, dado el hiperparámetro \\(\\lambda\\), el término de regularización es: \\[s=\\frac{\\lambda}{2} \\left(||W^{(1)}||^{2}_{F}+||W^{(2)}||^{2}_{F} \\right),\\] Donde la norma de Frobenius de la matriz es simplemente la norma \\(l_2\\) aplicada tras aplanar la matriz a un vector. Finalmente, la pérdida regularizada del modelo en un ejemplo de datos dado es: \\[J=L+s.\\] Refiriéndose a \\(J\\) como la función objetivo en la siguiente discusión. A continuación se muestra el flujo computacional del proceso Forward. Figure 5.1: Computational graph of forward propagation of Dive into Deep Learning Book 5.3.2 Backpropagation La retropropagación se refiere al método para calcular el gradiente de los parámetros de una red neuronal. En resumen, el método recorre la red en orden inverso, desde la capa de salida hasta la de entrada, según la regla de la cadena del cálculo. El algoritmo almacena las variables intermedias (derivadas parciales) necesarias para calcular el gradiente con respecto a algunos parámetros. Supongamos que tenemos funciones \\(Y=f(X)\\) y \\(Z=g(Y)\\), donde la entrada y la salida \\(X, Y, Z\\) son tensores de formas arbitrarias. Utilizando la regla de la cadena, podemos calcular la derivada de \\(Z\\) con respecto a \\(X\\) mediante: \\[\\frac{\\partial Z}{\\partial X} = \\text{prod} \\left( \\frac{\\partial Z}{\\partial Y}, \\frac{\\partial Y}{\\partial X} \\right)\\] Aquí usamos el operador \\(prod\\) para multiplicar sus argumentos después de realizar las operaciones necesarias, como la transposición y el intercambio de posiciones de entrada. Para vectores, esto es sencillo: se trata simplemente de una multiplicación matriz-matriz. Para tensores de mayor dimensión, usamos el operador correspondiente. El operador \\(prod\\) elimina toda la sobrecarga de notación. El objetivo de la retropropagación es calcular los gradientes \\(\\frac{\\partial J}{\\partial W^{(1)}}\\) y \\(\\frac{\\partial J}{\\partial W^{(2)}}\\). Para ello, aplicamos la regla de la cadena y calculamos, a su vez, el gradiente de cada variable y parámetro intermedio. El orden de los cálculos se invierte con respecto a los realizados en la propagación hacia adelante, ya que debemos comenzar con el resultado del grafo computacional y avanzar hacia los parámetros. El primer paso es calcular los gradientes de la función objetivo \\(J=L+s\\) con respecto al término de pérdida \\(L\\) y al término de regularización \\(s\\): \\[ \\frac{\\partial J}{\\partial L} = 1 \\quad \\text{and} \\quad\\frac{\\partial J}{\\partial s}=1 \\] A continuación, calculamos el gradiente de la función objetivo con respecto a la variable de la capa de salida \\(o\\) según la regla de la cadena: \\[ \\frac{\\partial J}{\\partial o}=\\text{prod}\\left( \\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial o} \\right) = \\frac{\\partial J}{\\partial o}\\in \\mathbb{R}^{q}. \\] A continuación, calculamos los gradientes del término de regularización con respecto a ambos parámetros: \\[ \\frac{\\partial s}{\\partial W^{(1)}}=\\lambda W^{(1)} \\quad and \\quad \\frac{\\partial s}{\\partial W^{(2)}}=\\lambda W^{(2)} \\] Ahora podemos calcular el gradiente \\(\\frac{\\partial J}{\\partial W^{(2)}}\\) de los parámetros del modelo más cercanos a la capa de salida. Usando la regla de la cadena obtenemos: \\[ \\frac{\\partial J}{\\partial W^{(2)}}=prod\\left(\\frac{\\partial J}{\\partial o}, \\frac{\\partial o}{\\partial W^{(2)}}\\right)+prod\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial W^{(2)}}\\right)=\\frac{\\partial J}{\\partial o}h^{T}+\\lambda W^{(2)} \\] Para obtener el gradiente con respecto a \\(W^{(1)}\\), necesitamos continuar la retropropagación a lo largo de la capa de salida hasta la capa oculta. El gradiente con respecto a la salida de la capa oculta \\(\\frac{\\partial J}{\\partial h}\\in \\mathbb{R}^{h}\\) está dado por: \\[ \\frac{\\partial J}{\\partial h}=prod\\left(\\frac{\\partial J}{\\partial o}, \\frac{\\partial o}{\\partial h}\\right)=W^{(2)^{T}}\\frac{\\partial J}{\\partial o} \\] Dado que la función de activación \\(\\phi\\) se aplica elemento por elemento, para calcular el gradiente \\(\\frac{\\partial J}{\\partial z} \\in \\mathbb{R}^{h}\\) de la variable intermedia \\(z\\) es necesario utilizar el operador de multiplicación elemento por elemento, que denotamos como \\(\\odot\\): \\[ \\frac{\\partial J}{\\partial z}=prod\\left(\\frac{\\partial J}{\\partial h}, \\frac{\\partial h}{\\partial z}\\right)=\\frac{\\partial J}{\\partial h}\\odot\\phi^{\\prime}(z). \\] Finalmente, podemos obtener el gradiente \\(\\frac{\\partial J}{\\partial W^{(1)}}\\in \\mathbb{R}^{h\\times d}\\) de los parámetros del modelo más cercanos a la capa de entrada. Según la regla de la cadena, obtenemos: \\[ \\frac{\\partial J}{\\partial W^{(1)}}=prod\\left(\\frac{\\partial J}{\\partial z}, \\frac{\\partial z}{\\partial W^{(1)}}\\right)+prod\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial W^{(1)}}\\right)=\\frac{\\partial J}{\\partial z}x^{T}+\\lambda W^{(1)} \\] Al entrenar redes neuronales, la propagación hacia adelante y la retropropagación se necesitan mutuamente. En la fase hacia adelante se recorre el grafo computacional según sus dependencias y se calculan variables intermedias que luego serán reutilizadas en la retropropagación. En una red simple, por ejemplo, el término de regularización depende de los parámetros actuales \\(W^{(1)}\\) y \\(W^{(2)}\\), actualizados previamente mediante retropropagación. A su vez, el cálculo de gradientes en la retropropagación depende de valores generados hacia adelante, como la salida oculta $h&amp;. En resumen, el entrenamiento alterna continuamente entre ambas fases: la propagación hacia adelante produce valores intermedios, la retropropagación usa esos valores para obtener gradientes y actualizar parámetros, y esos parámetros actualizados se usan en la siguiente pasada hacia adelante. Esto requiere almacenar los valores intermedios hasta que termine la retropropagación. 5.3.3 Resumen La propagación hacia adelante calcula, paso a paso, las variables intermedias del grafo computacional de la red, avanzando desde la entrada hasta la salida. La retropropagación hace lo mismo pero en sentido inverso: calcula y almacena los gradientes de esas variables y de los parámetros. Durante el entrenamiento, ambas fases dependen una de la otra, por lo que se necesita mucha más memoria que en la simple fase de predicción. 5.3.4 Ejercicios Suponga que las entradas \\(X\\) de una función escalar \\(f\\) son matrices \\(n \\times m\\). ¿Cuál es la dimensionalidad del gradiente \\(f\\) de con respecto a \\(X\\)? Agregue un sesgo a la capa oculta del modelo descrito en esta sección (no es necesario incluirlo en el término de regularización). Dibuje el grafo computacional correspondiente. Obtenga las ecuaciones de propagación hacia adelante y hacia atrás. Calcule la huella de memoria para el entrenamiento y la predicción en el modelo descrito en esta sección. Suponga que desea calcular las derivadas secundarias. ¿Qué ocurre con el grafo computacional? ¿Cuánto tiempo espera que tarde el cálculo? Suponga que el grafo computacional es demasiado grande para su GPU. ¿Puede particionarlo en más de una GPU? ¿Cuáles son las ventajas y desventajas del entrenamiento en un minibatch más pequeño? 5.4 Estabilidad Numérica e Inicialización Hasta ahora, todos los modelos que hemos visto necesitaban inicializar sus parámetros usando alguna distribución predefinida. Hemos pasado por alto cómo se eligen realmente estos valores, lo que podría dar la impresión de que no importa demasiado. Pero en realidad, la forma de inicializar los parámetros es clave para que una red neuronal aprenda bien y mantenga la estabilidad numérica. ¡¡ RECORDAR !! la forma de inicializar los parámetros es clave para que una red neuronal aprenda bien y mantenga la estabilidad numérica. Además, la inicialización está estrechamente relacionada con la función de activación elegida. Ambas decisiones influyen en qué tan rápido converge el algoritmo de optimización. Una mala combinación puede provocar problemas típicos como gradientes que explotan o desaparecen (exploding or vanishing). En esta sección exploraremos estos temas con más detalle y veremos reglas prácticas que resultan muy útiles al trabajar con deep learning. 5.4.1 Explotación y Desvanecimiento de Gradientes Considera una red neuronal con \\(L\\) capas, entrada \\(x\\) y salida \\(o\\). Con cada capa \\(l\\) definida por la transformación \\(f_l\\) parametrizada por los pesos \\(W^{(l)}\\), cuya capa oculta de salida es \\(h^{(l)}\\) (siendo \\(h^{(0)}=x\\)), la red puede ser expresada como: \\[ h^{(l)}=f_l(h^{(l-1)}) \\quad \\text{y entonces} \\quad o=f_L \\circ \\cdot \\cdot \\cdot \\circ f_{1}(x) \\] Si la salida y la entrada de la capa oculta son vectores, podemos escribir el gradiente de \\(o\\) con respecto a cualquier conjunto de parámetros \\(W^{(l)}\\) de la siguiente manera: \\[ \\partial_{W^{(l)}}o=\\underbrace{\\partial_{h^{(l-1)}}h^{L}}_{M^{(L)}} \\cdot \\cdot \\cdot \\underbrace{\\partial_{h^{(l)}}h^{l+1}}_{M^{(l+1)}}\\underbrace{\\partial_{W^{(l)}}h^{l}}_{v^{(l)}} \\] Resumiendo, este gradiente es el producto de \\(L-l\\) matrices \\(M^{(L)}\\cdot\\cdot\\cdot M^{(l+1)}\\) y el vector gradiente \\(v^{(l)}.\\) Esto puede causar problemas numéricos similares a cuando multiplicamos muchas probabilidades y obtenemos valores extremadamente pequeños. En probabilidades solemos pasar a “log-espacio” para evitarlo, pero aquí el problema es más serio: las matrices involucradas \\(M^{(l)}\\) pueden tener eigenvalores muy grandes o muy pequeños, y su producto puede descontrolarse. Cuando los gradientes se vuelven inestables, no solo fallan las representaciones numéricas, sino que también se afecta el proceso de optimización. Podemos terminar con actualizaciones demasiado grandes, que destrozan el modelo (gradientes que explotan), o demasiado pequeñas, donde los parámetros casi no cambian y la red deja de aprender (gradientes que desaparecen). 5.4.1.1 Desvanecimiento de Gradientes Un culpable frecuente del problema de gradientes que desaparecen es la función de activación \\(\\sigma\\) que se aplica después de cada operación lineal. Históricamente, la función sigmoide \\(\\frac{1}{1+exp(-x)}\\) fue muy usada porque se parece a una función de umbral, evocando la idea de neuronas biológicas que “disparan” o no. Sin embargo, si se observa más de cerca la sigmoide, vemos por qué puede provocar que los gradientes se vuelvan muy pequeños y, por lo tanto, que el aprendizaje se frene. # Datos y cómputo de la sigmoide y su gradiente x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True) y = torch.sigmoid(x) y.backward(torch.ones_like(x)) # Convertimos a numpy para graficar x_np = x.detach().numpy() y_np = y.detach().numpy() grad_np = x.grad.detach().numpy() # Graficar plt.figure(figsize=(4.5, 2.5)) plt.plot(x_np, y_np, label=&#39;sigmoid&#39;) plt.plot(x_np, grad_np, label=&#39;gradient&#39;) plt.legend() plt.xlabel(&#39;x&#39;) plt.grid(True) plt.tight_layout() plt.show() La sigmoide tiene un inconveniente importante: su gradiente se hace casi cero cuando la entrada es muy grande o muy pequeña. Esto significa que, al retropropagar a través de muchas capas, el gradiente total tiende a desaparecer a menos que todas las sigmoides trabajen en una zona muy específica cerca de cero. En redes profundas esto era un problema común: en algún punto el gradiente simplemente se “cortaba”. ¡¡ RECORDAR !! Funciones como ReLU se volvieron la opción estándar. Aunque son menos parecidas a las neuronas biológicas, resultan mucho más estables y facilitan entrenar redes más profundas. 5.4.1.2 Gradientes Explosivos El problema opuesto, cuando los gradientes explotan, puede ser igualmente complejo. Para ilustrarlo mejor, dibujamos 100 matrices aleatorias gaussianas y las multiplicamos por una matriz inicial. Para la escala elegida (la elección de la varianza \\(\\sigma^{2}=1\\)), el producto de la matriz explota. Cuando esto ocurre debido a la inicialización de una red profunda, no tenemos posibilidad de lograr la convergencia de un optimizador de descenso de gradiente. M = torch.normal(0, 1, size=(4, 4)) print(&#39;a single matrix \\n&#39;,M) ## a single matrix ## tensor([[ 0.9059, -0.5833, 0.3908, 1.6229], ## [-0.0290, 1.2576, 1.6023, -0.5520], ## [-0.6166, -0.7285, -0.9683, -1.1237], ## [-0.6062, -0.0816, 1.6717, 1.7170]]) for i in range(100): M = M @ torch.normal(0, 1, size=(4, 4)) print(&#39;after multiplying 100 matrices\\n&#39;, M) ## after multiplying 100 matrices ## tensor([[-2.9649e+22, 1.4944e+23, -1.6709e+23, -9.2844e+22], ## [-1.9762e+22, 9.9603e+22, -1.1137e+23, -6.1882e+22], ## [ 3.4320e+22, -1.7298e+23, 1.9342e+23, 1.0747e+23], ## [-2.5175e+22, 1.2689e+23, -1.4188e+23, -7.8834e+22]]) 5.4.1.3 Rompiendo la Simetría Otro problema en el diseño de redes neuronales es la simetría inherente a su parametrización. Supongamos que tenemos un MLP simple con una capa oculta y dos unidades. En este caso, podríamos permutar los pesos \\(W^{(1)}\\) de la primera capa y, de igual manera, los de la capa de salida para obtener la misma función. No hay ninguna diferencia especial entre la primera y la segunda unidad oculta. En otras palabras, tenemos simetría de permutación entre las unidades ocultas de cada capa. Esto es más que una simple molestia teórica. Consideremos el MLP de una capa oculta mencionado anteriormente con dos unidades ocultas. A modo de ilustración, supongamos que la capa de salida transforma las dos unidades ocultas en una sola unidad de salida. Imaginemos qué sucedería si inicializáramos todos los parámetros de la capa oculta como \\(W^{(1)}=c\\) para una constante \\(c\\). En este caso, durante la propagación hacia adelante, cualquiera de las unidades ocultas toma las mismas entradas y parámetros, lo que produce la misma activación que se aplica a la unidad de salida. Durante la retropropagación, la diferenciación de la unidad de salida con respecto a los parámetros \\(W^{(1)}\\) da como resultado un gradiente cuyos elementos toman el mismo valor. Por lo tanto, tras una iteración basada en gradientes (p. ej., el descenso de gradiente estocástico en minibatch), todos los elementos de \\(W^{(1)}\\) siguen teniendo el mismo valor. Dichas iteraciones nunca romperían la simetría por sí solas y es posible que nunca seamos capaces de apreciar el potencial expresivo de la red. La capa oculta se comportaría como si solo tuviera una unidad. Cabe destacar que, si bien el descenso de gradiente estocástico en minibatch no rompería esta simetría, la regularización por abandono (que se presentará más adelante) sí lo haría. 5.4.2 Inicialización paramétrica Una forma de abordar, o al menos mitigar, los problemas planteados anteriormente es mediante una inicialización cuidadosa. Como veremos más adelante, un cuidado adicional durante la optimización y una regularización adecuada pueden mejorar aún más la estabilidad. 5.4.2.1 Inicialización Default En las secciones anteriores, utilizamos una distribución normal para inicializar los valores de nuestros pesos. Si no especificamos el método de inicialización, el framework utilizará un método de inicialización aleatorio predeterminado, que suele funcionar bien en la práctica para problemas de tamaño moderado. 5.4.2.2 Inicialización Xavier Veamos la distribución de escala de una salida \\(o_i\\) para una capa completamente conectada sin no linealidades. Con \\(n_{in}\\) entradas \\(x_{ij}\\) y sus pesos asociados \\(w_{ij}\\) para esta capa, la salida viene dada por \\[ o_i=\\sum_{j=1}^{n_{in}}{w_{ij}x_{j}} \\] Los pesos \\(w_{ij}\\) se extraen independientemente de la misma distribución. Además, supongamos que esta distribución tiene media y varianza \\(\\sigma^{2}\\) cero. Tenga en cuenta que esto no significa que la distribución deba ser gaussiana, solo que la media y la varianza deben existir. Por ahora, supongamos que las entradas de la capa \\(x_{j}\\) también tienen media y varianza \\(\\gamma^{2}\\) cero y que son independientes de \\(w_{ij}\\) e independientes entre sí. En este caso, podemos calcular la media de \\(o_i\\): \\[\\begin{align} E[o_i] &amp;= \\sum_{j=1}^{n_{in}}{E[w_{ij}x_{j}]} \\\\ &amp;= \\sum_{j=1}^{n_{in}}{E[w_{ij}]E[x_{j}]} \\\\ &amp;= 0 \\end{align}\\] y la varianza: \\[\\begin{align} Var[o_i] &amp;= E[o_{i}^{2}] - \\left(E[o_i] \\right)^{2} \\\\ &amp;= \\sum_{j=1}^{n_{in}}{E[w^{2}_{ij}x^{2}_{j}]} - 0 \\\\ &amp;= \\sum_{j=1}^{n_{in}}{E[w^{2}_{ij}]E[x^{2}_{j}]} \\\\ &amp;= n_{in}\\sigma^{2}\\gamma^{2}. \\end{align}\\] Una forma de mantener la varianza fija es establecer \\(n_{in}\\sigma^{2}=1\\). Consideremos ahora la retropropagación. En este caso, nos enfrentamos a un problema similar, aunque con gradientes que se propagan desde las capas más cercanas a la salida. Utilizando el mismo razonamiento que para la propagación hacia adelante, vemos que la varianza de los gradientes puede dispararse a menos que \\(n_{out}\\sigma^{2}=1\\), donde \\(n_{out}\\) es el número de salidas de esta capa. Esto nos plantea un dilema: no podemos satisfacer ambas condiciones simultáneamente. En su lugar, simplemente intentamos satisfacer: \\[ \\frac{1}{2}(n_{in}+n_{out})\\sigma^{2}=1 \\quad \\text{o equivalentemente } \\quad \\sigma=\\sqrt{\\frac{2}{n_{in}+n_{out}}} \\] Este es el razonamiento subyacente a la inicialización de Xavier, ahora estándar y prácticamente beneficiosa, llamada así por el primer autor de sus creadores (Glorot y Bengio, 2010). Normalmente, la inicialización de Xavier muestrea ponderaciones de una distribución gaussiana con media y varianza cero \\(\\sigma^{2}=\\frac{2}{n_{in}+n_{out}}\\). También podemos adaptar esto para elegir la varianza al muestrear ponderaciones de una distribución uniforme. Nótese que la distribución uniforme \\(U(-a, a)\\) tiene varianza \\(\\frac{a^{2}}{3}\\). Al sustituir \\(\\frac{a^{2}}{3}\\) en nuestra condición \\(\\sigma^{2}\\), nos lleva a inicializar según: \\[U\\left(-\\sqrt{\\frac{6}{n_{in}+n_{out}}}, \\sqrt{\\frac{6}{n_{in}+n_{out}}} \\right)\\] Aunque el supuesto de inexistencia de no linealidades en el razonamiento matemático anterior puede violarse fácilmente en redes neuronales, el método de inicialización de Xavier resulta funcionar bien en la práctica. 5.4.3 Ejercicios ¿Puedes diseñar otros casos donde una red neuronal pueda presentar simetría que deba romperse, además de la simetría de permutación en las capas de una MLP? ¿Podemos inicializar todos los parámetros de peso en la regresión lineal o en la regresión softmax con el mismo valor? Consulta los límites analíticos de los autovalores del producto de dos matrices. ¿Qué te indica esto sobre cómo asegurar que los gradientes estén bien condicionados? Si sabemos que algunos términos divergen, ¿podemos corregirlo posteriormente? Consulta el artículo sobre escalamiento de tasa adaptativo por capas para inspirarte (You et al., 2017). 5.5 Generalización en Deep Learning El teorema “no free lunch” nos recuerda que ningún algoritmo aprende bien en todas las situaciones: para generalizar necesitamos asumir algo sobre cómo es el mundo. Estas suposiciones se llaman sesgos inductivos. Por ejemplo, una red profunda tiene el sesgo de construir funciones complejas combinando funciones simples capa a capa. En machine learning solemos seguir dos pasos: Ajustar el modelo a los datos de entrenamiento. Medir la generalización evaluando en datos separados. La diferencia entre ambos resultados es la brecha de generalización, y si es grande hablamos de overfitting: el modelo “memoriza” el entrenamiento pero falla con ejemplos nuevos. En el enfoque clásico, el overfitting se relaciona con tener modelos demasiado complejos. La solución típica es regularizar, reduciendo parámetros o limitando su tamaño para evitar que el modelo se vuelva excesivamente flexible. Sin embargo, en deep learning esta idea se vuelve contraintuitiva. Las redes profundas son tan expresivas que pueden ajustar perfectamente incluso enormes conjuntos de datos. Lo sorprendente es que, aun alcanzando cero error de entrenamiento, a veces mejoramos la generalización haciendo el modelo más grande, añadiendo capas, neuronas o entrenando más tiempo. Esto da lugar al fenómeno de double descent, donde aumentar la complejidad primero empeora y luego mejora la generalización. En resumen, aunque las redes puedan memorizarlo todo, en la práctica generalizan, y muchas de las herramientas del deep learning —algunas que restringen el modelo y otras que lo hacen más flexible— se usan precisamente para controlar el overfitting. Lo curioso es que la teoría clásica no explica bien este éxito: medidas como VC dimension o Rademacher complexity no predicen por qué las redes profundas generalizan tan bien. 5.5.1 Sobreajuste y Regularización El teorema no free lunch dice que para generalizar necesitamos sesgos inductivos, es decir, suposiciones sobre cómo es el mundo. Las redes profundas tienen el sesgo de construir funciones complejas a partir de funciones simples. En ML primero ajustamos los datos de entrenamiento y luego medimos la generalización. Si la diferencia es grande, aparece el overfitting. En el enfoque clásico, esto se controla reduciendo la complejidad del modelo mediante regularización. Pero en deep learning el panorama es distinto: las redes pueden memorizar perfectamente el entrenamiento, y aun así generalizar mejor al hacerlas más grandes. Esto lleva al fenómeno de double descent, donde más complejidad primero perjudica y luego ayuda. Lo más curioso es que la teoría clásica no explica por qué las redes profundas generalizan tan bien. Aun así, en la práctica usamos una combinación de trucos y regularizaciones para reducir el overfitting y mejorar la generalización. 5.5.2 Inspiración de los no paramétricos Aunque las redes neuronales tienen millones de parámetros, a veces es más útil pensar en ellas como modelos no paramétricos. En estos modelos, la complejidad crece con la cantidad de datos, en lugar de estar fija desde el inicio. Un ejemplo clásico de modelo no paramétrico es k-nearest neighbors (k-NN): el algoritmo básicamente memoriza el conjunto de entrenamiento y, al predecir, busca los puntos más cercanos según alguna métrica de distancia. Con \\(k=1\\), siempre logra cero error de entrenamiento, y aun así puede generalizar bien bajo ciertas condiciones. Eso sí, distintas métricas de distancia implican diferentes sesgos inductivos y pueden producir predictores distintos. Las redes neuronales profundas, al estar sobrerrepresentadas (tienen muchos más parámetros de los necesarios para ajustar los datos), también tienden a interpolar o memorizar completamente el entrenamiento, comportándose en varios aspectos como modelos no paramétricos. De hecho, investigaciones recientes muestran una conexión profunda entre redes muy anchas y métodos de kernels. En particular, cuando un MLP se hace infinitamente ancho, su comportamiento converge a un método de kernel no paramétrico llamado neural tangent kernel (NTK). Aunque el NTK no explica todo lo que ocurre en redes modernas, sirve como herramienta teórica para entender por qué los modelos sobredimensionados pueden generalizar tan bien. 5.5.3 Early Stopping Las redes neuronales profundas pueden memorizar etiquetas arbitrarias o aleatorias, pero lo hacen tarde en el entrenamiento. Estudios recientes muestran que primero aprenden los ejemplos correctamente etiquetados y solo después empiezan a ajustar las etiquetas incorrectas o ruidosas. Esto tiene una implicación importante: si el modelo ya ajustó los datos limpios pero aún no ha memorizado las etiquetas aleatorias, entonces ya está generalizando. Este comportamiento motiva el uso de early stopping como técnica de regularización. En lugar de limitar los pesos, limitamos el número de épocas entrenadas. Lo usual es vigilar el error de validación y detener el entrenamiento cuando deja de mejorar después de cierta “paciencia”. Esto no solo mejora la generalización en presencia de ruido, sino que ahorra mucho tiempo y costo computacional, especialmente en modelos grandes. Cuando las etiquetas no contienen ruido (problemas bien separables como gatos vs. perros), el early stopping aporta poco. Pero cuando hay ruido, ambigüedad o variabilidad natural en las etiquetas (como en medicina), el early stopping se vuelve crucial, porque entrenar hasta que el modelo memorice el ruido suele ser perjudicial. 5.5.4 Métodos clásicos de regularización para redes profundas Anteriormente vimos técnicas clásicas de regularización, como weight decay, que añade un término a la función de pérdida para penalizar pesos grandes. Según la norma usada, se conoce como ridge (\\(l_2\\)) o lasso (\\(l_1\\)). En el enfoque clásico, estas penalizaciones limitan la complejidad del modelo y evitan que ajuste etiquetas arbitrarias. En deep learning, el weight decay sigue siendo muy usado, pero se ha observado que, por sí solo, no impide que las redes profundas interpolen completamente los datos. Por eso, sus beneficios suelen entenderse mejor cuando se combina con early stopping. Sin esta combinación, el efecto de técnicas como weight decay puede deberse menos a “restringir” la capacidad del modelo y más a introducir sesgos inductivos compatibles con los patrones reales de los datos. Aun así, los regularizadores clásicos siguen siendo populares en práctica. Además, el deep learning ha desarrollado variantes inspiradas en estas ideas, como agregar ruido durante el entrenamiento. En la siguiente sección aparecerá dropout, una técnica muy usada cuyo funcionamiento efectivo también tiene una base teórica parcial, pero funciona muy bien en la práctica. 5.5.5 Ejercicios ¿En qué sentido las medidas tradicionales basadas en la complejidad no tienen en cuenta la generalización de las redes neuronales profundas? ¿Por qué la interrupción anticipada podría considerarse una técnica de regularización? ¿Cómo suelen determinar los investigadores el criterio de parada? ¿Qué factor importante parece diferenciar los casos en los que la interrupción temprana conduce a grandes mejoras en la generalización? Más allá de la generalización, describa otro beneficio de dejar de fumar temprano. 5.6 Dropout Pensemos brevemente en lo que esperamos de un buen modelo predictivo. Queremos que tenga un buen desempeño sobre datos no vistos. La teoría clásica de generalización sugiere que, para cerrar la brecha entre el rendimiento en entrenamiento y en prueba, deberíamos aspirar a un modelo simple. La simplicidad puede presentarse en forma de un número pequeño de dimensiones. Otra noción útil de simplicidad es la suavidad, es decir, que la función no sea sensible a pequeños cambios en sus entradas. Por ejemplo, al clasificar imágenes, esperaríamos que añadir algo de ruido aleatorio a los píxeles sea en su mayoría inofensivo. Bishop (1995) formalizó esta idea cuando demostró que el entrenamiento con ruido en la entrada es equivalente a la regularización de Tikhonov. Este trabajo estableció una conexión matemática clara entre el requisito de que una función sea suave (y por tanto simple) y el requisito de que sea resistente a perturbaciones en la entrada. Posteriormente, Srivastava et al. (2014) desarrollaron una idea ingeniosa para aplicar también la idea de Bishop a las capas internas de una red. Su propuesta, llamada dropout, consiste en inyectar ruido durante el cálculo de cada capa interna en la propagación hacia adelante, y se ha convertido en una técnica estándar para entrenar redes neuronales. El método se llama dropout porque literalmente se “eliminan” algunas neuronas durante el entrenamiento. A lo largo del entrenamiento, en cada iteración, el dropout estándar consiste en poner a cero una fracción de los nodos en cada capa antes de calcular la capa siguiente, la técnica de dropout en sí ha demostrado ser duradera, y diversas variantes de dropout están implementadas en la mayoría de las bibliotecas de aprendizaje profundo. &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD En la regularización dropout estándar, se ponen a cero una fracción de los nodos en cada capa y luego se corrige el sesgo de cada capa normalizando por la fracción de nodos que se conservaron (no eliminados). En otras palabras, con una probabilidad de dropout \\(p\\), cada activación intermedia \\(h\\) se sustituye por una variable aleatoria \\(h&#39;\\) de la siguiente manera: \\[ h&#39; = \\begin{cases} 0 &amp; \\text{con probabilidad } p \\\\ \\frac{h}{1-p} &amp; \\text{en otro caso} \\end{cases} \\] Por diseño, la esperanza se mantiene inalterada, es decir, \\(E[h&#39;] = h\\). 5.6.1 Dropout en la práctica Recuerda el MLP con una capa oculta y cinco unidades ocultas de la Sección 4.1.1. Cuando aplicamos dropout a una capa oculta, poniendo a cero cada unidad oculta con probabilidad \\(p\\), el resultado puede verse como una red que contiene solo un subconjunto de las neuronas originales. En la Fig. 4.6.1, \\(h_2\\) y \\(h_5\\) han sido eliminadas. En consecuencia, el cálculo de las salidas ya no depende de \\(h_2\\) o \\(h_5\\) y sus respectivos gradientes también desaparecen al realizar la retropropagación (backpropagation). De esta manera, el cálculo de la capa de salida no puede ser excesivamente dependiente de ningún elemento individual de \\(h_1, \\dots, h_5\\). Figure 5.2: MLP antes y después del dropout Típicamente, desactivamos el dropout en el momento de la prueba (test time). Dado un modelo entrenado y un nuevo ejemplo, no eliminamos ningún nodo y, por lo tanto, no necesitamos normalizar. Sin embargo, existen algunas excepciones: algunos investigadores utilizan el dropout en el momento de la prueba como una heurística para estimar la incertidumbre de las predicciones de la red neuronal: si las predicciones coinciden a través de muchas salidas diferentes de dropout, entonces podríamos decir que la red tiene mayor confianza. 5.6.2 Implementación desde cero Para implementar la función de dropout para una sola capa, debemos extraer tantas muestras de una variable aleatoria de Bernoulli (binaria) como dimensiones tenga nuestra capa, donde la variable toma el valor \\(1\\) (mantener) con probabilidad \\(1 - p\\) y \\(0\\) (eliminar) con probabilidad \\(p\\). Una forma sencilla de implementar esto es extraer primero muestras de la distribución uniforme \\(U[0, 1]\\). Luego podemos conservar aquellos nodos para los cuales la muestra correspondiente sea mayor que \\(p\\), descartando el resto. En el siguiente código, implementamos una función dropout_layer que elimina elementos en el tensor de entrada X con probabilidad dropout, reescalando el resto como se describió anteriormente: dividiendo los supervivientes por 1.0 - dropout. def dropout_layer(X, dropout): assert 0 &lt;= dropout &lt;= 1 if dropout == 1: return torch.zeros_like(X) mask = (torch.rand(X.shape) &gt; dropout).float() return mask * X / (1.0 - dropout) Podemos probar la función dropout_layer con algunos ejemplos. En las siguientes líneas de código, pasamos nuestra entrada X a través de la operación de dropout, con probabilidades \\(0\\), \\(0.5\\) y \\(1\\), respectivamente. import torch X = torch.arange(16, dtype = torch.float32).reshape((2, 8)) print(&#39;dropout_p = 0:&#39;, dropout_layer(X, 0)) ## dropout_p = 0: tensor([[ 0., 1., 2., 3., 4., 5., 6., 7.], ## [ 8., 9., 10., 11., 12., 13., 14., 15.]]) print(&#39;dropout_p = 0.5:&#39;, dropout_layer(X, 0.5)) ## dropout_p = 0.5: tensor([[ 0., 0., 0., 0., 0., 0., 0., 14.], ## [16., 0., 20., 22., 24., 0., 28., 30.]]) print(&#39;dropout_p = 1:&#39;, dropout_layer(X, 1)) ## dropout_p = 1: tensor([[0., 0., 0., 0., 0., 0., 0., 0.], ## [0., 0., 0., 0., 0., 0., 0., 0.]]) 5.6.3 Definición del modelo El modelo a continuación aplica dropout a la salida de cada capa oculta (siguiendo a la función de activación). Podemos configurar las probabilidades de dropout para cada capa de forma separada. Una elección común es establecer una probabilidad de dropout más baja en las capas cercanas a la de entrada. Nos aseguramos de que el dropout esté activo únicamente durante el entrenamiento. class DropoutMLPScratch(Classifier): def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2, dropout_1, dropout_2, lr): super().__init__() self.save_hyperparameters() self.lin1 = nn.LazyLinear(num_hiddens_1) self.lin2 = nn.LazyLinear(num_hiddens_2) self.lin3 = nn.LazyLinear(num_outputs) self.relu = nn.ReLU() def forward(self, X): H1 = self.relu(self.lin1(X.reshape((X.shape[0], -1)))) if self.training: H1 = dropout_layer(H1, self.dropout_1) H2 = self.relu(self.lin2(H1)) if self.training: H2 = dropout_layer(H2, self.dropout_2) return self.lin3(H2) 5.6.4 Entrenamiento Lo siguiente es similar al entrenamiento de los MLP descritos anteriormente. hparams = { &#39;num_outputs&#39;:10, &#39;num_hiddens_1&#39;:256, &#39;num_hiddens_2&#39;:256, &#39;dropout_1&#39;:0.5, &#39;dropout_2&#39;:0.5, &#39;lr&#39;:0.1 } model = DropoutMLPScratch(**hparams) data = FashionMNISTData(batch_size=256) trainer = Trainer(max_epochs=10) trainer.fit(model, data) ## Epoch 1: train_loss=1.158, val_loss=0.822, train_acc=0.578, val_acc=0.684 ## Epoch 2: train_loss=0.689, val_loss=0.653, train_acc=0.755, val_acc=0.767 ## Epoch 3: train_loss=0.582, val_loss=0.585, train_acc=0.795, val_acc=0.793 ## Epoch 4: train_loss=0.529, val_loss=0.558, train_acc=0.813, val_acc=0.803 ## Epoch 5: train_loss=0.498, val_loss=0.520, train_acc=0.823, val_acc=0.819 ## Epoch 6: train_loss=0.476, val_loss=0.510, train_acc=0.832, val_acc=0.817 ## Epoch 7: train_loss=0.456, val_loss=0.489, train_acc=0.837, val_acc=0.827 ## Epoch 8: train_loss=0.442, val_loss=0.493, train_acc=0.843, val_acc=0.820 ## Epoch 9: train_loss=0.429, val_loss=0.472, train_acc=0.849, val_acc=0.835 ## Epoch 10: train_loss=0.419, val_loss=0.461, train_acc=0.851, val_acc=0.837 5.6.5 Implementación concisa Con las APIs de alto nivel, todo lo que necesitamos hacer es añadir una capa Dropout después de cada capa totalmente conectada (fully connected), pasando la probabilidad de dropout como único argumento a su constructor. Durante el entrenamiento, la capa Dropout eliminará aleatoriamente las salidas de la capa anterior (o, equivalentemente, las entradas a la capa posterior) de acuerdo con la probabilidad de dropout especificada. Cuando no se está en modo de entrenamiento, la capa Dropout simplemente deja pasar los datos durante la fase de prueba. class DropoutMLP(Classifier): def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2, dropout_1, dropout_2, lr): super().__init__() self.save_hyperparameters() self.net = nn.Sequential( nn.Flatten(), nn.LazyLinear(num_hiddens_1), nn.ReLU(), nn.Dropout(dropout_1), nn.LazyLinear(num_hiddens_2), nn.ReLU(), nn.Dropout(dropout_2), nn.LazyLinear(num_outputs) ) def forward(self, X): return self.net(X) A continuación, entrenamos el modelo. model = DropoutMLP(**hparams) trainer.fit(model, data) ## Epoch 1: train_loss=1.166, val_loss=0.799, train_acc=0.575, val_acc=0.697 ## Epoch 2: train_loss=0.684, val_loss=0.648, train_acc=0.754, val_acc=0.771 ## Epoch 3: train_loss=0.579, val_loss=0.588, train_acc=0.797, val_acc=0.789 ## Epoch 4: train_loss=0.525, val_loss=0.557, train_acc=0.813, val_acc=0.798 ## Epoch 5: train_loss=0.490, val_loss=0.533, train_acc=0.826, val_acc=0.813 ## Epoch 6: train_loss=0.469, val_loss=0.509, train_acc=0.833, val_acc=0.823 ## Epoch 7: train_loss=0.451, val_loss=0.486, train_acc=0.839, val_acc=0.827 ## Epoch 8: train_loss=0.439, val_loss=0.496, train_acc=0.844, val_acc=0.818 ## Epoch 9: train_loss=0.426, val_loss=0.465, train_acc=0.848, val_acc=0.837 ## Epoch 10: train_loss=0.417, val_loss=0.466, train_acc=0.853, val_acc=0.833 5.6.6 Resumen Además de controlar el número de dimensiones y el tamaño del vector de pesos, dropout es otra herramienta más para evitar el sobreajuste. A menudo, estas herramientas se utilizan de manera conjunta. Obsérvese que dropout se usa únicamente durante el entrenamiento: reemplaza una activación \\(h\\) por una variable aleatoria cuyo valor esperado es \\(h\\). 5.6.7 Ejercicios ¿Qué sucede si se cambian las probabilidades de dropout para la primera y la segunda capa? En particular, ¿qué ocurre si se intercambian las probabilidades de ambas capas? Diseña un experimento para responder a estas preguntas, describe tus resultados de forma cuantitativa y resume las conclusiones cualitativas. Incrementa el número de épocas y compara los resultados obtenidos al usar dropout con aquellos en los que no se utiliza. ¿Cuál es la varianza de las activaciones en cada capa oculta cuando se aplica dropout y cuando no se aplica? Dibuja una gráfica que muestre cómo evoluciona esta cantidad a lo largo del tiempo para ambos modelos. ¿Por qué dropout no se utiliza típicamente en la fase de prueba? Usando el modelo de esta sección como ejemplo, compara los efectos de usar dropout y weight decay (decaimiento de pesos). ¿Qué ocurre cuando dropout y weight decay se usan al mismo tiempo? ¿Son los resultados aditivos? ¿Existen rendimientos decrecientes (o algo peor)? ¿Se anulan entre sí? ¿Qué sucede si aplicamos dropout a los pesos individuales de la matriz de pesos en lugar de a las activaciones? Inventa otra técnica para inyectar ruido aleatorio en cada capa que sea distinta de la técnica estándar de dropout. ¿Puedes desarrollar un método que supere a dropout en el conjunto de datos Fashion-MNIST (para una arquitectura fija)? cbe3c29 (book from container) En la regularización dropout estándar, se ponen a cero una fracción de los nodos en cada capa y luego se corrige el sesgo de cada capa normalizando por la fracción de nodos que se conservaron (no eliminados). En otras palabras, con una probabilidad de dropout \\(p\\), cada activación intermedia \\(h\\) se sustituye por una variable aleatoria \\(h&#39;\\) de la siguiente manera:\\[h&#39; = \\begin{cases} 0 &amp; \\text{con probabilidad } p \\\\ \\frac{h}{1-p} &amp; \\text{en otro caso} \\end{cases}\\]Por diseño, la esperanza se mantiene inalterada, es decir, \\(E[h&#39;] = h\\). 5.6.8 Dropout en la práctica Recuerda el MLP con una capa oculta y cinco unidades ocultas de la Sección 4.1.1. Cuando aplicamos dropout a una capa oculta, poniendo a cero cada unidad oculta con probabilidad \\(p\\), el resultado puede verse como una red que contiene solo un subconjunto de las neuronas originales. En la Fig. 4.6.1, \\(h_2\\) y \\(h_5\\) han sido eliminadas. En consecuencia, el cálculo de las salidas ya no depende de \\(h_2\\) o \\(h_5\\) y sus respectivos gradientes también desaparecen al realizar la retropropagación (backpropagation). De esta manera, el cálculo de la capa de salida no puede ser excesivamente dependiente de ningún elemento individual de \\(h_1, \\dots, h_5\\). Figure 5.3: MLP antes y después del dropout Típicamente, desactivamos el dropout en el momento de la prueba (test time). Dado un modelo entrenado y un nuevo ejemplo, no eliminamos ningún nodo y, por lo tanto, no necesitamos normalizar. Sin embargo, existen algunas excepciones: algunos investigadores utilizan el dropout en el momento de la prueba como una heurística para estimar la incertidumbre de las predicciones de la red neuronal: si las predicciones coinciden a través de muchas salidas diferentes de dropout, entonces podríamos decir que la red tiene mayor confianza. 5.6.9 Implementación desde cero Para implementar la función de dropout para una sola capa, debemos extraer tantas muestras de una variable aleatoria de Bernoulli (binaria) como dimensiones tenga nuestra capa, donde la variable toma el valor \\(1\\) (mantener) con probabilidad \\(1 - p\\) y \\(0\\) (eliminar) con probabilidad \\(p\\). Una forma sencilla de implementar esto es extraer primero muestras de la distribución uniforme \\(U[0, 1]\\). Luego podemos conservar aquellos nodos para los cuales la muestra correspondiente sea mayor que \\(p\\), descartando el resto. En el siguiente código, implementamos una función dropout_layer que elimina elementos en el tensor de entrada X con probabilidad dropout, reescalando el resto como se describió anteriormente: dividiendo los supervivientes por 1.0 - dropout. def dropout_layer(X, dropout): assert 0 &lt;= dropout &lt;= 1 if dropout == 1: return torch.zeros_like(X) mask = (torch.rand(X.shape) &gt; dropout).float() return mask * X / (1.0 - dropout) Podemos probar la función dropout_layer con algunos ejemplos. En las siguientes líneas de código, pasamos nuestra entrada X a través de la operación de dropout, con probabilidades \\(0\\), \\(0.5\\) y \\(1\\), respectivamente. import torch X = torch.arange(16, dtype = torch.float32).reshape((2, 8)) print(&#39;dropout_p = 0:&#39;, dropout_layer(X, 0)) ## dropout_p = 0: tensor([[ 0., 1., 2., 3., 4., 5., 6., 7.], ## [ 8., 9., 10., 11., 12., 13., 14., 15.]]) print(&#39;dropout_p = 0.5:&#39;, dropout_layer(X, 0.5)) ## dropout_p = 0.5: tensor([[ 0., 0., 0., 0., 8., 0., 12., 0.], ## [ 0., 0., 20., 22., 0., 0., 0., 0.]]) print(&#39;dropout_p = 1:&#39;, dropout_layer(X, 1)) ## dropout_p = 1: tensor([[0., 0., 0., 0., 0., 0., 0., 0.], ## [0., 0., 0., 0., 0., 0., 0., 0.]]) 5.6.10 Definición del modelo El modelo a continuación aplica dropout a la salida de cada capa oculta (siguiendo a la función de activación). Podemos configurar las probabilidades de dropout para cada capa de forma separada. Una elección común es establecer una probabilidad de dropout más baja en las capas cercanas a la de entrada. Nos aseguramos de que el dropout esté activo únicamente durante el entrenamiento. class DropoutMLPScratch(Classifier): def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2, dropout_1, dropout_2, lr): super().__init__() self.save_hyperparameters() self.lin1 = nn.LazyLinear(num_hiddens_1) self.lin2 = nn.LazyLinear(num_hiddens_2) self.lin3 = nn.LazyLinear(num_outputs) self.relu = nn.ReLU() def forward(self, X): H1 = self.relu(self.lin1(X.reshape((X.shape[0], -1)))) if self.training: H1 = dropout_layer(H1, self.dropout_1) H2 = self.relu(self.lin2(H1)) if self.training: H2 = dropout_layer(H2, self.dropout_2) return self.lin3(H2) 5.6.11 Entrenamiento Lo siguiente es similar al entrenamiento de los MLP descritos anteriormente. hparams = { &#39;num_outputs&#39;:10, &#39;num_hiddens_1&#39;:256, &#39;num_hiddens_2&#39;:256, &#39;dropout_1&#39;:0.5, &#39;dropout_2&#39;:0.5, &#39;lr&#39;:0.1 } model = DropoutMLPScratch(**hparams) data = FashionMNISTData(batch_size=256) trainer = Trainer(max_epochs=10) trainer.fit(model, data) ## Epoch 1: train_loss=1.187, val_loss=0.825, train_acc=0.567, val_acc=0.695 ## Epoch 2: train_loss=0.698, val_loss=0.661, train_acc=0.748, val_acc=0.755 ## Epoch 3: train_loss=0.583, val_loss=0.593, train_acc=0.794, val_acc=0.787 ## Epoch 4: train_loss=0.528, val_loss=0.557, train_acc=0.813, val_acc=0.798 ## Epoch 5: train_loss=0.495, val_loss=0.513, train_acc=0.823, val_acc=0.819 ## Epoch 6: train_loss=0.472, val_loss=0.509, train_acc=0.833, val_acc=0.818 ## Epoch 7: train_loss=0.456, val_loss=0.487, train_acc=0.837, val_acc=0.829 ## Epoch 8: train_loss=0.443, val_loss=0.480, train_acc=0.845, val_acc=0.827 ## Epoch 9: train_loss=0.430, val_loss=0.505, train_acc=0.847, val_acc=0.815 ## Epoch 10: train_loss=0.422, val_loss=0.483, train_acc=0.850, val_acc=0.830 5.6.12 Implementación concisa Con las APIs de alto nivel, todo lo que necesitamos hacer es añadir una capa Dropout después de cada capa totalmente conectada (fully connected), pasando la probabilidad de dropout como único argumento a su constructor. Durante el entrenamiento, la capa Dropout eliminará aleatoriamente las salidas de la capa anterior (o, equivalentemente, las entradas a la capa posterior) de acuerdo con la probabilidad de dropout especificada. Cuando no se está en modo de entrenamiento, la capa Dropout simplemente deja pasar los datos durante la fase de prueba. class DropoutMLP(Classifier): def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2, dropout_1, dropout_2, lr): super().__init__() self.save_hyperparameters() self.net = nn.Sequential( nn.Flatten(), nn.LazyLinear(num_hiddens_1), nn.ReLU(), nn.Dropout(dropout_1), nn.LazyLinear(num_hiddens_2), nn.ReLU(), nn.Dropout(dropout_2), nn.LazyLinear(num_outputs) ) def forward(self, X): return self.net(X) A continuación, entrenamos el modelo. model = DropoutMLP(**hparams) trainer.fit(model, data) ## Epoch 1: train_loss=1.175, val_loss=0.824, train_acc=0.567, val_acc=0.705 ## Epoch 2: train_loss=0.692, val_loss=0.664, train_acc=0.753, val_acc=0.759 ## Epoch 3: train_loss=0.582, val_loss=0.585, train_acc=0.794, val_acc=0.792 ## Epoch 4: train_loss=0.529, val_loss=0.542, train_acc=0.813, val_acc=0.811 ## Epoch 5: train_loss=0.499, val_loss=0.514, train_acc=0.823, val_acc=0.818 ## Epoch 6: train_loss=0.475, val_loss=0.514, train_acc=0.832, val_acc=0.815 ## Epoch 7: train_loss=0.455, val_loss=0.521, train_acc=0.839, val_acc=0.815 ## Epoch 8: train_loss=0.440, val_loss=0.477, train_acc=0.843, val_acc=0.833 ## Epoch 9: train_loss=0.431, val_loss=0.490, train_acc=0.845, val_acc=0.824 ## Epoch 10: train_loss=0.423, val_loss=0.484, train_acc=0.850, val_acc=0.821 5.6.13 Resumen Además de controlar el número de dimensiones y el tamaño del vector de pesos, dropout es otra herramienta más para evitar el sobreajuste. A menudo, estas herramientas se utilizan de manera conjunta. Obsérvese que dropout se usa únicamente durante el entrenamiento: reemplaza una activación \\(h\\) por una variable aleatoria cuyo valor esperado es \\(h\\). 5.6.14 Ejercicios ¿Qué sucede si se cambian las probabilidades de dropout para la primera y la segunda capa? En particular, ¿qué ocurre si se intercambian las probabilidades de ambas capas? Diseña un experimento para responder a estas preguntas, describe tus resultados de forma cuantitativa y resume las conclusiones cualitativas. Incrementa el número de épocas y compara los resultados obtenidos al usar dropout con aquellos en los que no se utiliza. ¿Cuál es la varianza de las activaciones en cada capa oculta cuando se aplica dropout y cuando no se aplica? Dibuja una gráfica que muestre cómo evoluciona esta cantidad a lo largo del tiempo para ambos modelos. ¿Por qué dropout no se utiliza típicamente en la fase de prueba? Usando el modelo de esta sección como ejemplo, compara los efectos de usar dropout y weight decay (decaimiento de pesos). ¿Qué ocurre cuando dropout y weight decay se usan al mismo tiempo? ¿Son los resultados aditivos? ¿Existen rendimientos decrecientes (o algo peor)? ¿Se anulan entre sí? ¿Qué sucede si aplicamos dropout a los pesos individuales de la matriz de pesos en lugar de a las activaciones? Inventa otra técnica para inyectar ruido aleatorio en cada capa que sea distinta de la técnica estándar de dropout. ¿Puedes desarrollar un método que supere a dropout en el conjunto de datos Fashion-MNIST (para una arquitectura fija)? "],["guía-del-constructor.html", "Capítulo 6 Guía del Constructor", " Capítulo 6 Guía del Constructor "],["redes-neuronales-convolucionales.html", "Capítulo 7 Redes Neuronales Convolucionales", " Capítulo 7 Redes Neuronales Convolucionales "],["redes-neuronales-convolucionales-modernas.html", "Capítulo 8 Redes Neuronales Convolucionales Modernas", " Capítulo 8 Redes Neuronales Convolucionales Modernas "],["redes-neuronales-recurrentes.html", "Capítulo 9 Redes Neuronales Recurrentes", " Capítulo 9 Redes Neuronales Recurrentes "],["redes-neuronales-recurrentes-modernas.html", "Capítulo 10 Redes Neuronales Recurrentes Modernas", " Capítulo 10 Redes Neuronales Recurrentes Modernas "],["mecanismos-de-atención-y-transformers.html", "Capítulo 11 Mecanismos de Atención y Transformers", " Capítulo 11 Mecanismos de Atención y Transformers "],["algoritmos-de-optimización.html", "Capítulo 12 Algoritmos de Optimización", " Capítulo 12 Algoritmos de Optimización "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
