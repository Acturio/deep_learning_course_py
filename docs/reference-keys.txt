fig:unnamed-chunk-65
fig:unnamed-chunk-71
fig:unnamed-chunk-80
introducción-a-deep-learning
introducción-al-aprendizaje-automático
motivación-los-paradigmas-del-conocimiento
machine-learning-supervisado
aprendizaje-supervisado
componentes-del-aprendizaje-supervisado
tipos-de-datos-en-el-aprendizaje-supervisado
númericos
categóricos
series-de-tiempo
los-problemas-de-machine-learning
el-problema-inverso
generación-del-modelo-que-aproxime-al-sistema
por-qué-deep-learning
dimensión-vc
el-truco-del-kernel-svm
ingeniería-de-caracteristicas
beneficios-del-deep-learning
teorema-de-aproximación-universal
extracción-automática-de-características-feature-extraction
escalabilidad-con-datos-y-cómputo
preliminares
algebra-lineal
introducción
motivación
vectores
matrices
transformaciones-lineales
escalares
campos-algebraicos
dato-curioso
vectores-1
matrices-1
alternativas-conceptuales-para-entenderlas
propiedades-matemáticas
en-el-aprendizaje-profundo
resumen
tensores
intuición-geométrica
su-rol-en-el-aprendizaje-profundo
propiedades-básicas-de-la-aritmética-de-tensores
reducciones-sobre-tensores
ejemplos
intuición-geométrica-1
en-aprendizaje-profundo
tipos-comunes-de-reducciones
resumen-1
producto-punto
interpretación-geométrica
en-el-aprendizaje-profundo-1
resumen-2
producto-entre-matrices-y-vectores
interpretación-geométrica-1
teorema-del-rango-y-nulidad
en-aprendizaje-profundo-1
resumen-3
multiplicación-matricial
interpretación-geométrica-2
propiedades-importantes
en-el-aprendizaje-profundo-2
producto-matricial-como-cambio-de-base-del-espacio-vectorial
resumen-4
normas
ejemplos-de-normas
norma-euclidiana-o-l2
norma-manhattan-o-l1
norma-máxima-linfty
norma-frobenius
interpretación-geométrica-3
en-aprendizaje-profundo-2
resumen-5
cálculo-diferencial
introducción-1
motivación-1
derivadas-y-diferenciación
funciones
derivada
derivada-vectorial
algunas-formulas-y-reglas
reglas-útiles-para-derivar-funciones-multivariable
regla-de-la-cadena
interpretación-geométrica-de-la-derivada
teorema-fundamental-del-cálculo
autodiferenciación
historia
qué-es
fundamento-matemático
dos-modos-principales
modo-directo
el-jacobiano-j_fx-y
el-gradiente-nabla-gx-y
el-hessiano-h_gx-y
uso-de-estos-resultados
modo-inverso
redes-neuronales-lineales-para-regresión
introducción-2
regresiones-lineales-simples
función-de-pérdida-y-función-de-costo
función-de-pérdida-loss-function
función-de-costo-cost-function
supuestos-en-la-regresión-lineal
regresión-lineal-múltiple
estimación-de-los-parámetros
derivación-paso-a-paso
bondad-de-ajuste
supuestos-del-modelo-lineal-múltiple
regularización-en-la-regresión-lineal
ridge-regression
lasso-regression
elastic-net-regression
comparación-de-métodos
consideraciones
sesgo-y-varianza
sesgo
varianza
compromiso-sesgovarianza
en-regresión-lineal
conclusión-de-la-sección
redes-neuronales-lineales-para-clasificación
modelos-lineales-generalizados
historia-1
definición
regresión-logística-para-clasificación
redes-neuronales
historia-2
definición-1
neurona
redes-neuronales-1
perceptrón-multicapa
perceptrones-multicapa
capas-ocultas
limitaciones-de-los-modelos-lineales
incorporación-de-capas-ocultas
de-lo-lineal-a-lo-no-lineal
aproximadores-universales
funciones-de-activación
función-relu
función-sigmoide
función-tanh
discusión
ejercicios
implementación-de-perceptrones-multicapa
implementación-desde-cero
inicialización-de-parámetros-del-modelo
modelo
entrenamiento
implementación
modelo-1
entrenamiento-1
ejercicios-1
forward-propagation-backward-propagation
forward-propagation
backpropagation
resumen-6
ejercicios-2
estabilidad-numérica-e-inicialización
explotación-y-desvanecimiento-de-gradientes
desvanecimiento-de-gradientes
gradientes-explosivos
rompiendo-la-simetría
inicialización-paramétrica
inicialización-default
inicialización-xavier
ejercicios-3
generalización-en-deep-learning
sobreajuste-y-regularización
inspiración-de-los-no-paramétricos
early-stopping
métodos-clásicos-de-regularización-para-redes-profundas
ejercicios-4
dropout
dropout-en-la-práctica
implementación-desde-cero-1
definición-del-modelo
entrenamiento-2
implementación-concisa
resumen-7
ejercicios-5
dropout-en-la-práctica-1
implementación-desde-cero-2
definición-del-modelo-1
entrenamiento-3
implementación-concisa-1
resumen-8
ejercicios-6
gpus
paralelismo-y-concurrencia
reflexión-concurrencia-y-paralelismo
qué-es-concurrencia
qué-es-paralelismo
relación-entre-concurrencia-y-paralelismo
conclusión
complejidad-computacional
analizando-la-complejidad-computacional-en-el-problema-de-ordenamiento-de-números
notación-big-o
bubble-sort-vs-merge-sort.
bubble-sort
merge-sort
comparación-entre-algoritmos-de
conclusión-1
el-papel-del-hardware
implementación-de-multiplicación-de-matrices-en-distintas-arquitecturas
es-viable-acelerar-cualquier-algoritmo-en-gpu-tpu
el-costo-oculto-mover-los-datos
el-segundo-gran-obstáculo-la-concurrencia-homogeneizable
cuándo-merece-la-pena-usar-una-gpu
entonces-aceleramos-cualquier-algoritmo
cuales-son-las-arquitecturas-más-comunes-y-favorables-para-procesar-algoritmos-de-deeplearning
cuales-son-las-arquitecturas-más-comunes-y-favorables-para-procesar-algoritmos-de-deep-learning
redes-neuronales-convolucionales
redes-neuronales-convolucionales-modernas
redes-neuronales-recurrentes
redes-neuronales-recurrentes-modernas
mecanismos-de-atención-y-transformers
algoritmos-de-optimización
introducción-a-la-teoría-de-gráficas.
qué-es-una-gráfica
problemas-clásicos-de-teoría-de-gráficas-selección
caminos-y-conectividad
detección-de-comunidades
centralidad-e-influencia
emparejamiento-y-asignación
ecosistema-de-herramientas-para-el-trabajo-con-grafos
por-qué-combinar-graficas-y-deep-learning
a.-multilayer-percepton-en-las-features-tabulares-de-cora
b.-modelo-basado-en-una-capa-lineal-que-se-multiplica-por-la-matriz-de-adyacencia.
qué-esta-haciendo-la-red
tipos-de-problemas-de-gnn
existe-un-teorema-de-aproximación-univeral-tau-para-gnns
enfoques-de-aprendizaje-transductivo-e-inductivo
graph-neural-network
pytorch-geometric-y-graph-neural-networks
graph-convolutional-networks-gnn
cómo-funcionan
implementación-en-pytorch-geometric
ejemplo-predicción-del-volumen-de-tráfico-en-wikipedia-regresión
ejemplo-clasificación-de-artículos-de-investigación-por-categoría
graph-attention-network-gat
cómo-funcionan-1
implementación-en-pytorch-geometric-1
ejemplo-clasificación-de-artículos-de-investigación-por-categoría-parte-ii
embeddings-y-graph-neural-network
qué-es-un-embedding
word2vec
node2vec
ejemplo-aplicando-el-modelo-de-skipgrams-a-random-walks
sageconv
ejemplo-clasificación-de-artículos-de-investigación-por-categoría-parte-ii-1
ejemplo-clasificación-de-artículos-de-investigación-por-categoría-parte-ii-2
