<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 3 Redes neuronales Lineales para Regresión | Deep Learning</title>
  <meta name="description" content="Capítulo 3 Redes neuronales Lineales para Regresión | Deep Learning" />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 3 Redes neuronales Lineales para Regresión | Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Capítulo 3 Redes neuronales Lineales para Regresión | Deep Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 3 Redes neuronales Lineales para Regresión | Deep Learning" />
  
  <meta name="twitter:description" content="Capítulo 3 Redes neuronales Lineales para Regresión | Deep Learning" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="preliminares.html"/>
<link rel="next" href="redes-neuronales-lineales-para-clasificación.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="img/aif-logo.png" width="280"></a></li|
|:-:|  
<center>Curso de Redes Neuronales Artificiales</center>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>BIENVENIDA</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#objetivo"><i class="fa fa-check"></i>Objetivo</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#alcances-del-programa"><i class="fa fa-check"></i>Alcances del Programa</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#temario"><i class="fa fa-check"></i>Temario:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#c%C3%B3digo"><i class="fa fa-check"></i>Código</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#duraci%C3%B3n-y-evaluaci%C3%B3n-del-programa"><i class="fa fa-check"></i>Duración y evaluación del programa</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recursos-y-din%C3%A1mica"><i class="fa fa-check"></i>Recursos y dinámica</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agenda"><i class="fa fa-check"></i>Agenda</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bibliograf%C3%ADa"><i class="fa fa-check"></i>Bibliografía</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Parte 1: Bases y Preeliminares</b></span></li>
<li class="chapter" data-level="1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html"><i class="fa fa-check"></i><b>1</b> Introducción a Deep Learning</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#introducci%C3%B3n-al-aprendizaje-autom%C3%A1tico"><i class="fa fa-check"></i><b>1.1</b> Introducción al aprendizaje automático</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#motivaci%C3%B3n-los-paradigmas-del-conocimiento"><i class="fa fa-check"></i><b>1.1.1</b> Motivación: Los paradigmas del conocimiento</a></li>
<li class="chapter" data-level="1.1.2" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#machine-learning-supervisado"><i class="fa fa-check"></i><b>1.1.2</b> Machine learning supervisado</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#por-qu%C3%A9-deep-learning"><i class="fa fa-check"></i><b>1.2</b> ¿Por qué Deep Learning?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#dimensi%C3%B3n-vc"><i class="fa fa-check"></i><b>1.2.1</b> Dimensión VC</a></li>
<li class="chapter" data-level="1.2.2" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#el-truco-del-kernel-svm"><i class="fa fa-check"></i><b>1.2.2</b> El truco del Kernel (SVM)</a></li>
<li class="chapter" data-level="1.2.3" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#ingenier%C3%ADa-de-caracteristicas"><i class="fa fa-check"></i><b>1.2.3</b> Ingeniería de caracteristicas</a></li>
<li class="chapter" data-level="1.2.4" data-path="introducción-a-deep-learning.html"><a href="introducción-a-deep-learning.html#beneficios-del-deep-learning"><i class="fa fa-check"></i><b>1.2.4</b> Beneficios del deep learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="preliminares.html"><a href="preliminares.html"><i class="fa fa-check"></i><b>2</b> Preliminares</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminares.html"><a href="preliminares.html#algebra-lineal"><i class="fa fa-check"></i><b>2.1</b> Algebra Lineal</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="preliminares.html"><a href="preliminares.html#introducci%C3%B3n"><i class="fa fa-check"></i><b>2.1.1</b> Introducción</a></li>
<li class="chapter" data-level="2.1.2" data-path="preliminares.html"><a href="preliminares.html#motivaci%C3%B3n"><i class="fa fa-check"></i><b>2.1.2</b> Motivación</a></li>
<li class="chapter" data-level="2.1.3" data-path="preliminares.html"><a href="preliminares.html#escalares"><i class="fa fa-check"></i><b>2.1.3</b> Escalares</a></li>
<li class="chapter" data-level="2.1.4" data-path="preliminares.html"><a href="preliminares.html#vectores-1"><i class="fa fa-check"></i><b>2.1.4</b> Vectores</a></li>
<li class="chapter" data-level="2.1.5" data-path="preliminares.html"><a href="preliminares.html#matrices-1"><i class="fa fa-check"></i><b>2.1.5</b> Matrices</a></li>
<li class="chapter" data-level="2.1.6" data-path="preliminares.html"><a href="preliminares.html#tensores"><i class="fa fa-check"></i><b>2.1.6</b> Tensores</a></li>
<li class="chapter" data-level="2.1.7" data-path="preliminares.html"><a href="preliminares.html#reducciones-sobre-tensores"><i class="fa fa-check"></i><b>2.1.7</b> Reducciones sobre tensores</a></li>
<li class="chapter" data-level="2.1.8" data-path="preliminares.html"><a href="preliminares.html#producto-punto"><i class="fa fa-check"></i><b>2.1.8</b> Producto punto</a></li>
<li class="chapter" data-level="2.1.9" data-path="preliminares.html"><a href="preliminares.html#producto-entre-matrices-y-vectores"><i class="fa fa-check"></i><b>2.1.9</b> Producto entre matrices y vectores</a></li>
<li class="chapter" data-level="2.1.10" data-path="preliminares.html"><a href="preliminares.html#multiplicaci%C3%B3n-matricial"><i class="fa fa-check"></i><b>2.1.10</b> Multiplicación matricial</a></li>
<li class="chapter" data-level="2.1.11" data-path="preliminares.html"><a href="preliminares.html#normas"><i class="fa fa-check"></i><b>2.1.11</b> Normas</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="preliminares.html"><a href="preliminares.html#c%C3%A1lculo-diferencial"><i class="fa fa-check"></i><b>2.2</b> Cálculo Diferencial</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="preliminares.html"><a href="preliminares.html#introducci%C3%B3n-1"><i class="fa fa-check"></i><b>2.2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2.2" data-path="preliminares.html"><a href="preliminares.html#motivaci%C3%B3n-1"><i class="fa fa-check"></i><b>2.2.2</b> Motivación</a></li>
<li class="chapter" data-level="2.2.3" data-path="preliminares.html"><a href="preliminares.html#derivadas-y-diferenciaci%C3%B3n"><i class="fa fa-check"></i><b>2.2.3</b> Derivadas y diferenciación</a></li>
<li class="chapter" data-level="2.2.4" data-path="preliminares.html"><a href="preliminares.html#regla-de-la-cadena"><i class="fa fa-check"></i><b>2.2.4</b> Regla de la cadena</a></li>
<li class="chapter" data-level="2.2.5" data-path="preliminares.html"><a href="preliminares.html#interpretaci%C3%B3n-geom%C3%A9trica-de-la-derivada"><i class="fa fa-check"></i><b>2.2.5</b> Interpretación geométrica de la derivada</a></li>
<li class="chapter" data-level="2.2.6" data-path="preliminares.html"><a href="preliminares.html#teorema-fundamental-del-c%C3%A1lculo"><i class="fa fa-check"></i><b>2.2.6</b> Teorema fundamental del cálculo</a></li>
<li class="chapter" data-level="2.2.7" data-path="preliminares.html"><a href="preliminares.html#autodiferenciaci%C3%B3n"><i class="fa fa-check"></i><b>2.2.7</b> Autodiferenciación</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html"><i class="fa fa-check"></i><b>3</b> Redes neuronales Lineales para Regresión</a>
<ul>
<li class="chapter" data-level="3.1" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#introducci%C3%B3n-2"><i class="fa fa-check"></i><b>3.1</b> Introducción</a></li>
<li class="chapter" data-level="3.2" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#regresiones-lineales-simples"><i class="fa fa-check"></i><b>3.2</b> Regresiones lineales simples</a></li>
<li class="chapter" data-level="3.3" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#funci%C3%B3n-de-p%C3%A9rdida-y-funci%C3%B3n-de-costo"><i class="fa fa-check"></i><b>3.3</b> Función de pérdida y función de costo</a></li>
<li class="chapter" data-level="3.4" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#supuestos-en-la-regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>3.4</b> Supuestos en la regresión lineal</a></li>
<li class="chapter" data-level="3.5" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#regresi%C3%B3n-lineal-m%C3%BAltiple"><i class="fa fa-check"></i><b>3.5</b> Regresión Lineal Múltiple</a></li>
<li class="chapter" data-level="3.6" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#estimaci%C3%B3n-de-los-par%C3%A1metros"><i class="fa fa-check"></i><b>3.6</b> Estimación de los parámetros</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#derivaci%C3%B3n-paso-a-paso"><i class="fa fa-check"></i><b>3.6.1</b> Derivación paso a paso</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#bondad-de-ajuste"><i class="fa fa-check"></i><b>3.7</b> Bondad de ajuste</a></li>
<li class="chapter" data-level="3.8" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#supuestos-del-modelo-lineal-m%C3%BAltiple"><i class="fa fa-check"></i><b>3.8</b> Supuestos del modelo lineal múltiple</a></li>
<li class="chapter" data-level="3.9" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#regularizaci%C3%B3n-en-la-regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>3.9</b> Regularización en la Regresión Lineal</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#ridge-regression"><i class="fa fa-check"></i><b>3.9.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="3.9.2" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#lasso-regression"><i class="fa fa-check"></i><b>3.9.2</b> Lasso Regression</a></li>
<li class="chapter" data-level="3.9.3" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#elastic-net-regression"><i class="fa fa-check"></i><b>3.9.3</b> Elastic Net Regression</a></li>
<li class="chapter" data-level="3.9.4" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#comparaci%C3%B3n-de-m%C3%A9todos"><i class="fa fa-check"></i><b>3.9.4</b> Comparación de métodos</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#consideraciones"><i class="fa fa-check"></i><b>3.10</b> Consideraciones</a></li>
<li class="chapter" data-level="3.11" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#sesgo-y-varianza"><i class="fa fa-check"></i><b>3.11</b> Sesgo y Varianza</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#sesgo"><i class="fa fa-check"></i><b>3.11.1</b> Sesgo</a></li>
<li class="chapter" data-level="3.11.2" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#varianza"><i class="fa fa-check"></i><b>3.11.2</b> Varianza</a></li>
<li class="chapter" data-level="3.11.3" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#compromiso-sesgovarianza"><i class="fa fa-check"></i><b>3.11.3</b> Compromiso sesgo–varianza</a></li>
<li class="chapter" data-level="3.11.4" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#en-regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>3.11.4</b> En regresión lineal</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="redes-neuronales-lineales-para-regresión.html"><a href="redes-neuronales-lineales-para-regresión.html#conclusi%C3%B3n-de-la-secci%C3%B3n"><i class="fa fa-check"></i><b>3.12</b> Conclusión de la sección</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html"><i class="fa fa-check"></i><b>4</b> Redes neuronales Lineales para Clasificación</a>
<ul>
<li class="chapter" data-level="4.1" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#modelos-lineales-generalizados"><i class="fa fa-check"></i><b>4.1</b> Modelos Lineales Generalizados</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#historia-1"><i class="fa fa-check"></i><b>4.1.1</b> Historia</a></li>
<li class="chapter" data-level="4.1.2" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#definici%C3%B3n"><i class="fa fa-check"></i><b>4.1.2</b> Definición</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#regresi%C3%B3n-log%C3%ADstica-para-clasificaci%C3%B3n"><i class="fa fa-check"></i><b>4.2</b> Regresión Logística para Clasificación</a></li>
<li class="chapter" data-level="4.3" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#redes-neuronales"><i class="fa fa-check"></i><b>4.3</b> Redes neuronales</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#historia-2"><i class="fa fa-check"></i><b>4.3.1</b> Historia</a></li>
<li class="chapter" data-level="4.3.2" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#definici%C3%B3n-1"><i class="fa fa-check"></i><b>4.3.2</b> Definición</a></li>
<li class="chapter" data-level="4.3.3" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#neurona"><i class="fa fa-check"></i><b>4.3.3</b> Neurona</a></li>
<li class="chapter" data-level="4.3.4" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#redes-neuronales-1"><i class="fa fa-check"></i><b>4.3.4</b> Redes Neuronales</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="redes-neuronales-lineales-para-clasificación.html"><a href="redes-neuronales-lineales-para-clasificación.html#notas-adicionales"><i class="fa fa-check"></i>Notas adicionales</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html"><i class="fa fa-check"></i><b>5</b> Perceptrón Multicapa</a>
<ul>
<li class="chapter" data-level="5.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#perceptrones-multicapa"><i class="fa fa-check"></i><b>5.1</b> Perceptrones multicapa</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#capas-ocultas"><i class="fa fa-check"></i><b>5.1.1</b> Capas ocultas</a></li>
<li class="chapter" data-level="5.1.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#funciones-de-activaci%C3%B3n"><i class="fa fa-check"></i><b>5.1.2</b> Funciones de activación</a></li>
<li class="chapter" data-level="5.1.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#discusi%C3%B3n"><i class="fa fa-check"></i><b>5.1.3</b> Discusión</a></li>
<li class="chapter" data-level="5.1.4" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios"><i class="fa fa-check"></i><b>5.1.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n-de-perceptrones-multicapa"><i class="fa fa-check"></i><b>5.2</b> Implementación de Perceptrones Multicapa</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n-desde-cero"><i class="fa fa-check"></i><b>5.2.1</b> Implementación desde Cero</a></li>
<li class="chapter" data-level="5.2.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#implementaci%C3%B3n"><i class="fa fa-check"></i><b>5.2.2</b> Implementación</a></li>
<li class="chapter" data-level="5.2.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios-1"><i class="fa fa-check"></i><b>5.2.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#forward-propagation-backward-propagation"><i class="fa fa-check"></i><b>5.3</b> Forward Propagation, Backward Propagation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#forward-propagation"><i class="fa fa-check"></i><b>5.3.1</b> Forward Propagation</a></li>
<li class="chapter" data-level="5.3.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#backpropagation"><i class="fa fa-check"></i><b>5.3.2</b> Backpropagation</a></li>
<li class="chapter" data-level="5.3.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#resumen-6"><i class="fa fa-check"></i><b>5.3.3</b> Resumen</a></li>
<li class="chapter" data-level="5.3.4" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios-2"><i class="fa fa-check"></i><b>5.3.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#estabilidad-num%C3%A9rica-e-inicializaci%C3%B3n"><i class="fa fa-check"></i><b>5.4</b> Estabilidad Numérica e Inicialización</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#explotaci%C3%B3n-y-desvanecimiento-de-gradientes"><i class="fa fa-check"></i><b>5.4.1</b> Explotación y Desvanecimiento de Gradientes</a></li>
<li class="chapter" data-level="5.4.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#inicializaci%C3%B3n-param%C3%A9trica"><i class="fa fa-check"></i><b>5.4.2</b> Inicialización paramétrica</a></li>
<li class="chapter" data-level="5.4.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios-3"><i class="fa fa-check"></i><b>5.4.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#generalizaci%C3%B3n-en-deep-learning"><i class="fa fa-check"></i><b>5.5</b> Generalización en Deep Learning</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#sobreajuste-y-regularizaci%C3%B3n"><i class="fa fa-check"></i><b>5.5.1</b> Sobreajuste y Regularización</a></li>
<li class="chapter" data-level="5.5.2" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#inspiraci%C3%B3n-de-los-no-param%C3%A9tricos"><i class="fa fa-check"></i><b>5.5.2</b> Inspiración de los no paramétricos</a></li>
<li class="chapter" data-level="5.5.3" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#early-stopping"><i class="fa fa-check"></i><b>5.5.3</b> Early Stopping</a></li>
<li class="chapter" data-level="5.5.4" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#m%C3%A9todos-cl%C3%A1sicos-de-regularizaci%C3%B3n-para-redes-profundas"><i class="fa fa-check"></i><b>5.5.4</b> Métodos clásicos de regularización para redes profundas</a></li>
<li class="chapter" data-level="5.5.5" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejercicios-4"><i class="fa fa-check"></i><b>5.5.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#dropout"><i class="fa fa-check"></i><b>5.6</b> Dropout</a></li>
<li class="chapter" data-level="5.7" data-path="perceptrón-multicapa.html"><a href="perceptrón-multicapa.html#ejemplo"><i class="fa fa-check"></i><b>5.7</b> Ejemplo</a></li>
</ul></li>
<li class="part"><span><b>Parte 2: Técnicas Modernas de Deep Learning</b></span></li>
<li class="chapter" data-level="6" data-path="guía-del-constructor.html"><a href="guía-del-constructor.html"><i class="fa fa-check"></i><b>6</b> Guía del Constructor</a></li>
<li class="chapter" data-level="7" data-path="redes-neuronales-convolucionales.html"><a href="redes-neuronales-convolucionales.html"><i class="fa fa-check"></i><b>7</b> Redes Neuronales Convolucionales</a></li>
<li class="chapter" data-level="8" data-path="redes-neuronales-convolucionales-modernas.html"><a href="redes-neuronales-convolucionales-modernas.html"><i class="fa fa-check"></i><b>8</b> Redes Neuronales Convolucionales Modernas</a></li>
<li class="chapter" data-level="9" data-path="redes-neuronales-recurrentes.html"><a href="redes-neuronales-recurrentes.html"><i class="fa fa-check"></i><b>9</b> Redes Neuronales Recurrentes</a></li>
<li class="chapter" data-level="10" data-path="redes-neuronales-recurrentes-modernas.html"><a href="redes-neuronales-recurrentes-modernas.html"><i class="fa fa-check"></i><b>10</b> Redes Neuronales Recurrentes Modernas</a></li>
<li class="chapter" data-level="11" data-path="mecanismos-de-atención-y-transformers.html"><a href="mecanismos-de-atención-y-transformers.html"><i class="fa fa-check"></i><b>11</b> Mecanismos de Atención y Transformers</a></li>
<li class="part"><span><b>Parte 3: Escalabilidad, Eficiencia y Aplicaciones</b></span></li>
<li class="chapter" data-level="12" data-path="algoritmos-de-optimización.html"><a href="algoritmos-de-optimización.html"><i class="fa fa-check"></i><b>12</b> Algoritmos de Optimización</a></li>
<li class="divider"></li>
<li><a href="./"><img src="img/aif-logo.png" width="280"></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="redes-neuronales-lineales-para-regresión" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Capítulo 3</span> Redes neuronales Lineales para Regresión<a href="redes-neuronales-lineales-para-regresión.html#redes-neuronales-lineales-para-regresi%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introducción-2" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Introducción<a href="redes-neuronales-lineales-para-regresión.html#introducci%C3%B3n-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El propósito de la regresión lineal es modelar la relación entre una variable que queremos predecir (por ejemplo, el precio de una vivienda) y una o más variables que podrían influir en ella (como los metros cuadrados, la ubicación o el número de habitaciones). Este tipo de análisis nos permite identificar patrones, hacer predicciones y entender cómo cambian los valores de una variable cuando las otras se modifican. En esencia, la regresión lineal busca encontrar una línea que mejor describa la tendencia general en los datos, ofreciendo una forma sencilla pero poderosa de explorar relaciones cuantitativas.</p>
<p><em>El término “regresión” proviene del trabajo del científico y estadístico británico Francis Galton a finales del siglo XIX. Galton, mientras estudiaba la relación entre la altura de padres e hijos, observó que los hijos de padres muy altos tendían a ser más bajos que ellos, y los hijos de padres muy bajos tendían a ser más altos. Es decir, las alturas “regresaban” hacia un valor promedio de la población. Para describir este fenómeno, Galton utilizó la expresión “regresión hacia la media”. Con el tiempo, este concepto se generalizó y dio origen al término “regresión” en estadística, que hoy usamos para describir modelos que ajustan relaciones entre variables.</em></p>
</div>
<div id="regresiones-lineales-simples" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Regresiones lineales simples<a href="redes-neuronales-lineales-para-regresión.html#regresiones-lineales-simples" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Las regresiones lineales buscan predecir una variable númerica (variable independiente), a partir de una o más variables (variables independientes), con el supuesto de que la relación que se da, entre la variable dependiente, con la independientes, es <strong>aproximadamente</strong> lineal.</p>
<p>Partamos del siguiente problema en el que se busca establecer el precio de una vivienda a partir de los metros cuadrados que lo conforman. Al graficar las dos características mencionadas se observa lo siguiente:</p>
<p><img src="img/03_Linear_Neural_Networks_for_Regression/uno.jpeg" width="450pt" height="450pt" style="display: block; margin: auto;" /></p>
<p>Al observar el gráfico se puede observar que el precio de la vivienda crece conforme los metros de la propiedad aumentan, manteniendo una relación “lineal”. Dicha relación la representamos entonces con la siguiente fórmula:</p>
<p><span class="math display">\[
Precio=\beta*metros^2+b
\]</span>
Si bien la relación no es lineal a la perfección, podemos pensar en la recta que mejor se ajuste, para así, obtener predicciones de la variable dependiente a partir de la variable independiente.</p>
</div>
<div id="función-de-pérdida-y-función-de-costo" class="section level2 hasAnchor" style="border:2px solid #ccc; padding:10px; border-radius:8px;" number="3.3">
<h2><span class="header-section-number">3.3</span> Función de pérdida y función de costo<a href="redes-neuronales-lineales-para-regresión.html#funci%C3%B3n-de-p%C3%A9rdida-y-funci%C3%B3n-de-costo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="función-de-pérdida-loss-function" class="section level4 hasAnchor" number="3.3.0.1">
<h4><span class="header-section-number">3.3.0.1</span> Función de pérdida (Loss Function)<a href="redes-neuronales-lineales-para-regresión.html#funci%C3%B3n-de-p%C3%A9rdida-loss-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La función de pérdida (o loss function) es una herramienta que mide qué tan bien o mal está funcionando un modelo de predicción para una observación individual. En otras palabras, compara el valor real con el predicho y asigna un número que representa el error cometido.</p>
<p><span class="math display">\[
\mathcal{L}(y,\hat{y})
\]</span></p>
</div>
<div id="función-de-costo-cost-function" class="section level4 hasAnchor" number="3.3.0.2">
<h4><span class="header-section-number">3.3.0.2</span> Función de costo (Cost Function)<a href="redes-neuronales-lineales-para-regresión.html#funci%C3%B3n-de-costo-cost-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La función de costo es una medida global del error del modelo completo.
Se obtiene al promediar (o combinar de otra forma) las pérdidas de todas las observaciones del conjunto de datos.</p>
<p><span class="math display">\[
\mathcal{J}(\theta)
\]</span>
Existen diferentes funciones de pérdida y costo, se elige cual debe considerarse en base al problema que se quiere resolver.</p>
</div>
</div>
<p><br></p>
<p>Regresando a la identidad de la regresión lineal, dado que la recta queda completamente definida por la pendiente y por la ordenada al origen, nuestro problema consiste en encontrar valores para estas, tales que mejor ajustan la recta a los datos. Definimos las funciones de pérdida y costo, en la regresión lineal, de la siguiente forma:</p>
<p><span class="math display">\[
\bullet \quad\mathcal{L}=(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2
\]</span>
<span class="math display">\[
\bullet \quad \mathcal{J}(\theta,b) = \frac{1}{n} \sum_{i=1}^{n} \mathcal{L}(y_i, \hat{y}_i)
= \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
= \frac{1}{n} \sum_{i=1}^{n} (y_i - (\beta x_i+b))^2
\]</span></p>
<p><br>
<strong>Parámetros de la regresión lineal</strong>
<br>
—————————————————————————————————————————————————————————-</p>
<p><span class="math display">\[
\textbf{Modelo:}\qquad y_i = \beta_0 + \beta_1 x_i + \varepsilon_i,\quad i=1,\dots,n
\]</span></p>
<p><span class="math display">\[
\text{Minimizamos la suma de cuadrados de residuos:}\qquad
S(\beta_0,\beta_1) = \sum_{i=1}^n \big(y_i - \beta_0 - \beta_1 x_i\big)^2
\]</span></p>
<p><span class="math display">\[
\textbf{Derivadas parciales y ecuaciones normales:}
\]</span></p>
<p><span class="math display">\[
\frac{\partial S}{\partial \beta_0}
= -2 \sum_{i=1}^n \big(y_i - \beta_0 - \beta_1 x_i\big) = 0
\;\;\Longrightarrow\;\;
\sum_{i=1}^n y_i = n\beta_0 + \beta_1 \sum_{i=1}^n x_i .
\]</span></p>
<p><span class="math display">\[
\frac{\partial S}{\partial \beta_1}
= -2 \sum_{i=1}^n x_i \big(y_i - \beta_0 - \beta_1 x_i\big) = 0
\;\;\Longrightarrow\;\;
\sum_{i=1}^n x_i y_i = \beta_0 \sum_{i=1}^n x_i + \beta_1 \sum_{i=1}^n x_i^2
\]</span></p>
<p><span class="math display">\[
\text{Definiendo }\bar{x}=\frac{1}{n}\sum x_i,\;\bar{y}=\frac{1}{n}\sum y_i,
\text{ la primera ecuación da }\beta_0 = \bar{y} - \beta_1 \bar{x}
\]</span></p>
<p><span class="math display">\[
\text{Sustituyendo en la segunda:}\quad
\sum x_i y_i
= (\bar{y} - \beta_1 \bar{x}) \sum x_i + \beta_1 \sum x_i^2
= n\bar{x}\bar{y} - n\beta_1 \bar{x}^2 + \beta_1 \sum x_i^2
\]</span></p>
<p><span class="math display">\[
\Longrightarrow\quad
\sum x_i y_i - n\bar{x}\bar{y}
= \beta_1 \Big(\sum x_i^2 - n\bar{x}^2\Big)
\]</span></p>
<p><span class="math display">\[
\text{Notando que }\;
S_{xy} := \sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y}) = \sum x_i y_i - n\bar{x}\bar{y},
\quad
S_{xx} := \sum_{i=1}^n (x_i-\bar{x})^2 = \sum x_i^2 - n\bar{x}^2,
\]</span></p>
<p><span class="math display">\[
\text{obtenemos}\qquad
\boxed{\,\hat{\beta}1 = \dfrac{S{xy}}{S_{xx}}\,}.
\]</span></p>
<p><span class="math display">\[
\text{Finalmente}\qquad
\boxed{\,\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}\,}.
\]</span></p>
<p><span class="math display">\[
\text{Formas equivalentes:}\qquad
\hat{\beta}_1
= \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2}
= \frac{\operatorname{Cov}(X,Y)}{\operatorname{Var}(X)},
\qquad
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}.
\]</span>
—————————————————————————————————————————————————————————-</p>
<p><img src="img/03_Linear_Neural_Networks_for_Regression/dos.jpeg" width="450pt" height="450pt" style="display: block; margin: auto;" /></p>
<div id="supuestos-en-la-regresión-lineal" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Supuestos en la regresión lineal<a href="redes-neuronales-lineales-para-regresión.html#supuestos-en-la-regresi%C3%B3n-lineal" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Linealidad</li>
</ol>
<p>La relación entre las variables independientes (predictoras) y la variable dependiente debe ser lineal.</p>
<p>Esto implica que los efectos de los predictores sobre la respuesta son proporcionales y aditivos.
Comprobación: Gráficos de dispersión o residuos vs. predicciones.</p>
<ol start="2" style="list-style-type: decimal">
<li>Independencia de los errores</li>
</ol>
<p>Los residuos (errores) deben ser independientes entre sí.</p>
<ol start="3" style="list-style-type: decimal">
<li>Homocedasticidad</li>
</ol>
<p>La varianza de los errores debe ser constante para todos los valores de las variables independientes.</p>
<ol start="4" style="list-style-type: decimal">
<li>Normalidad de los errores</li>
</ol>
<p>Los errores del modelo deben seguir una distribución normal con media cero.</p>
<p>Este supuesto es importante para la inferencia (p-valores, intervalos de confianza).
Comprobación: Histograma o gráfico Q-Q de los residuos, prueba de Shapiro-Wilk.</p>
<ol start="5" style="list-style-type: decimal">
<li>Ausencia de multicolinealidad</li>
</ol>
<p>Las variables independientes no deben estar altamente correlacionadas entre sí.</p>
<p>La multicolinealidad puede inflar las varianzas de los coeficientes.
Comprobación: VIF (Variance Inflation Factor), matriz de correlaciones.</p>
<ol start="6" style="list-style-type: decimal">
<li>No presencia de valores atípicos influyentes</li>
</ol>
<p>Los outliers o puntos de alta influencia pueden distorsionar el ajuste del modelo.</p>
<p>Es importante identificarlos y tratarlos adecuadamente.
Comprobación: Distancia de Cook, leverage, o gráfico de residuos estandarizados.</p>
<hr />
</div>
<div id="regresión-lineal-múltiple" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Regresión Lineal Múltiple<a href="redes-neuronales-lineales-para-regresión.html#regresi%C3%B3n-lineal-m%C3%BAltiple" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La regresión lineal múltiple es una extensión de la regresión lineal simple, que permite modelar la relación entre una variable dependiente y dos o más variables independientes. Su objetivo es estimar cómo cada variable explicativa contribuye, en promedio, al comportamiento de la variable objetivo.
Dada una varieble dependiente Y, que busca ser explicada por p características.</p>
<p>Para un registro de la variable dependiente, el modelo se expresa como:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \dots + \beta_p X_{ip} + \varepsilon_i
\]</span></p>
<p>donde:<br />
- <span class="math inline">\(Y_i\)</span>: valor de la variable dependiente para la observación <span class="math inline">\(i\)</span>,<br />
- <span class="math inline">\(X_{ij}\)</span>: valor de la variable explicativa <span class="math inline">\(j\)</span> para la observación <span class="math inline">\(i\)</span>,<br />
- <span class="math inline">\(\beta_0\)</span>: intercepto,<br />
- <span class="math inline">\(\beta_j\)</span>: coeficiente asociado a la variable <span class="math inline">\(X_j\)</span>,<br />
- <span class="math inline">\(\varepsilon_i\)</span>: término de error (diferencia entre el valor real y el predicho).</p>
<p>Así considerando todos los registros y la forma en la que se operan los vectores y las matrices, tenemos:</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\]</span></p>
</div>
<div id="estimación-de-los-parámetros" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Estimación de los parámetros<a href="redes-neuronales-lineales-para-regresión.html#estimaci%C3%B3n-de-los-par%C3%A1metros" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El método más común para estimar los parámetros es el de Mínimos Cuadrados Ordinarios (OLS), que busca minimizar la suma de los errores cuadráticos:</p>
<p><span class="math display">\[
(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
\]</span></p>
<p>La solución matricial es:</p>
<p><span class="math display">\[
\boxed{\hat{\boldsymbol{\beta}} = (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}}
\]</span></p>
<hr />
<p>Para encontrar los valores de <span class="math inline">\(\boldsymbol{\beta}\)</span> que minimizan la suma de los errores cuadráticos:</p>
<p><span class="math display">\[
\min_{\boldsymbol{\beta}} \; (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})&#39;(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
\]</span></p>
<hr />
<div id="derivación-paso-a-paso" class="section level3 hasAnchor" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> Derivación paso a paso<a href="redes-neuronales-lineales-para-regresión.html#derivaci%C3%B3n-paso-a-paso" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><em>Expandir la expresión:</em></li>
</ol>
<p><span class="math display">\[
S(\boldsymbol{\beta}) = \mathbf{y}&#39;\mathbf{y} - 2\mathbf{y}&#39;\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\beta}&#39;\mathbf{X}&#39;\mathbf{X}\boldsymbol{\beta}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><em>Derivar respecto a</em> <span class="math inline">\(\boldsymbol{\beta}\)</span>:</li>
</ol>
<p><span class="math display">\[
\frac{\partial S}{\partial \boldsymbol{\beta}} = -2\mathbf{X}&#39;\mathbf{y} + 2\mathbf{X}&#39;\mathbf{X}\boldsymbol{\beta}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><em>Igualar a cero</em> (condición de mínimo):</li>
</ol>
<p><span class="math display">\[
\mathbf{X}&#39;\mathbf{X}\boldsymbol{\beta} = \mathbf{X}&#39;\mathbf{y}
\]</span></p>
<hr />
<p><span class="math display">\[
\boxed{\hat{\boldsymbol{\beta}} = (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}}
\]</span></p>
</div>
</div>
<div id="bondad-de-ajuste" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Bondad de ajuste<a href="redes-neuronales-lineales-para-regresión.html#bondad-de-ajuste" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Una vez estimado el modelo, se puede evaluar qué tan bien explica los datos mediante el coeficiente de determinación <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
\]</span></p>
<ul>
<li><span class="math inline">\(R^2\)</span> cercano a 1 → el modelo explica gran parte de la variabilidad de <span class="math inline">\(Y\)</span>.<br />
</li>
<li><span class="math inline">\(R^2\)</span> cercano a 0 → el modelo explica poco o nada.</li>
</ul>
<p>Sin embargo la <span class="math inline">\(R^2\)</span> no penaliza la complejidad del modelo. Esto es, al aumentar el número de regresores, el factor <span class="math inline">\(\sum (y_i - \hat{y}_i)^2\)</span>, no puede aumentar</p>
<p>El valor de la R^2 ajustada, penaliza la inclusión de variables irrelevantes, que pueden ser altamente colineales. Para esto considera el número de predictores y el tamaño de la muestra. Para hallar el valor de la R^2 ajustada se tienes que calcular primero el valor de la R^2, así entonces se tiene:</p>
<p><span class="math display">\[
R^2_{ajustada}=1-\frac{(1-R^2)(n-1)}{n-p-1}
\]</span>
La diferencia entre ambas métricas es que r-cuadrada siempre aumenta o permanece cuando se agregan predictores, aunque los predictores que se añaden no mejoren significativamente el modelo. En cambio la R-cuadrada ajustada, puede disminuir si un nuev predictor no mejora el modelo.
—</p>
</div>
<div id="supuestos-del-modelo-lineal-múltiple" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> Supuestos del modelo lineal múltiple<a href="redes-neuronales-lineales-para-regresión.html#supuestos-del-modelo-lineal-m%C3%BAltiple" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Para que los resultados del modelo sean válidos, se asume que:</p>
<ol style="list-style-type: decimal">
<li><strong>Linealidad</strong>: la relación entre <span class="math inline">\(Y\)</span> y las <span class="math inline">\(X_j\)</span> es lineal.<br />
</li>
<li><strong>Independencia</strong>: los errores son independientes entre sí.<br />
</li>
<li><strong>Esperanza condicional cero</strong>: <span class="math inline">\(\mathbb{E}[\varepsilon_i \mid X] = 0\)</span>.<br />
</li>
<li><strong>Homocedasticidad</strong>: la varianza de los errores es constante.<br />
</li>
<li><strong>No multicolinealidad</strong>: las variables independientes no están perfectamente correlacionadas.<br />
</li>
<li><strong>Normalidad</strong>: los errores se distribuyen normalmente.</li>
</ol>
<hr />
</div>
<div id="regularización-en-la-regresión-lineal" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Regularización en la Regresión Lineal<a href="redes-neuronales-lineales-para-regresión.html#regularizaci%C3%B3n-en-la-regresi%C3%B3n-lineal" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Cuando un modelo de regresión lineal se ajusta demasiado a los datos (sobreajuste), o existen muchas variables correlacionadas, los coeficientes pueden volverse inestables.<br />
Para controlar este problema, existen métodos de regularización, que agregan una penalización a la función de costo.</p>
<p>Los tres enfoques más comunes son <strong>Ridge, Lasso</strong> y <strong>Elastic Net</strong>.</p>
<hr />
<div id="ridge-regression" class="section level3 hasAnchor" number="3.9.1">
<h3><span class="header-section-number">3.9.1</span> Ridge Regression<a href="redes-neuronales-lineales-para-regresión.html#ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Agrega una penalización cuadrática a los coeficientes para evitar sobreajuste.Los coeficientes al aportar a la función de costo, buscan ser más pequeño y con esto variables que aportaban mucho a la explicabilidad de la variable dependiente pierden relevancia.</p>
<p><span class="math display">\[
\mathcal{J}_{\text{ridge}}(\theta)
= \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
+ \lambda \sum_{j=1}^{p} \beta_j^2
\]</span></p>
<ul>
<li>Penaliza los coeficientes grandes.<br />
</li>
<li>Los reduce, pero no los lleva exactamente a cero.<br />
</li>
<li>Útil cuando existe multicolinealidad entre las variables.</li>
</ul>
<hr />
</div>
<div id="lasso-regression" class="section level3 hasAnchor" number="3.9.2">
<h3><span class="header-section-number">3.9.2</span> Lasso Regression<a href="redes-neuronales-lineales-para-regresión.html#lasso-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Agrega una penalización absoluta (valor absoluto) que puede llevar algunos coeficientes a cero.</p>
<p><span class="math display">\[
\mathcal{J}_{\text{lasso}}(\theta)
= \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
+ \lambda \sum_{j=1}^{p} |\beta_j|
\]</span></p>
<ul>
<li>Además de reducir los coeficientes, elimina algunos por completo.<br />
</li>
<li>Actúa como un método de selección automática de variables.<br />
</li>
<li>Es útil en problemas de alta dimensionalidad.</li>
</ul>
<hr />
</div>
<div id="elastic-net-regression" class="section level3 hasAnchor" number="3.9.3">
<h3><span class="header-section-number">3.9.3</span> Elastic Net Regression<a href="redes-neuronales-lineales-para-regresión.html#elastic-net-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Combina las penalizaciones de Ridge y Lasso, logrando un balance entre ambos enfoques.</p>
<p><span class="math display">\[
\mathcal{J}_{\text{elastic}}(\theta)
= \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
+ \lambda \left(
  \alpha \sum_{j=1}^{p} |\beta_j|
  + (1 - \alpha) \sum_{j=1}^{p} \beta_j^2
\right)
\]</span></p>
<p>donde:<br />
- <span class="math inline">\(\lambda\)</span> controla la intensidad de la regularización,<br />
- <span class="math inline">\(\alpha \in [0,1]\)</span> controla el balance entre Lasso y Ridge.</p>
<p><em>Casos especiales:</em><br />
- <span class="math inline">\(\alpha = 1\)</span> → modelo <em>Lasso</em><br />
- <span class="math inline">\(\alpha = 0\)</span> → modelo <em>Ridge</em></p>
<hr />
</div>
<div id="comparación-de-métodos" class="section level3 hasAnchor" number="3.9.4">
<h3><span class="header-section-number">3.9.4</span> Comparación de métodos<a href="redes-neuronales-lineales-para-regresión.html#comparaci%C3%B3n-de-m%C3%A9todos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="9%" />
<col width="16%" />
<col width="33%" />
<col width="22%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Método</th>
<th align="left">Penalización</th>
<th align="left">Efecto sobre los coeficientes</th>
<th align="left">Selecciona variables</th>
<th align="left">Principal uso</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><em>Ridge</em></td>
<td align="left"><span class="math inline">\(\lambda \sum \beta_j^2\)</span></td>
<td align="left">Los reduce (sin anularlos)</td>
<td align="left">No</td>
<td align="left">Multicolinealidad</td>
</tr>
<tr class="even">
<td align="left"><em>Lasso</em></td>
<td align="left"><span class="math inline">\(\lambda \sum |\beta_j|\)</span></td>
<td align="left">Algunos se vuelven 0</td>
<td align="left">sí</td>
<td align="left">Alta dimensionalidad</td>
</tr>
<tr class="odd">
<td align="left"><em>Elastic Net</em></td>
<td align="left"><span class="math inline">\(\lambda [\alpha \sum |\beta_j| + (1-\alpha)\sum \beta_j^2]\)</span></td>
<td align="left">Combina ambos efectos</td>
<td align="left">Parcial</td>
<td align="left">Balance entre Ridge y Lasso</td>
</tr>
</tbody>
</table>
<p>La regularización busca un equilibrio entre ajuste y simplicidad.<br />
Mientras Ridge controla la magnitud de los coeficientes, Lasso puede eliminarlos.<br />
Elastic Net combina ambos enfoques para lograr un modelo más robusto y equilibrado.</p>
</div>
</div>
<div id="consideraciones" class="section level2 hasAnchor" number="3.10">
<h2><span class="header-section-number">3.10</span> Consideraciones<a href="redes-neuronales-lineales-para-regresión.html#consideraciones" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Es útil escalar los valores de la variables dependientes, sobre todo al regularizar las regresiones.</p>
<p>La correlación de Pearson, es una buena herramienta para seleccionar variables independientes que modelen a una variable dependiente.</p>
</div>
<div id="sesgo-y-varianza" class="section level2 hasAnchor" number="3.11">
<h2><span class="header-section-number">3.11</span> Sesgo y Varianza<a href="redes-neuronales-lineales-para-regresión.html#sesgo-y-varianza" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El desempeño de un modelo de regresión (o de cualquier modelo de aprendizaje) depende de su capacidad para <em>equilibrar el sesgo y la varianza</em>.<br />
Estos dos conceptos explican los errores que comete el modelo al predecir valores nuevos.</p>
<div id="sesgo" class="section level3 hasAnchor" number="3.11.1">
<h3><span class="header-section-number">3.11.1</span> Sesgo<a href="redes-neuronales-lineales-para-regresión.html#sesgo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El <em>sesgo</em> mide cuánto se alejan las predicciones promedio del modelo del valor real esperado. En términos simples, representa el <em>error por simplificación del modelo</em>.</p>
<p><em>Alta sesgo → modelo demasiado simple</em>, que no logra capturar la complejidad de los datos (subajuste).</p>
</div>
<div id="varianza" class="section level3 hasAnchor" number="3.11.2">
<h3><span class="header-section-number">3.11.2</span> Varianza<a href="redes-neuronales-lineales-para-regresión.html#varianza" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La <em>varianza</em> mide cuánto cambian las predicciones del modelo si se entrena con diferentes conjuntos de datos. Representa la <em>sensibilidad del modelo al ruido</em>.</p>
<p><em>Alta varianza → modelo demasiado complejo</em>, que se ajusta demasiado a los datos de entrenamiento (sobreajuste).</p>
<p><strong>Consecuencia:</strong><br />
Predicciones muy diferentes en nuevos datos.</p>
</div>
<div id="compromiso-sesgovarianza" class="section level3 hasAnchor" number="3.11.3">
<h3><span class="header-section-number">3.11.3</span> Compromiso sesgo–varianza<a href="redes-neuronales-lineales-para-regresión.html#compromiso-sesgovarianza" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Existe un <em>equilibrio</em> entre ambos conceptos:</p>
<table>
<thead>
<tr class="header">
<th align="left">Tipo de modelo</th>
<th align="center">Sesgo</th>
<th align="center">Varianza</th>
<th align="left">Resultado</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Modelo simple (pocos parámetros)</td>
<td align="center">Alto</td>
<td align="center">Bajo</td>
<td align="left">Subajuste</td>
</tr>
<tr class="even">
<td align="left">Modelo complejo (muchos parámetros)</td>
<td align="center">Bajo</td>
<td align="center">Alto</td>
<td align="left">Sobreajuste</td>
</tr>
<tr class="odd">
<td align="left">Modelo equilibrado</td>
<td align="center">Medio</td>
<td align="center">Medio</td>
<td align="left">Buen ajuste</td>
</tr>
</tbody>
</table>
</div>
<div id="en-regresión-lineal" class="section level3 hasAnchor" number="3.11.4">
<h3><span class="header-section-number">3.11.4</span> En regresión lineal<a href="redes-neuronales-lineales-para-regresión.html#en-regresi%C3%B3n-lineal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>La <em>regresión lineal clásica</em> tiende a tener <em>bajo sesgo</em> y <em>baja varianza</em> si el número de variables es razonable.<br />
</li>
<li>Cuando se agregan muchas variables o hay multicolinealidad, la <em>varianza aumenta</em>, porque los coeficientes se vuelven inestables.</li>
<li>Métodos de <em>regularización</em> (como Ridge o Lasso) se usan precisamente para <em>reducir la varianza</em>, a costa de introducir un poco de sesgo.</li>
</ul>
</div>
</div>
<div id="conclusión-de-la-sección" class="section level2 hasAnchor" number="3.12">
<h2><span class="header-section-number">3.12</span> Conclusión de la sección<a href="redes-neuronales-lineales-para-regresión.html#conclusi%C3%B3n-de-la-secci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Finalmente buscando conectar el tema con las redes neuronales. La regresión lineal es el primer modelo que se ajusta al construir una neurona, pues recibe las variables de entrada (variables independientes), una vez ajustada la regresión lineal, esta pasará por una función de activación la cual rompe con la relación lineal entre las variables independientes con la variable dependiente.</p>

<!-- ::: watermark -->
<!-- <img src="img/aif-logo.png" width="400"/> -->
<!-- ::: -->
</div>
</div>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script>

$('.pipehover_incremental tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).prevAll().addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});


$('.pipehover_select_one_row tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});

</script>
            </section>

          </div>
        </div>
      </div>
<a href="preliminares.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="redes-neuronales-lineales-para-clasificación.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["deep_learning_course_python.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
