<!-- ::: watermark -->
<!-- <img src="img/aif-logo.png" width="400"/> -->
<!-- ::: -->


# Redes neuronales Lineales para ClasificaciÃ³n

## Historia

**RegresiÃ³n lineal**

```{r echo=FALSE, fig.align='left', out.width='50%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/legendre-gauss.png")
```
El primer mÃ©todo de regresiÃ³n lineal documentado es el mÃ©todo de los mÃ­nimos cuadrados, publicado por Legendre en 1805. 
Posteriormente, Gauss publicÃ³ un trabajo donde se desarrolla con mayor detalle el tema.


**Modelo Lineal Generalizado (GLM)**

```{r echo=FALSE, fig.align='left', out.width='50%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/nelder-wedderburn.png")
```
Los GLM fueron formulados por John Nelder y Robert Wedderburn en 1972 como una manera de unificar modelos estadÃ­sticos.

Un modelo lineal generalizado es una generalizaciÃ³n flexible de la regresiÃ³n lineal ordinaria.

**Redes Neuronales**

```{r echo=FALSE, fig.align='left', out.width='50%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig3.png")
```
En 1943 Warren McCulloch y Walter Pitts crearon un modelo para redes neuronales basados en la lÃ³gica de umbral. Este modelo seÃ±alÃ³ el camino para que la investigaciÃ³n de redes neuronales se divida en dos enfoques distintos:

1. Un enfoque centrado en los procesos biolÃ³gicos en el cerebro.
2. Otro en la aplicaciÃ³n de redes neuronales para la inteligencia artificial.

```{r echo=FALSE, fig.align='left', out.width='40%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig4.png")
```
Para la aplicaciÃ³n de la inteligencia artificial, fue Marvin Minsky quien dio una gran aportaciÃ³n con el libro **â€œPerceptronesâ€** en 1969.
Minsky es considerado como uno de los padres de las ciencias de la computaciÃ³n y fue cofundador del laboratorio de inteligencia artificial del MIT


## Modelos Lineales Generalizados


Los Modelos Lineales Generalizados (GLM) son una extensiÃ³n de los modelos lineales tradicionales que permiten modelar variables respuesta que no necesariamente siguen una distribuciÃ³n normal (por ejemplo, binarias, de conteo o proporciones). Relacionan la distribuciÃ³n aleatoria de la variable dependiente (variable objetivo) en el experimento, con la parte sistemÃ¡tica a travÃ©s de una funciÃ³n llamada funciÃ³n de enlace.

Por lo tanto, un modelo lineal generalizado, tiene tres componentes  bÃ¡sicos:

- **Componente aleatorio** *(Y ~ DistribuciÃ³n de probabilidad)*: Identifica la variable de objetivo y su distribuciÃ³n de probabilidad.
- **Componente sistemÃ¡tica** *(ğœ‚ = x^TW)*: Especifica las variables explicativas de la funciÃ³n predictora lineal.
- **FunciÃ³n de enlace** *(ğ‘”(ğ) = ğœ‚)*: Es una funciÃ³n del valor esperado de Y como una combinaciÃ³n lineal de las variables explicativas

Algebraicamente:
$$
	ğ”¼(ğ’€)=ğ=ğ‘”^{âˆ’1}(WX)
$$

Los estudios de Wedderburn se limitaron los componentes aleatorios a la familia exponencial, cuya aplicaciÃ³n se puede dar de la siguiente manera:

- **Normal**: regresiÃ³n lineal
- **Binomial**: regresiÃ³n logÃ­stica
- **Poisson**: regresiÃ³n de conteo
- **Gamma**: regresiÃ³n para tiempos o tasas positivas
- Inversa Gaussiana, etc.

| DistribuciÃ³n | FunciÃ³n de Enlace          | Nombre de modelo      |
|--------------|----------------------------|-----------------------|
| Normal       | g(Î¼) = Î¼                   | RegresiÃ³n lineal      |
| Binomial     | g(Î¼) = log(Î¼ /(1âˆ’Î¼))       | RegresiÃ³n logÃ­stica   |
| Poisson      | g(Î¼) = log(Î¼)              | RegresiÃ³n de Poisson  |


## RegresiÃ³n LogÃ­stica para ClasificaciÃ³n

Iniciemos recordando que los algoritmos de Machine Learning pueden ser clasificados como:
```{r echo=FALSE, fig.align='left', out.width='40%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig5.png")
```

Los ***mÃ©todos de clasificaciÃ³n, mÃ©todos predictivos,  reconocimiento de patrones, aprendizaje supervisado*** son aquellos algoritmos en los cuales vamos a clasificar a los objetos en un conjunto de categorÃ­as previamente definidas.

En estos mÃ©todos existe una variable, conocida como variable objetivo con la cual se buscarÃ¡n dos resultados:

- **Evidenciar** la razÃ³n por la cual existen las clases en cuestiÃ³n, es decir, encontrar factores que puedan agrupar a los individuos en sus respectivos grupos.
- Proyectar el ingreso de **nuevos individuos** en alguna de las caracterÃ­sticas anteriores.

**DefiniciÃ³n**

La clasificaciÃ³n es la tarea de **aprender una funciÃ³n, f,** que pueda mapear cada conjunto de atributos a una categorÃ­a predefinida.
Busca predecir la clase a la cual pertenece un registro. 
La diferencia principal contra el anÃ¡lisis de conglomerados es el deseo de predecir a nuevos individuos.

Buscamos una funciÃ³n que vaya de D a C
$$
ğ‘“:ğ·â†’ Y
$$
Donde:

 - $ğ·={(ğ‘¥_ğ‘–,ğ‘¦_ğ‘–) | ğ‘–=1,2,â€¦,ğ‘}$ es el conjunto de individuos, incluyendo $ğ‘¥_ğ‘–$ corresponde a las caracterÃ­sticas explicativas de un individuo y $ğ‘¦_ğ‘–$ corresponde al valor de la clase
 - $Y={y _1,  y_2,â€¦, y_ğ‘š}$ es el conjunto de clases.

La funciÃ³n resultante puede ser un Ã¡rbol de decisiÃ³n, SVM, etc.

La funciÃ³n que clasificarÃ¡ el conjunto X a la clase Y, comÃºnmente es llamada modelo de clasificaciÃ³n.
```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig6.png")
```

Una vez recordado lo anterior, podemos definir a una regresiÃ³n logÃ­stica como un modelo en el cual se recibe una tabla relacional con *d* caracterÃ­sticas descriptivas y una clase a la cual pertenece cada individuo de la tabla.

Como resultado, el modelo nos proporciona la *probabilidad de pertenecer* a alguna clase.

**Planteamiento del problema**

El primer paso, consistirÃ­a en construir el modelo de regresiÃ³n lineal, de la siguiente manera:
$$
ğ‘¦_ğ‘–=ğ›½_0+ğ›½_1 ğ‘¥_ğ‘–+ğœ–_ğ‘–
$$
Vamos a suponer un modelo de **regresiÃ³n clÃ¡sica** de tal forma que:
$$
ğ”¼(ğ‘¦_ğ‘– | ğ‘¥_ğ‘– )=ğ›½_0+ğ›½_1 ğ‘¥_ğ‘–
$$
Al intentar buscar probabilidades a travÃ©s de una regresiÃ³n lineal, obtendrÃ­amos lo siguiente:

Sea $ğ‘_ğ‘–$ la probabilidad de pertenecer a la clase 1 dadas las caracterÃ­sticas del individuo:
$$
ğ‘_ğ‘–=â„™(ğ‘¦=1|ğ‘¥_ğ‘– )
$$

Podemos asumir que $ğ‘_ğ‘–$ sigue una distribuciÃ³n Bernoulli, de tal forma que su esperanza sea:
$$
ğ”¼(ğ‘¦|ğ‘¥_ğ‘– )=ğ‘_ğ‘–Ã—1+(1âˆ’ğ‘_ğ‘– )Ã—0=ğ‘_ğ‘–
$$
De tal forma que:
$$
ğ‘_ğ‘–= ğ›½_0+ğ›½_1 ğ‘¥_ğ‘–
$$
No obstante, los resultados pueden resultar absurdos debido a que nuestra fÃ³rmula nos puede proporcionar **probabilidades negativas** o **mayores a cero** $!$

Las probabilidades no tienen valores superiores a 1 ni negativas, pero el concepto de momios $p / (1-p)$ tiene como imagen de cero a infinito. La funciÃ³n exponencial tiene la misma imagen, entonces, la regresiÃ³n logÃ­stica tiene la intenciÃ³n de encontrar la siguiente igualdad
$$
â„™(ğ‘¦=1|ğ‘¥_ğ‘– )/(1âˆ’â„™(ğ‘¦=1|ğ‘¥_ğ‘– ) )=ğ‘’^{(ğ›½_0+ğ›½_1 ğ‘¥_ğ‘– )}
$$
Al aplicar el logaritmo natual, podemos apreciar la definiciÃ³n de la funciÃ³n logit:
$$
logit[ğ‘_ğ‘– ]=ln[ğ‘_ğ‘–/(1âˆ’ğ‘_ğ‘– )]=ğ›½_0+ğ›½_1 ğ‘¥_ğ‘–
$$
Recordando que la funciÃ³n logit es la inversa de la funciÃ³n logistica (o sigmoide)







