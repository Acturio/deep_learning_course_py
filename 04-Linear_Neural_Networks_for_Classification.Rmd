<!-- ::: watermark -->
<!-- <img src="img/aif-logo.png" width="400"/> -->
<!-- ::: -->


# Redes neuronales Lineales para ClasificaciÃ³n


## Modelos Lineales Generalizados

### Historia

**RegresiÃ³n lineal**

```{r echo=FALSE, fig.align='left', out.width='50%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig1.png")
```
El primer mÃ©todo de regresiÃ³n lineal documentado es el mÃ©todo de los mÃ­nimos cuadrados, publicado por Legendre en 1805. 
Posteriormente, Gauss publicÃ³ un trabajo donde se desarrolla con mayor detalle el tema.


**Modelo Lineal Generalizado (GLM)**

```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig2.png")
```
Los GLM fueron formulados por John Nelder y Robert Wedderburn en 1972 como una manera de unificar modelos estadÃ­sticos.

Un modelo lineal generalizado es una generalizaciÃ³n flexible de la regresiÃ³n lineal ordinaria.

### DefiniciÃ³n

Los Modelos Lineales Generalizados (GLM) son una extensiÃ³n de los modelos lineales tradicionales que permiten modelar variables respuesta que no necesariamente siguen una distribuciÃ³n normal (por ejemplo, binarias, de conteo o proporciones). Relacionan la distribuciÃ³n aleatoria de la variable dependiente (variable objetivo) en el experimento, con la parte sistemÃ¡tica a travÃ©s de una funciÃ³n llamada funciÃ³n de enlace.

Por lo tanto, un modelo lineal generalizado, tiene tres componentes  bÃ¡sicos:

- **Componente aleatorio** *(Y ~ DistribuciÃ³n de probabilidad)*: Identifica la variable de objetivo y su distribuciÃ³n de probabilidad.
- **Componente sistemÃ¡tica** *(ğœ‚ = xW)*: Especifica las variables explicativas de la funciÃ³n predictora lineal.
- **FunciÃ³n de enlace** *(ğ‘”(ğ) = ğœ‚)*: Es una funciÃ³n del valor esperado de Y como una combinaciÃ³n lineal de las variables explicativas

Algebraicamente:
$$
	ğ”¼(ğ’€)=ğ=ğ‘”^{âˆ’1}(WX)
$$

Los estudios de Wedderburn se limitaron los componentes aleatorios a la familia exponencial, cuya aplicaciÃ³n se puede dar de la siguiente manera:

- **Normal**: regresiÃ³n lineal
- **Binomial**: regresiÃ³n logÃ­stica
- **Poisson**: regresiÃ³n de conteo
- **Gamma**: regresiÃ³n para tiempos o tasas positivas
- Inversa Gaussiana, etc.

| DistribuciÃ³n | FunciÃ³n de Enlace          | Nombre de modelo      |
|--------------|----------------------------|-----------------------|
| Normal       | g(Î¼) = Î¼                   | RegresiÃ³n lineal      |
| Binomial     | g(Î¼) = log(Î¼ /(1âˆ’Î¼))       | RegresiÃ³n logÃ­stica   |
| Poisson      | g(Î¼) = log(Î¼)              | RegresiÃ³n de Poisson  |


Podemos apreciar una regresiÃ³n logÃ­stica como una neurona:
```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig18.png")
```


Haciendo uso de una regresiÃ³n logÃ­stica, podemos pasar de *how much?* a *wich class?*, el diagrama serÃ­a el siguiente:
```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig19.png")
```


## RegresiÃ³n LogÃ­stica para ClasificaciÃ³n

Iniciemos recordando que los algoritmos de Machine Learning pueden ser clasificados como:
```{r echo=FALSE, fig.align='left', out.width='40%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig5.png")
```

Los ***mÃ©todos de clasificaciÃ³n, mÃ©todos predictivos,  reconocimiento de patrones, aprendizaje supervisado*** son aquellos algoritmos en los cuales vamos a clasificar a los objetos en un conjunto de categorÃ­as previamente definidas.

En estos mÃ©todos existe una variable, conocida como variable objetivo con la cual se buscarÃ¡n dos resultados:

- **Evidenciar** la razÃ³n por la cual existen las clases en cuestiÃ³n, es decir, encontrar factores que puedan agrupar a los individuos en sus respectivos grupos.
- Proyectar el ingreso de **nuevos individuos** en alguna de las caracterÃ­sticas anteriores.

**DefiniciÃ³n**

La clasificaciÃ³n es la tarea de **aprender una funciÃ³n, f,** que pueda mapear cada conjunto de atributos a una categorÃ­a predefinida.
Busca predecir la clase a la cual pertenece un registro. 
La diferencia principal contra el anÃ¡lisis de conglomerados es el deseo de predecir a nuevos individuos.

Buscamos una funciÃ³n que vaya de D a C
$$
ğ‘“:ğ·â†’ Y
$$
Donde:

 - $ğ·={(ğ‘¥_ğ‘–,ğ‘¦_ğ‘–) | ğ‘–=1,2,â€¦,ğ‘}$ es el conjunto de individuos, incluyendo $ğ‘¥_ğ‘–$ corresponde a las caracterÃ­sticas explicativas de un individuo y $ğ‘¦_ğ‘–$ corresponde al valor de la clase
 - $Y={y _1,  y_2,â€¦, y_ğ‘š}$ es el conjunto de clases.

La funciÃ³n resultante puede ser un Ã¡rbol de decisiÃ³n, SVM, etc.

La funciÃ³n que clasificarÃ¡ el conjunto X a la clase Y, comÃºnmente es llamada modelo de clasificaciÃ³n.
```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig6.png")
```

Una vez recordado lo anterior, podemos definir a una regresiÃ³n logÃ­stica como un modelo en el cual se recibe una tabla relacional con *d* caracterÃ­sticas descriptivas y una clase a la cual pertenece cada individuo de la tabla.

Como resultado, el modelo nos proporciona la *probabilidad de pertenecer* a alguna clase.

**Planteamiento del problema**

El primer paso, consistirÃ­a en construir el modelo de regresiÃ³n, de tal forma que el componente aleatorio ($y_i$) se conecte con la componente sistÃ©mica (regresiÃ³n lineal):
$$
ğ‘¦_ğ‘–â†”ğ›½_0+ğ›½_1 ğ‘¥_ğ‘–+ğœ–_ğ‘–
$$

---

Vamos a suponer un modelo de **regresiÃ³n lineal clÃ¡sica** de tal forma que:
$$
ğ”¼(ğ‘¦_ğ‘– | ğ‘¥_ğ‘– )=ğ›½_0+ğ›½_1 ğ‘¥_ğ‘–
$$
Al intentar buscar probabilidades a travÃ©s de una regresiÃ³n lineal, obtendrÃ­amos lo siguiente:

Sea $ğ‘_ğ‘–$ la probabilidad de pertenecer a la clase 1 dadas las caracterÃ­sticas del individuo:
$$
ğ‘_ğ‘–=â„™(ğ‘¦=1|ğ‘¥_ğ‘– )
$$

Podemos asumir que $ğ‘_ğ‘–$ sigue una distribuciÃ³n Bernoulli, de tal forma que su esperanza sea:
$$
ğ”¼(ğ‘¦|ğ‘¥_ğ‘– )=ğ‘_ğ‘–Ã—1+(1âˆ’ğ‘_ğ‘– )Ã—0=ğ‘_ğ‘–
$$
De tal forma que:
$$
ğ‘_ğ‘–= ğ›½_0+ğ›½_1 ğ‘¥_ğ‘–
$$
No obstante, los resultados pueden resultar absurdos debido a que nuestra fÃ³rmula nos puede proporcionar **probabilidades negativas** o **mayores a cero** $!$

---

Las probabilidades no tienen valores superiores a 1 ni negativas, pero el concepto de momios $p / (1-p)$ tiene como imagen de cero a infinito. La funciÃ³n exponencial tiene la misma imagen, entonces, la regresiÃ³n logÃ­stica relaciona ambos concpetos a travÃ©s de la siguiente igualdad
$$
â„™(ğ‘¦=1|ğ‘¥_ğ‘– )/(1âˆ’â„™(ğ‘¦=1|ğ‘¥_ğ‘– ) )=ğ‘’^{(ğ›½_0+ğ›½_1 ğ‘¥_ğ‘– )}
$$
Al aplicar el logaritmo natual, podemos apreciar la definiciÃ³n de la funciÃ³n logit:
$$
logit[ğ‘_ğ‘– ]=ln[ğ‘_ğ‘–/(1âˆ’ğ‘_ğ‘– )]=ğ›½_0+ğ›½_1 ğ‘¥_ğ‘–
$$
Recordando que la funciÃ³n logit es la inversa de la funciÃ³n logistica (o sigmoide)

```{r echo=FALSE, fig.align='left', fig.width=6, fig.height=4}
library(ggplot2)

# Datos para la curva
x <- seq(-6, 6, length.out = 200)
y <- 1 / (1 + exp(-x))
data <- data.frame(x, y)

# GrÃ¡fico con ggplot2
ggplot(data, aes(x=x, y=y)) +
  geom_line(color="steelblue", linewidth=1.2) +
  labs(
    title = "Curva logÃ­stica",
    x = "x",
    y = "f(x)"
  ) +
  theme_minimal(base_size=14)
```



## Redes neuronales

### Historia

**Redes Neuronales**

```{r echo=FALSE, fig.align='left', out.width='50%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig3.png")
```
En 1943 Warren McCulloch (neurocientÃ­fico, mÃ©dico, neurÃ³logo y fisiÃ³logo) y Walter Pitts (matemÃ¡tico, psicÃ³logo, filÃ³sofo y neurocientÃ­fico) crearon un modelo para redes neuronales basados en la lÃ³gica de umbral. Este modelo seÃ±alÃ³ el camino para que la investigaciÃ³n de redes neuronales se divida en dos enfoques distintos:

1. Un enfoque centrado en los procesos biolÃ³gicos en el cerebro.
2. Otro en la aplicaciÃ³n de redes neuronales para la inteligencia artificial.

```{r echo=FALSE, fig.align='left', out.width='40%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig4.png")
```
Para la aplicaciÃ³n de la inteligencia artificial, fue Marvin Minsky quien dio una gran aportaciÃ³n con el libro **â€œPerceptronesâ€** en 1969.
Minsky es considerado como uno de los padres de las ciencias de la computaciÃ³n y fue cofundador del laboratorio de inteligencia artificial del MIT

### DefiniciÃ³n

Las redes neuronales son algoritmos que tienen como objetivo el simular en una computadora la forma en la cual funcionan las redes neuronales en los humanos.
Puede recibir variables cuantitativas y cualitativas. Es un algoritmo sensible a cambios en los datos y parÃ¡metros

### Neurona

CÃ©lula del sistema nerviosos formada por un nÃºcleo y una serie de prolongaciones, una de las cuales es mÃ¡s larga que las demÃ¡s.
EstÃ¡n especializadas en la recepciÃ³n de estÃ­mulos y conducciÃ³n del impulso nerviosos entre ellas o con otros tipos celulares

```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig7.png")
```

<br>

**Dentrita:** Es la fuente de un impulso nerviosos. De hecho el impulso nervioso es unidireccional, es decir, solamente se transmite desde las dendritas hacia el axÃ³n (canal de entrada)

**AxÃ³n:** ProlongaciÃ³n que arranca del cuerpo de la neurona y termina en una ramificaciÃ³n que estÃ¡ en contacto con otras cÃ©lulas. Es la vÃ­a por la cual circulan los impulsos elÃ©ctricos (canal de salida)

**Sinapsis:** RegiÃ³n de comunicaciÃ³n entre el axÃ³n de una neurona y las dendritas o el cuerpo de otra

```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig8.png")
```

**EmulaciÃ³n:**

```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig9.png")
```

Las redes neuronales, tratan de modelar una red neuronal donde cada nodo es una neurona, y los arcos representan la sinapsis entre las neuronas. 
Como algoritmo, las redes neuronales reciben n entradas y pueden otorgar n salidas.

```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig10.png")
```

Un perceptrÃ³n puede entenderse como la unidad bÃ¡sica de inferencia en forma de discriminador lineal. Un perceptrÃ³n va a separar a los puntos a travÃ©s de un hiperplano
Es el modelo biolÃ³gico mÃ¡s sencillo hace referencia a una sola neurona. El perceptrÃ³n consiste de dos tipos de nodos:

- Nodos de entrada: ingreso de los atributos
- Nodos de salida: representa a los resultados del modelo

Un perceptrÃ³n genera sus resultados realizando una suma ponderada de sus atributos de entrada, restando un valor t y examinando el signo del resultado. 
Supongamos la siguiente tabla:

```{r echo=FALSE, fig.align='left', out.width='30%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig11.png")
```

El perceptrÃ³n serÃ­a aquella red neuronal que recibiera de entrada cada uno de los valores x y diera como resultado y

```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig12.png")
```

$$
y =
\begin{cases}
    \text{1, si:  } 0.3ğ‘¥_1+0.3ğ‘¥_1+0.3ğ‘¥_1âˆ’0.4 â‰¥ 0 \\
    \text{-1, si:  } 0.3ğ‘¥_1+0.3ğ‘¥_1+0.3ğ‘¥_1âˆ’0.4 < 0 \\
\end{cases}
$$
Como el resultado depende del signo, la funciÃ³n se representa como:
$$
ğ‘¦=ğ‘ ğ‘–ğ‘”ğ‘›(ğ‘¤,ğ‘¥)
$$
Cuando intentamos hacer la separaciÃ³n de un XOr, podemos notar que necesitamos dos discriminadores lineales:

```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig13.png")
```


### Redes Neuronales

Las redes neuronales, son una estructura mÃ¡s compleja que un perceptrÃ³n. Algunas de las complejidades son:

- Las redes neuronales tienen **varios nodos intermedios** entre los nodos de entrada y los nodos de salida.
- Las capas ocultas pueden utilizar **funciones de activaciÃ³n diferentes**

```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig14.png")
```

```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig15.png")
```


Con una capa oculta, es posible generar una funciÃ³n de discriminaciÃ³n para una tabla XOr

```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig17.png")
```


**Funciones de activaciÃ³n comunes**

```{r echo=FALSE, fig.align='left', out.width='60%'}
knitr::include_graphics("img/04_Linear_Neural_Networks_for_Classification/fig16.png")
```




## Notas adicionales {.unnumbered}
Curso de Git: https://www.youtube.com/watch?v=3GymExBkKjE&t=195s

